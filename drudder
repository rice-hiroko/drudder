#!/usr/bin/env python3

"""
Copyright (c) 2015-2016  Jonas Thiem et al.

This software is provided 'as-is', without any express or implied
warranty. In no event will the authors be held liable for any damages
arising from the use of this software.

Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:

1. The origin of this software must not be misrepresented; you must not
   claim that you wrote the original software. If you use this software
   in a product, an acknowledgement in the product documentation would be
   appreciated but is not required.
2. Altered source versions must be plainly marked as such, and must not be
   misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution.
"""

"""
    # What is this


    ## Motivation

    **drudder** is a tool which operates on top of docker and docker-compose to
    introduce a simpler and more powerful API.
    It doesn't add any notable new features (other than btrfs snapshot
    handling): instead it tries to offer a well-thought-out interface to
    simplify all the required daily tasks needed for managing your docker
    containers.

    **drudder** is not a fully fledged orchestration tool and is mainly useful
    if you have a single smaller production server where all software runs on.
    If you want to control a huge docker cloud on multiple physical machines,
    go elsewhere.


    ## Features

    - easily start/stop and manage multiple docker-compose controlled
      services in one go

    - creation of atomic snapshots of all your read-write volumes/live data
      without shutting down or pausing your containers (for backup purposes)

    - clean up all stopped containers and dangling volumes with one command

    - one self-contained file



    # Usage


    ## Basics

    These are the basic required commands for a simple single machine setup
    where all your services are located on the local filesystem inside
    the folder ```/srv/```.
    (see the section ```HOW TO add your service``` for details on how those
    services need to be set up)


    ```
      drudder list                - list all known services
      drudder start <service>     - starts the specified service
      drudder stop <service>      - stops the specified service
      drudder restart <service>   - restarts the given service.
                                    **WARNING**: the containers will *always*
                                    get rebuilt and recreated by this command
                                    (unless this would result in dangling
                                    volumes).
                                    All data in the containers outside of
                                    volumes will be reset!
      drudder rebuild <service>   - force rebuild of the service from the
                                    newest Dockerfile and/or image. Please note
                                    this is only required if you want to force
                                    a rebuild from the ground up, the (re)start
                                    actions will already update the container 
                                    if any of the relevant Dockerfiles were
                                    changed.
      drudder info <service>[/subservice] - show extended info about the
                                            service
      drudder logs <service>      - print logs of all docker containers of the
                                    service
      drudder shell <service>[/<subservice>]  - run a shell in the specified
                                                subservice's container
      drudder snapshot <service>  - makes a snapshot of the live data if
                                    enabled. (optional) This feature requires
                                    btrfs
    ```
    **Hint**: You can always use "all" as service target if you want to apply
    an action to all services on your machine.


    ## Maintenance

    These are rare special commands you might need for the occasional special
    maintenance.

    ```
      drudder install-tools       - install and update all required tools for
                                    running drudder on this computer
      drudder clean               - deletes all containers that aren't running
                                    and all dangling volumes
    ```

    # Installation

    Copy the drudder script to /usr/bin/ and set execution bit (chmod +x).
    Then run: ```sudo drudder install-tools```


    # HOW TO add your service

    drudder expects services to be grouped with the help of docker-compose /
    docker-compose.yml. The script will scan the following locations for
    services subfolders with a docker-compose.yml in them:

    - the current working directory when running the script
    - /usr/share/docker-services/
    - /srv/

    Each service folder inside one of those locatoins should contain:

    - docker-compose.yml to launch it. (folders without this file are skipped)
    - livedata/ subfolder where all read-write volumes are mounted to
                                (recommended, see snapshots as described below)

    To list all currently recognized services, type: `drudder list`

    Congratulations, you can now manage launch your service(s) with
    drudder!


    # HOW TO backup

    You should backup all your services. drudder provides snapshot
    functionality to help with this. While you could simply copy your service
    folder with all the mounted volumes in it, this can lead to corrupt copies
    when doing this while some services are operating (SQL databases etc.).

    To use drudder snapshots of your writable volumes during service
    operation, do this:

    1. Enable snapshots as described below

    2. Always run "drudder snapshot all" before you make your backup to get
       consistent snapshots of your writable volumes in a subfolder named
       livedata-snapshpots/ in each respective service folder.



    # HOW TO enable snapshots (optional)

    This feature allows you to easily copy all the live data of your read-write
    mounted volumes your containers as atomic snapshots even while your
    services are running and continue to write data.

    The snapshots will be atomic, therefore they should be suitable even for
    database realtime operations while the database is running and writing to
    the volume(s).


    ## Enable snapshots for a specific service

    How to enable snapshots for a service:

    1. Make sure your services folder is on a btrfs file system (not ext4).

    2. Each of your snapshot enabled services needs to have a subfolder
       livedata/ where all read-write volumes of it are located.


    ## Test/do it

    **Before you go into production, make sure to test snapshots for your
    service(s) at least once!**

    Calling:
       ``` drudder snapshot <service>|"all" ```
  
    will now use btrfs functionality to add a time-stamped folder with an
    atomic snapshot of livedata/ of the specified service(s) into a new
    livedata-snapshots/ subfolder - while your service can continue using the
    volume thanks to btrfs' copy-on-write snapshot functionality.


    ## Restore a snapshot

    You can easily restore such a snapshot by shutting down your service
    temporarily, copying back a snapshot into livedata/ and turning your
    service back on.

"""

""" Copyright (C) Jonas Thiem et al., 2015-2016
"""

TOOL_VERSION="0.1"
BUG_URL="https://github.com/JonasT/drudder"

import argparse
from argparse import RawTextHelpFormatter, HelpFormatter
import base64
from collections import OrderedDict
import copy
import datetime
import json
import multiprocessing
import os
import platform
import pty
import queue
import subprocess
import random
import re
import shutil
import sys
import textwrap
import threading
import time
import traceback
import uuid

class SubprocessHelper(object):
    @staticmethod
    def check_output_with_stdout_copy(cmd, shell=False, stdout=None,
            stderr=None, cwd=None, env=None):
        # Get actual stdout/stderr as to be used internally:
        actual_stdout = stdout
        if stdout == None or stdout == sys.stderr:
            actual_stdout = subprocess.PIPE
        actual_stderr = stderr
        if stderr == None or stderr == subprocess.STDOUT or \
                stderr == sys.stderr:
            actual_stderr = subprocess.PIPE

        # Handle various other parameters:
        if cwd == None:
            cwd = os.getcwd()
        if env == None:
            env = os.environ.copy()

        # Launch command:
        process = subprocess.Popen(cmd, shell=shell,
            stdout=actual_stdout, stderr=actual_stderr,
            cwd=cwd, env=env)

        # Output reader thread definition:
        class OutputReader(threading.Thread):
            def __init__(self, process, fileobj, writer, copy_to=None):
                super().__init__()
                self.process = process
                self.fileobj = fileobj
                self.writer_func = writer
                self.copy_to = copy_to

            def run(self):
                while True:
                    try:
                        c = self.fileobj.read(1)

                        # Make sure it is bytes:
                        try:
                            c = c.decode("utf-8", "replace")
                        except AttributeError:
                            pass

                        # Write to copy file handle if present:
                        if self.copy_to != None:
                            try:
                                self.copy_to.write(c)
                            except TypeError:
                                self.copy_to.write(c.decode(
                                    "utf-8", "replace"))

                        # Process writer func:
                        if self.writer_func != None:
                            self.writer_func(c)
                    except OSError:
                        break

        # Launch output readers as required and collect data:
        stdout_reader = None
        stderr_reader = None
        presented_output_store = {
            "data" : b""
        }
        def writer(data):
            presented_output_store["data"] += data
        if actual_stdout == subprocess.PIPE:  # we need to collect stdout.
            copy_to = stdout
            if stdout == None or stdout == subprocess.PIPE:
                copy_to = sys.stdout
            stdout_reader = OutputReader(process, process.stdout, writer,
                copy_to=copy_to)
            stdout_reader.start()
        if actual_stderr == subprocess.PIPE:  # we need to collect stderr.
            copy_to = stderr
            if stderr == None or stderr == subprocess.PIPE:
                copy_to = sys.stderr
            if stderr == subprocess.STDOUT:
                stderr_reader = OutputReader(process, process.stderr, writer,
                    copy_to=copy_to)
            else:
                stderr_reader = OutputReader(process, process.stderr, None,
                    copy_to=copy_to)
            stderr_reader.start()

        # Wait for process to end:
        process.wait()
        time.sleep(1)
        try:
            process.kill()
        except Exception:
            pass

        # Evaluate whether it ran successfully:
        exit_code = process.returncode
        if exit_code != 0:
            new_error = subprocess.CalledProcessError(
                cmd=cmd, returncode=exit_code)
            if stdout_reader != None:
                new_error.output = stdout_reader.complete_output
            raise new_error
        if stdout_reader != None or (stderr_reader != None and \
                stderr == subprocess.STDOUT):
            return presented_output_store["data"]
        return None

    @staticmethod
    def check_output_with_isolated_pty(cmd, shell=False, cwd=None,
            stdout=None, stderr=None, timeout=None, env=None):
        """ This is a re-implementation of subprocess.check_output() which
            will isolate the child process in a pseudo-TTY to prevent it from
            accessing the main terminal itself where drudder itself is
            running from.
        """
        if cwd == None:
            cwd = os.getcwd()
        if stderr == None:
            stderr = subprocess.DEVNULL
        if env == None:
            env = os.environ.copy()

        parent_conn, child_conn = multiprocessing.Pipe()
        pid, fd = pty.fork()

        # Child with pseudo TTY (since some tools like to detect the terminal
        # size and greatly mess with our parsing):
        if pid == 0:
            result_dict = dict()
            error_happened = None
            result = None
            # Run actual command in forked process with fake TTY:
            try:
                if stdout != None:
                    result = subprocess.check_output(cmd, shell=shell,
                        cwd=cwd, stdout=stdout, stderr=stderr,
                        timeout=timeout, env=env)
                else:
                    result = subprocess.check_output(cmd, shell=shell,
                        cwd=cwd, stderr=stderr, timeout=timeout,
                        env=env)
                try:
                    result = result.encode("utf-8", "replace")
                except AttributeError:
                    pass
            except Exception as e:
                if isinstance(e, CalledProcessError):
                    result_dict["result"] = base64.b64encode(
                        e.output).decode("utf-8", "ignore")
                error_happened = e

            # Collect result and send back to parent:
            result_dict["error_happened"] = \
                None if error_happened is None else str(
                    error_happened)
            if result != None:
                result_dict["result"] = base64.b64encode(
                    result).decode("utf-8", "ignore")
            dumped = json.dumps(result_dict)
            child_conn.send(dumped)
            time.sleep(1)
            child_conn.close()

            # We're done. Parent will receive result and do stuff:
            time.sleep(1)
            os.exit(0)
        else:
            try:
                # Wait for child to execute process and report result:
                if timeout == None:
                    result_dict = json.loads(parent_conn.recv())
                else:
                    if not parent_conn.poll(timeout):
                        class PtyTimeoutError(subprocess.CalledProcessError):
                            def __init__(self, exit_code, cmd, cwd):
                                super().__init__(exit_code, cmd)
                                self.cwd = cwd

                            def __repr__(self):
                                return "PtyTimeoutError: command " +\
                                    "timed out: " + str(self.cmd) +\
                                    " (cwd: " + str(self.cwd) + ")"

                            def __str__(self):
                                return self.__repr__()

                        e = PtyTimeoutError(1, cmd, cwd)
                        e.output = b""
                        raise e
                    result_dict = json.loads(parent_conn.recv())
            finally:
                parent_conn.close()
            if result_dict["error_happened"] != None:
                e = subprocess.CalledProcessError(1, cmd)
                if result_dict["result"] != None:
                    e.output = base64.b64decode(result_dict["result"])
                else:
                    e.output = b""
                raise e
            return base64.b64decode(result_dict["result"])


class YAMLInstallInfo(object):
    @staticmethod
    def test_for_yaml():
        try:
            import yaml
            return True
        except ImportError:
            path = YAMLInstallInfo.custom_yaml_install_path()
            if path != None:
                return True
        return False

    @staticmethod
    def custom_yaml_install_path_parent():
        if platform.system().lower() == "windows":
            return os.path.join(os.environ['WINDIR'], "drudder-PyYAML")
        else:
            return os.path.join("/var", "lib", "drudder-PyYAML")

    @staticmethod
    def custom_yaml_install_path():
        parent_folder = YAMLInstallInfo.custom_yaml_install_path_parent()
        if not os.path.exists(parent_folder):
            return None
        for fname in os.listdir(parent_folder):
            if fname.startswith("PyYAML-"):
                full_path = os.path.join(parent_folder, fname)
                if os.path.isdir(full_path):
                    return full_path
        return None
       

class Installer(object):
    @staticmethod
    def latest_docker_machine_version():
        import urllib.request
        tag_url = "https://github.com/docker/machine/releases/latest/"
        try:
            target = urllib.request.urlopen(tag_url)
            final_url = target.geturl()
            try:
                final_url = final_url.decode("utf-8")
            except AttributeError:
                pass
            if final_url.endswith("/"):
                final_url = final_url[:-1]
            version_tag = final_url.rpartition("/")[2]
        except Exception as e:
            raise RuntimeError("failed to obtain docker-machine version tag")
        return version_tag

    @staticmethod
    def latest_docker_compose_version():
        import urllib.request
        tag_url = "https://github.com/docker/compose/releases/latest/"
        try:
            target = urllib.request.urlopen(tag_url)
            final_url = target.geturl()
            try:
                final_url = final_url.decode("utf-8")
            except AttributeError:
                pass
            if final_url.endswith("/"):
                final_url = final_url[:-1]
            version_tag = final_url.rpartition("/")[2]
        except Exception as e:
            raise RuntimeError("failed to obtain docker-compose version tag")
        return version_tag

    @staticmethod
    def install_docker_machine():
        return Installer.install_docker_tool(
            tool_name="docker-machine")

    @staticmethod
    def install_docker_compose():
        return Installer.install_docker_tool(
            tool_name="docker-compose")

    @staticmethod
    def install_docker_tool(tool_name=None):
        if tool_name == None:
            raise ValueError(
                "provide the name of the tool to be installed!")

        # Getting various helper functions:
        if tool_name == "docker-machine":
            path_func = SystemInfo.docker_machine_path
            version_func = Installer.latest_docker_machine_version
            short_name = "machine"
        elif tool_name == "docker-compose":
            path_func = SystemInfo.docker_compose_path
            version_func = Installer.latest_docker_compose_version
            short_name = "compose"
        else:
            raise ValueError("unsupported docker tool: " + tool_name)

        # Check if the given tool is already installed:
        if path_func(fail_if_not_found=False) != None:
            # Already installed, nothing to do.
            return

        # Follow the "latest" URL to the actual latest tag:
        try:
            version_tag = version_func()
        except RuntimeError:
            print("drudder: error: failed to obtain " + tool_name +\
                "version tag",
                file=sys.stderr)
            print("Please check your internet connectivity.",
                file=sys.stderr)
            sys.exit(1)

        import urllib.request
        
        # Put together the download URL:
        # Reference URL:
        # https://github.com/docker/machine/releases/download/
        #   v0.7.0/docker-machine-`uname -s`-`uname -m`
        if platform.system().lower() == "windows":
            url = "https://github.com/docker/" + short_name +\
                "/releases/" +\
                "download/" + str(version_tag) + "/docker-" +\
                short_name + "-" +\
                "Windows-x86_64.exe"
        else:
            url = "https://github.com/docker/" +\
                short_name + "/releases/" +\
                "download/" + str(version_tag) + "/docker-" +\
                short_name + "-" +\
                subprocess.check_output(["uname", "-s"]).\
                    decode("utf-8").strip() + "-" +\
                subprocess.check_output(["uname", "-m"]).\
                    decode("utf-8").strip()

        # Download docker-machine binary:
        print("Downloading " + tool_name + "... (this might take " +\
                "a while)")
        try:
            local_filename, headers = urllib.request.urlretrieve(
                url)
        except Exception as e:
            print("drudder: error: failed to obtain " + str(url),
                file=sys.stderr)
            print("Please check your internet connectivity.",
                file=sys.stderr)
            sys.exit(1)

        # Put binary into place for use:
        if platform.system().lower() == "windows":
            docker_tool_target = os.path.join(os.environ['WINDIR'],
                "system32", tool_name + ".exe")
        else:
            docker_tool_target = os.path.join("/usr", "local", "bin",
                tool_name)
        try:
            shutil.copy(local_filename, docker_tool_target)
        except Exception as e:
            print("drudder: error: failed to create path: " +\
                str(docker_tool_target), file=sys.stderr)
            print("Please re-run this with sudo / administrator " +\
                "privileges.", file=sys.stderr)
            sys.exit(1)

        # Set executable flag (+x):
        import stat
        os.chmod(docker_tool_target,
            os.stat(docker_tool_target).st_mode
            | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)

    @staticmethod
    def check_and_install_missing():
        missing = []
        if not YAMLInstallInfo.test_for_yaml():
            missing.append("PyYAML")
        if not SystemInfo.docker_path(fail_if_not_found=False):
            missing.append("docker")
        if not SystemInfo.docker_machine_path(fail_if_not_found=False):
            missing.append("docker-machine")
        if not SystemInfo.docker_compose_path(fail_if_not_found=False):
            missing.append("docker-compose")

        explicit_install = False
        if len(sys.argv) >= 2:
            if sys.argv[1].strip() == "install-tools":
                explicit_install = True

        if len(missing) > 0:
            do_install = True
            if not explicit_install:
                print("drudder: warning: missing components detected: " +\
                    ", ".join(missing) + ". Invoking installer...")
                do_install = SystemInfo.yesno(
                    "Do you want to install (system-wide): " +\
                    ", ".join(missing), default_yes=False)
            else:
                print("Installing the following components: " +\
                    ", ".join(missing))
            if do_install:
                for component in missing:
                    if component == "PyYAML":
                        Installer.install_yaml()
                    elif component == "docker-machine":
                        Installer.install_docker_machine()
                    elif component == "docker-compose":
                        Installer.install_docker_compose()
                    elif component == "docker":
                        Installer.install_docker()
            else:
                print("drudder: error: missing components: " +\
                    ", ".join(missing))
                sys.exit(1)

        if explicit_install:
            print("All tools installed.", file=sys.stderr)
            sys.exit(0)

    @staticmethod
    def install_yaml():
        if YAMLInstallInfo.test_for_yaml():
            # YAML is available, nothing to do.
            return

        # Obtain PyPI page for download link:
        import urllib.request
        pyyaml_url = 'https://pypi.python.org/pypi/PyYAML'
        try:
            with urllib.request.urlopen(
                    pyyaml_url) as response:
                html = response.read()
                try:
                    html = html.decode("utf-8", "replace")
                except AttributeError:
                    pass
        except Exception as e:
            print("drudder: error: failed to obtain " + str(pyyaml_url),
                file=sys.stderr)
            print("Please check your internet connectivity.", file=sys.stderr)
            sys.exit(1)
        def extract_source_link():
            _html = html
            pos = 0
            while True:
                def extract_block_url_title(block):
                    link_start = block.find("<a href=\"")
                    if link_start < 0:
                        return (None, None)
                    link_start += len("<a href=\"")
                    link_end = block[link_start:].find("\"")
                    if link_end < 0:
                        return (None, None)
                    link_end += link_start
                    text_block_start = block.find(">",
                        block.find("<td ", link_end))
                    if text_block_start < 0:
                        return (None, None)
                    text_block_end = block.find("<", text_block_start)
                    if text_block_end < 0:
                        return (None, None)
                    link_target = block[link_start:link_end]
                    block_title = block[text_block_start:text_block_end]
                    return (link_target, block_title)
                # Find next relevant block start:
                next_block_start_1 = _html.find("<tr class=\"odd", pos)
                next_block_start_2 = _html.find("<tr class=\"even", pos)
                next_block_start = next_block_start_1
                if next_block_start < 0 or (next_block_start_2 >= 0 and \
                        next_block_start_2 < next_block_start):
                    next_block_start = next_block_start_2
                if next_block_start < 0:
                    return None
                # Get the end to the block:
                next_block_end = _html[next_block_start:].find("</tr>")
                if next_block_end <= 0:
                    return None
                next_block_end += next_block_start
                # Analyze block for link
                (url, title) = extract_block_url_title(_html[
                    next_block_start:next_block_end])
                if url != None and title.strip().lower().find("source") >= 0:
                    return url.partition("#")[0]

        # PyYAML source download link:
        yaml_source_link = extract_source_link()
        if yaml_source_link == None:
            print("drudder: error: failed to obtain PyYAML download link. " +\
                "Please check your internet connection, and if this " +\
                "problem persists, file a bug at " +\
                BUG_URL, file=sys.stderr)
            sys.exit(1)
        if not yaml_source_link.startswith("https://"):
            print("drudder: error: PyYAML download link is bogus, " +\
                "unexpected protocol: " + str(yaml_source_link))
            print("Please file a bug at " +\
                BUG_URL, file=sys.stderr)
            sys.exit(1)

        # Make sure the install base path exists:
        parent_path = YAMLInstallInfo.custom_yaml_install_path_parent()
        if not os.path.exists(parent_path):
            try:
                os.mkdir(parent_path) 
            except OSError:
                print("drudder: error: failed to create path: " +\
                    str(parent_path), file=sys.stderr)
                print("Please re-run this with sudo / administrator " +\
                    "privileges.", file=sys.stderr)
                sys.exit(1)

        # Get source:
        try:
            print("Downloading PyYAML... (this might take " +\
                "a while)")
            local_filename, headers = urllib.request.urlretrieve(
                yaml_source_link)
            source_extract_path = os.path.join(
                parent_path, "PyYaml.tar.gz")
            shutil.copy(local_filename, source_extract_path)
            os.remove(local_filename)
        except Exception as e:
            print("drudder: error: failed to obtain " + str(
                yaml_source_link_url),
                file=sys.stderr)
            print("Please check your internet connectivity.", file=sys.stderr)
            sys.exit(1)

        # Extract PyYAML:
        import tarfile
        tar_file = tarfile.open(source_extract_path)
        tar_file.extractall(parent_path)

    @staticmethod
    def install_docker():
        # Check if already installed:
        path_to_docker = SystemInfo.docker_path(fail_if_not_found=False)
        if path_to_docker != None:
            # Docker is already installed, nothing to do.
            return

        # Detect distribution:
        distribution = None
        distribution_codename = None
        distribution_version = None
        if os.path.exists("/etc/lsb-release"):
            with open("/etc/lsb-release") as f:
                contents = f.read()
            try:
                contents = contents.decode("utf-8", "replace")
            except AttributeError:
                pass
            contents = contents.replace("\r\n", "\n").replace("\r", "\n")
            for line in contents.split("\n"):
                _line = line.strip()
                if _line.startswith("DISTRIB_ID="):
                    distribution = _line.partition("=")[2].lower()
                elif _line.startswith("DISTRIB_CODENAME"):
                    distribution_codename = _line.partition("=")[2].lower()
                elif _line.startswith("DISTRIB_RELEASE"):
                    distribution_version = _line.partition("=")[2]
        elif os.path.exists("/etc/os-release"):
            with open("/etc/os-release") as f:
                contents = f.read()
            try:
                contents = contents.decode("utf-8", "replace")
            except AttributeError:
                pass
            contents = contents.replace("\r\n", "\n").replace("\r", "\n")
            for line in contents.split("\n"):
                _line = line.strip()
                if _line.startswith("ID="):
                    distribution = _line.partition("=")[2].lower()
                    if distribution.startswith("\"") and \
                            distribution.endswith("\""):
                        distribution = distribution[1:-1]
                elif _line.startswith("VERSION=") and _line.find("(") > 0:
                    distribution_codename = ((_line.partition("=")[2]).\
                        partition("(")[2]).partition(")")[0].lower()
                elif _line.startswith("VERSION_ID="):
                    extract = line.partition("=")[2]
                    if extract.startswith("\"") and \
                            extract.endswith("\""):
                        extract = extract[1:-1]
                    try:
                        distribution_version = int(extract)
                    except (TypeError, ValueError):
                        pass
        if distribution == None and os.path.exists("/etc/redhat-release"):
            with open("/etc/redhat-release") as f:
                contents = f.read()
            try:
                contents = contents.decode("utf-8", "replace")
            except AttributeError:
                pass
            contents = contents.strip()
            if contents.startswith("Fedora release "):
                distribution = "fedora"
                distribution_version = contents[len("Fedora release "):].lstrip()
                distribution_version = distribution_version.partition(" ")[0]
        if distribution == None:
            print("drudder: error: failed to obtain detect distribution. " +\
                "This distribution is most likely not supported for " +\
                "docker install.",
                file=sys.stderr)
            print("If you want to help change this, please file a ticket "+\
                "at " + str(BUGS_URL), file=sys.stderr)
            sys.exit(1)

        # Debian/Ubuntu docker install:
        if distribution == "ubuntu" or distribution == "debian":
            if distribution_codename == None:
                print("drudder: error: failed to detect detailed " +\
                    "distribution code name: " + distribution,
                    file=sys.stderr)
                print("Please report this problem at " + BUGS_URL,
                    file=sys.stderr)
                sys.exit(1)
            try:
                subprocess.check_output(["apt-key", "adv", "--keyserver",
                    "hkp://p80.pool.sks-keyservers.net:80", "--recv-keys",
                    "58118E89F3A912897C070ADBF76221572C52609D"])
            except subprocess.CalledProcessError:
                print("drudder: error: distribution key import failed.",
                    file=sys.stderr) 
                print("Please re-run this with sudo / administrator " +\
                    "privileges, and check your internet connectivity.",
                    file=sys.stderr)
                print("If you think this is a bug, please file a ticket "+\
                    "at " + str(BUGS_URL), file=sys.stderr)
                sys.exit(1)
            sources_lst_file = os.path.join("/etc", "apt", "sources.list.d",
                "docker.list")
            if os.path.exists(sources_lst_file):
                try:
                    os.remove(sources_lst_file)
                except OSError:
                    print("drudder: error: failed to remove file: " +\
                        str(sources_lst_file))
                    print("Please re-run this with sudo / administrator " +\
                        "privileges", file=sys.stderr)
                    sys.exit(1)
            try:
                with open(sources_lst_file, "w") as f:
                    f.write("deb https://apt.dockerproject.org/repo " +\
                        distribution + "-" + distribution_codename + " main")
            except OSError:
                print("drudder: error: failed to create file: " +\
                    str(sources_lst_file))
                print("Please re-run this with sudo / administrator " +\
                    "privileges", file=sys.stderr)
                sys.exit(1)

            # Refresh package index with new repository added:
            try:
                SubprocessHelper.check_output_with_stdout_copy(
                    ["apt-get", "update"],
                    stderr=subprocess.STDOUT)
            except subprocess.CalledProcessError as e:
                if e.output.find("E: The method driver " +\
                        "/usr/lib/apt/methods/https could not be found") >= 0:
                    SubprocessHelper.check_output_with_stdout_copy(
                        ["apt-get", "install", "apt-transport-https"],
                        stderr=subprocess.STDOUT)
                    SubprocessHelper.check_output_with_stdout_copy(
                        ["apt-get", "update"],
                        stderr=subprocess.STDOUT)
                else:
                    raise e

            # Install docker-engine package:
            SubprocessHelper.check_output_with_stdout_copy(
                ["apt-get", "install", "-y", "docker-engine"],
                stderr=subprocess.STDOUT)
            return
        # Fedora docker install:
        elif distribution == "fedora":
            sources_lst_file = os.path.join("/etc", "yum.repos.d",
                "docker.repo")
            if os.path.exists(sources_lst_file):
                try:
                    os.remove(sources_lst_file)
                except OSError:
                    print("drudder: error: failed to remove file: " +\
                        str(sources_lst_file))
                    print("Please re-run this with sudo / administrator " +\
                        "privileges", file=sys.stderr)
                    sys.exit(1)
            try:
                with open(sources_lst_file, "w") as f:
                    f.write(textwrap.dedent("""\
                    [dockerrepo]
                    name=Docker Repository
                    baseurl=https://yum.dockerproject.org/repo/main/fedora/$releasever/
                    enabled=1
                    gpgcheck=1
                    gpgkey=https://yum.dockerproject.org/gpg
                    """))
            except OSError:
                print("drudder: error: failed to create file: " +\
                    str(sources_lst_file))
                print("Please re-run this with sudo / administrator " +\
                    "privileges", file=sys.stderr)
                sys.exit(1)

            # Install docker-engine package:
            print("Invoking system update...")
            exit_code = subprocess.call(["dnf", "install", \
                "-y", "docker-engine"])
            if exit_code != 0:
                raise RuntimeError("command failed: " +\
                    "dnf install -y docker-engine")
            return
        # Others:
        else:
            print("drudder: error: unsupported distribution: " + str(
                distribution))
            print("If you want to help change this, please file a ticket "+\
                "at " + str(BUGS_URL), file=sys.stderr)
            sys.exit(1)


# Only import this on Unix systems:
if platform.system().lower() != "windows":
    import stat

class SystemInfo(object):
    """ This class provides system info of various sorts about the installed
        docker versions, btrfs and more.
    """
    __cached_docker_path = None
    __cached_docker_machine_path = None

    @staticmethod
    def yesno(text, default_yes=False):
        if not default_yes:
            appendix = " [y/N]"
        else:
            appendix = " [Y/n]"
        ask_text = text + appendix
        result = input(ask_text).lower().strip()
        if result == "y" or result == "yes":
            return True
        if result == "n" or result == "no":
            return False
        if default_yes:
            return True
        return False

    @staticmethod
    def locate_binary(name):
        """ Locate a binary of some name in the common system-wide places
            and return the full path, or return None if it can't be found.
        """
        badchars = '\'"$<> %|&():*/\\{}#!?=\n\r\t[]\033'
        for char in badchars:
            if name.find(char) >= 0:
                raise ValueError("dangerous character in binary name")
        output = None
        try:
            output = subprocess.check_output("which " + name, shell=True,
                stderr=subprocess.STDOUT).\
                decode("utf-8", "ignore").strip()
        except subprocess.CalledProcessError:
            pass
        if output == None or len(output) == 0:
            return None
        return output

    @staticmethod
    def docker_machine_path(fail_if_not_found=True):
        if SystemInfo.__cached_docker_machine_path != None:
            return SystemInfo.__cached_docker_machine_path
        path = SystemInfo.locate_binary("docker-machine")
        if path != None:
            return path
        if fail_if_not_found is True:
            print("drudder: error: no docker-machine found. Is it installed?")
            sys.exit(1)
        return None

    @staticmethod
    def docker_path(fail_if_not_found=True):
        """ Locate docker binary and return its path, or print an error
            and exit the program with sys.exit(1) if not available.
        """
        if SystemInfo.__cached_docker_path != None:
            return SystemInfo.__cached_docker_path
        
        def behaves_like_docker(binary_path):
            output = None
            try:
                output = subprocess.check_output([binary_path, "--version"],
                    stderr=subprocess.STDOUT)
            except subprocess.CalledProcessError as e:
                output = e.output
            output = output.decode("utf-8", "ignore")
            return (output.lower().startswith("docker version "))

        test_names = [ "docker.io", "docker" ]

        for test_name in test_names:
            bin_path = SystemInfo.locate_binary(test_name)
            if bin_path == None:
                continue
            if behaves_like_docker(bin_path):
                return bin_path
        if fail_if_not_found is True:
            print("drudder: error: no docker found. Is it installed?")
            sys.exit(1)
        return None
    
    @staticmethod
    def is_btrfs_subvolume(path):
        """ Check if the given path is a btrfs subvolume. Returns True if
            if it is, or False if not. May print out various warnings if there
            were problems detecting this. May raise a ValueError if the path
            couldn't be properly examined for some reason.
        """
        path = os.path.realpath(path)
        nontrivial_error = "error: failed to map a btrs subvolume "+\
            "to its POSIX path. This seems to be a non-trivial setup."+\
            " You should do your snapshotting manually!!"

        # First, get the containing mount point:
        base_path = os.path.normpath(path + "/../")
        mount = SystemInfo.get_fs_mount_root(base_path)
        if mount == None:
            print("drudder: warning: internal problem: " +\
                "mount point of " +\
                str(base_path) + " was returned as: None")
            details = "cannot check path. Mount point search failed: " +\
                str(path)
            print_msg(nontrivial_error + "\n\nError details: " + details,
                color="red")
            raise ValueError(details)

        # Get btrfs subvolume list:
        output = subprocess.check_output([
            SystemInfo.locate_binary("btrfs"),
            "subvolume", "list", path]).\
            decode("utf-8", "ignore").strip().split("\n")
        for line in output:
            if len(line.strip()) == 0:
                continue
            if not line.startswith("ID ") or line.find(" path ") < 0:
                raise RuntimeError("unexpected btrfs tool output - " + \
                    "maybe incompatible tool version? Please report this." +\
                    " Full output: " + str(output))
            line = line[line.find(" path ")+len(" path "):].strip()
            full_path_guess = mount + line
            if not os.path.exists(full_path_guess):
                if line == "DELETED":
                    continue
                print_msg(nontrivial_error, color="red")
                return False
            if os.path.normpath(os.path.abspath(full_path_guess)) == \
                    os.path.normpath(os.path.abspath(path)):
                try:
                    output = subprocess.check_output([
                        SystemInfo.locate_binary("stat"),
                        "-c", "%i", path]).decode('utf-8', 'ignore').strip()
                except subprocess.CalledProcessError as e:
                    # Stat failed, although btrfs subvolume list lists it!
                    details = "btrfs volume listed by btrfs tool " +\
                        "cannot be stat'ed: " + str(path)
                    print_msg(nontrivial_error + "\n\nError details: " +\
                        details, color="red")
                    raise ValueError(details)
                if output != "256":
                    # Not a subvolume, although btrfs subvolume list lists it!
                    details = "btrfs volume listed by btrfs tool " +\
                        "doesn't appear to be " +\
                        "an actual btrfs volume according to stat: " +\
                         str(path)
                    print_msg(nontrivial_error + "\n\nError details: " +\
                        details, color="red")
                    raise ValueError(details)
                return True
        try:
            output = subprocess.check_output([
                SystemInfo.locate_binary("stat"),
                "-c", "%i", path]).decode('utf-8', 'ignore').strip()
        except subprocess.CalledProcessError as e:
            pass
        if output == "256":
            # Stat says it's a subvolume, although we don't think it is!
            details = "stat reports path as btrfs volume, but " +\
                "we didn't detect respective entry from btrfs tool: " +\
                str(path)
            print_msg(nontrivial_error + "\n\nError details: " + \
                details, color="red")
            raise ValueError(details)
        return False


    @staticmethod
    def docker_compose_path(fail_if_not_found=True):
        """ Locate docker-compose binary and return its path, or exit process
            with error if not available.
        """
        bin_path = SystemInfo.locate_binary("docker-compose")
        if bin_path != None:
            return bin_path
        if fail_if_not_found:
            print("drudder: error: no docker-compose found. " + \
                "Is it installed?")
            sys.exit(1)
        return None

    @staticmethod
    def btrfs_path():
        """ Locate btrfs helper tool binary and return its path, or return
            None if not found.
        """
        bin_path = SystemInfo.locate_binary("btrfs")
        if bin_path != None:
            return bin_path
        return None

    @staticmethod
    def get_fs_mount_root(path):
        """ Find out where the mount point of the filesystem is located of the
            given file path. (e.g. for a path "/home/myuser/somefile"
            located inside a home partition mounted at /home/, this would
            return "/home/") The information is probed using the "df" tool.
        """
        # Make the path absolute and make sure it leads to an existing thing:
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise ValueError("given path does not exist: " + str(path))

        # If this is a symlink itself, make sure the parent folder is
        # converted to the actual disk path (-> all parent symlinks are
        # resolved to actual disk paths):
        if os.path.islink(path):
            element_name = os.path.basename(os.path.normpath(path))
            parent = os.path.normpath(os.path.join(
                os.path.normpath(path), ".."))
            path = os.path.join(os.path.normpath(parent), element_name)
        else:
            # Not a symlink itself. Simply make sure it's the actual real
            # path on disk (-> all symlinks in the path are resolved):
            path = os.path.realpath(path)

        # Check again to make sure the real disk path is an existing thing:
        if not os.path.exists(path):
            raise ValueError(
                "converted real path does not exist: " + str(path))

        # Run "df" to find the mount point:
        output = subprocess.check_output([
            SystemInfo.locate_binary("df"), path]).\
            decode("utf-8", "ignore").strip()

        # Skip first line:
        if output.find("\n") <= 0:
            raise RuntimeError("failed to parse df output")
        output = output[output.find("\n")+1:]

        # Skip past first entry:
        skip_pos = output.find(" ")
        if skip_pos <= 0 or skip_pos >= len(output):
            raise RuntimeError("failed to parse df output")
        output = output[skip_pos+1:].strip()

        # Skip past all entries not starting with /
        while True:
            fwslash = output.find("/")
            spacepos = output.find(" ")
            if fwslash < 0:
                raise RuntimeError("failed to parse df output")
            if spacepos >= 0 and spacepos < fwslash:
                output = output[spacepos+1:].strip()
                continue
            break

        if not output.startswith("/"):
            raise RuntimeError("failed to parse df output")
        return output

    @staticmethod
    def filesystem_type_at_path(path):
        """ Find out the filesystem a given directory or file is on and return
            the name (e.g. "ext4", "btrfs", ...). The information is probed
            using the "df" tool.
        """
        if not os.path.exists(path):
            raise ValueError("given path does not exist: " + str(path))
        output = subprocess.check_output([
            SystemInfo.locate_binary("df"), path]).\
            decode("utf-8", "ignore")

        # Skip first line:
        if output.find("\n") <= 0:
            raise RuntimeError("failed to parse df output")
        output = output[output.find("\n")+1:]

        # Get first word being the FS root of the path:
        end_pos = output.find(" ")
        if end_pos <= 0:
            raise RuntimeError("failed to parse df output")
        device_of_path = output[:end_pos]
        if device_of_path == "-":
            # We can't find out the filesystem of this path.
            # -> try to find out the parent!
            get_parent = os.path.normpath(path + "/../")
            if get_parent == os.path.normpath(path) or path == "/":
                # We are already at the root.
                return None
            return SystemInfo.filesystem_type_at_path(os.path.normpath(
                path + "/../"))

        output = subprocess.check_output([
            SystemInfo.locate_binary("mount")]).\
            decode("utf-8", "ignore")
        def find_mount_for(device):
            for line in output.split("\n"):
                if not line.startswith(device + " "):
                    continue
                i = len(line) - 1
                while not line[i:].startswith(" type ") and i > 0:
                    i -= 1
                if not line[i:].startswith(" type "):
                    raise RuntimeError("failed to parse mount output")
                fs_type = line[i + len(" type "):].strip()
                if fs_type.find(" ") > 0:
                    fs_type = fs_type[:fs_type.find(" ")].strip()
                return fs_type
            return None
        result = find_mount_for(device_of_path)
        if result == None:
            result = find_mount_for(os.path.realpath(device_of_path))
        if result != None:
            return result
        raise RuntimeError('failed to find according mount entry for: ' +\
            path + " (device path: " + str(device_of_path) + ")")

    @staticmethod
    def btrfs_subvolume_stat_check(path):
        """ Checks whether something that is supposedly a btrfs volume is
            also identified as such by the "stat" tool (returns True) or not
            (returns False).
        """
        output = subprocess.check_output([locate_binary("stat"),
            "-c", "%i", path]).decode('utf-8', 'ignore').strip()
        return (output == "256")

Installer.check_and_install_missing()

yaml_available = False
try:
    import yaml
    yaml_available = True
except ImportError as e1:
    yaml_available = False
    yml_lib_path = YAMLInstallInfo.custom_yaml_install_path()
    if yml_lib_path != None and os.path.exists(yml_lib_path):
        sys.path.insert(0, yml_lib_path + "/lib3/")
        sys.path.insert(0, yml_lib_path + "/lib3/yaml/")
        try:
            import yaml
            yaml_available = True
        except ImportError as e2:
            pass
if not yaml_available:
    print("drudder: error: PyYAML import unexpectedly failed. " +\
        "Please file a bug at " + str(BUG_URL))
    sys.exit(1)

class DoubleLineBreakFormatter(HelpFormatter):
    """ Retains double line breaks/paragraphs """
    def _split_lines(self, text, width):
        return self._fill_text(text, width, "").splitlines(False)

    def _fill_text(self, t, width, indent):
        t = " ".join([s for s in t.replace("\t", " ").strip("\t ").split(" ")\
            if len(s) > 0]).replace("\n ", "\n").replace(" \n", " ")
        ts = re.sub("([^\n])\n([^\n])", "\\1 \\2", t).split("\n\n")
        result = [textwrap.fill(paragraph, width,
            initial_indent=indent, subsequent_indent=indent)\
            for paragraph in ts]
        return "\n\n".join(result)

class DockerComposeYml(object):
    def __init__(self, path, container=None):
        # If path just goes to directory, add file name:
        if os.path.isdir(path):
            path = os.path.join(path, "docker-compose.yml")

        # Remember path:
        self.path = path

        # Load up .yml contents:
        contents = None
        with open(path, "rb") as f:
            contents = f.read().decode("utf-8").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")
        self.parsed_obj = yaml.safe_load(contents)

        # Extract desired data:
        self.is_v2 = False
        self._desired_data = self.parsed_obj
        if isinstance(self.parsed_obj, dict) and \
                "version" in self.parsed_obj:
            v = 0
            try:
                v = int(self.parsed_obj["version"])
            except (TypeError, ValueError):
                pass
            if v >= 2:
                self.is_v2 = True
        if container != None: # data requested for specific container:
            self._desired_data = None
            if self.is_v2:
                self._desired_data = None
                if isinstance(self.parsed_obj, dict) and \
                        "services" in self.parsed_obj:
                    if isinstance(self.parsed_obj["services"], dict) and \
                            container in self.parsed_obj["services"]:
                        if isinstance(self.parsed_obj["services"]\
                                [container], dict):
                            self._desired_data = self.parsed_obj\
                                ["services"][container]
            else:
                if isinstance(self.parsed_obj, dict) and \
                        container in self.parsed_obj:
                    if isinstance(self.parsed_obj[container], dict):
                        self._desired_data = self.parsed_obj[container]

    @property
    def data(self):
        return self._desired_data

    @property
    def services(self):
        if not isinstance(self.parsed_obj, dict):
            return dict()
        if self.is_v2:
            if "services" in self.parsed_obj and \
                    isinstance(self.parsed_obj, dict):
                return copy.copy(self.parsed_obj["services"])
            return dict()
        else:
            return copy.copy(self.parsed_obj)
        return dict()

def print_msg(text, service=None, container=None, color="blue"):
    """ Print out a nicely formatted message prefixed with
        [drudder] and/or possibly a service or container name.

        You can specify a signal color, with red and yellow changing the
        displayed message type from INFO to ERROR or WARNING respectively.
    """
    info_msg = "INFO"
    if color == "red":
        info_msg = "\033[1;31mERROR"
    elif color == "yellow":
        info_msg = "\033[1;33mWARNING"
    elif color == "green":
        info_msg = "SUCCESS"

    def color_code():
        part = "\033[1;"
        if color == "blue":
            part += "34"
        elif color == "red":
            part += "31"
        elif color == "yellow":
            part += "33"
        elif color == "green":
            part += "32"
        elif color == "white":
            part += "37"
        return part + "m"

    service_part = ""
    if service != None and len(service) > 0:
        service_part = "\033[0m" + color_code() + service
        if container != None:
            service_part = service_part + "/" + container

    docker_services_part = ""
    if service == None or len(service) == 0:
        docker_services_part = color_code() + "drudder"

    initial_length = len("[drudder")
    if service != None:
        initial_length = len("[" + service)
        if container != None:
            initial_length += len("/" + container)
    initial_length += len("] ") 

    text_color = "\033[0m"
    if color == "yellow" or color == "red":
        text_color += color_code()

    # Split lines according to \n\n:
    lines = [line.strip() for line in text.split("\n\n")]

    # First line with complicated lead up:
    print_text = "\033[0m\033[1m[\033[0m" + docker_services_part + \
        service_part + "\033[0m\033[1m] " + info_msg +\
        " " + text_color
    first_line = True
    for line in lines:
        if first_line:
            print_text += textwrap.fill(line, width=70,
                initial_indent=(" " * initial_length),
                subsequent_indent=(" " * initial_length))[
                initial_length:]
            first_line = False
        else:
            print_text += "\n" + textwrap.fill(line, width=70,
                initial_indent=(" " * initial_length),
                subsequent_indent=(" " * initial_length))
    print_text += "\033[0m"
    print(print_text)

class DataVolumeMount(object):
    """ This represents a docker volume mount with the known information
        about this mount. This is used by the DataVolume class below to
        identify the various places it is mounted on the host's disk
    """
    def __init__(self, host_path=None,
            mount_container=None, mount_container_filesystem_path=None):
        self.host_path = host_path
        self.container = mount_container
        self.mount_container_filesystem_path = mount_container_filesystem_path

    def __repr__(self):
        if self.host_path != None:
            return self.host_path
        return "<unspecified volume mount of " + str(container) + ">"

class DataVolume(object):
    """ This specifies the known information about a data volume. A volume
        can be either with no host directory mount and system-wide/reusable
        with a name, no host directory mount and system-wide/reusable with a
        random id, or with a host directory mount (which means it is usually
        "owned" by just one specific containers).
    """
    def __init__(self, known_host_mount=None, id=None, name=None):
        self.known_host_mount = known_host_mount
        self.id = id
        self.name = None
        self.owning_container = None
        self.owning_container_path = None
        self.specified_in_yml = False

    def __hash__(self):
        return hash(str(self.id) + str(self.owning_container_path) +\
            str(self.known_host_mount))

    def __repr__(self):
        if self.id != None:
            return str(self.id)
        if self.owning_container_path != None:
            return str(self.owning_container_path)
        if self.known_host_mount != None:
            return str(self.known_host_mount)
        return "<DataVolume object with unknown mount and unknown id>"

    def __eq__(self, other):
        if other == None:
            return False
        if not hasattr(other, "id") and not hasattr(other, "name"):
            return False
        if not hasattr(other, "known_host_mount"):
            return False
        if self.id != None and other.id == self.id:
            return True
        if self.known_host_mount != None and \
                self.known_host_mount == other.known_host_mount:
            return True
        if self.owning_container != None and \
                self.owning_container_path != None:
            if self.owning_container == other.owning_container and \
                    os.path.normpath(self.owning_container_path) ==\
                    os.path.normpath(other.owning_container_path):
                return True
        return False

    def _set_owning_container(self, container, container_fs_path=None):
        """ Set the container this volume is associated with. Only for
            internal use. This is called for volumes that are specified as
            host mounts in docker-compose.yml and don't have a proper
            system-wide volume identifier/name for use in other containers.

            DEVELOPER NOTE (Jonas Thiem):
            In theory, a volume that is purely a host directory mount can be
            used by multiple containers of course. However, right now
            drudder only builds up volume information per container, and
            therefore this container-centric approach is sufficient for now.
            However, later this should be changed to allow multiple owning
            containers.
        """
        self.owning_container = container
        if container_fs_path != None:
            self.owning_container_path = container_fs_path

    @property
    def mounts(self):
        """ Return the known places this DataVolume is mounted at. Returns
            a list of DataVolumeMount instances (or an empty list).
        """
        if self.known_host_mount != None:
            if self.owning_container != None:
                return [ DataVolumeMount(host_path=self.known_host_mount,
                    mount_container=self.owning_container,
                    mount_container_filesystem_path=\
                    self.owning_container_path) ]
            else:
                return [ DataVolumeMount(host_path=self.known_host_mount) ]
        elif self.owning_container != None:
            return [ DataVolumeMount(mount_container=self.owning_container,
                mount_container_filesystem_path=\
                self.owning_container_path) ]
        else:
            return []

class ServiceDependency(object):
    """ This class holds the info describing the dependency to another
        service's container.
    """
    def __init__(self, other_service_name, other_service_path,
            other_service_container_name):
        self.service_name = other_service_name
        self.service_path = other_service_path
        self.container_name = other_service_container_name

    @property
    def container(self):
        """ The actual container instance to start/stop the container which
            is the target of this dependency.

            Please note this might be unavailable if the container doesn't
            belong to any known service, in which case accessing this property
            will raise ValueError.
        """
        if self.service_name == None:
            raise ValueError("the service that provides this dependency " +\
                "isn't known")
        return ServiceContainer(self.service_name, self.service_path,
            self.container_name)

    def __repr__(self):
        if self.service_name != None:
            return self.service_name + "/" + self.container_name
        return self.container_name + " (unknown service!!)"

class ServiceContainer(object):
    """ An instance of this class holds the info for a service's container.
        It can be used to e.g. obtain the system-wide docker container name,
        or the directory for the respective docker-compose.yml where
        docker-compose commands can be run.
    """
    def __init__(self, service_name, service_path, container_name,
            image_name=None):
        self.service_name = service_name
        self.service_path = service_path
        self.name = container_name
        self._known_image_name = image_name

    def __repr__(self):
        return self.service_name + "/" + self.name

    def __hash__(self):
        return hash(self.service_name + \
            os.path.normpath(os.path.abspath(self.service_path)) + \
            self.name)

    def __eq__(self, other):
        if other is None:
            return False
        if not hasattr(other, "service_name") or not hasattr(other,
                "service_path") or not hasattr(other, "name"):
            return False
        if other.service_name != self.service_name:
            return False
        if (os.path.normpath(os.path.abspath(self.service_path)) !=
                os.path.normpath(os.path.abspath(other.service_path))):
            return False
        if other.name != self.name:
            return False
        return True

    def __neq__(self, other):
        return not self.__eq__(other)

    @property
    def potentially_lost_volumes(self):
        # Find volumes that will be dangling and not re-attached by
        # docker-compose on container recreation:
        potentially_lost_volumes = []
        for volume in self.volumes:
            if volume.name != None and volume.specified_in_yml:
                # Named volume specified in .yml -> we should fine
                continue
            host_mounted = False
            for mount in volume.mounts:
                if mount.host_path != None:
                    host_mounted = True
                    break
            if host_mounted and volume.specified_in_yml:
                # Host mount specified in .yml -> we should be fine
                continue
            potentially_lost_volumes.append(volume)
        return potentially_lost_volumes

    def launch(self, force_container_recreation=False):
        if self.running:
            return

        # Warn about volumes that might be dangling after recreation
        potentially_lost_volumes = self.potentially_lost_volumes
        if len(potentially_lost_volumes) == 0 or \
                force_container_recreation:
            if len(potentially_lost_volumes) > 0:
                print_msg("some volumes might be potentially dangling, " +\
                    "but container recreation was forced by user",
                    service=self.service.name, container=self.name,
                    color="yellow")
            subprocess.check_call([SystemInfo.docker_compose_path(),
                "rm", "-f", self.name],
                cwd=self.service.service_path)
            subprocess.check_call([SystemInfo.docker_compose_path(),
                "build", self.name],
                cwd=self.service.service_path)
        else:
            for vol in potentially_lost_volumes:
                print_msg("ONLY LIMITED RESTART, CONTAINER WONT BE "+\
                    "UPDATED TO NEWEST IMAGE. " +\
                    "Cannot recreate container safely: " +\
                    "container has a volume that might be lost " +\
                    "or dangling after recreation: "+\
                    "" + str(vol),
                    service=self.service.name,
                    container=self.name, color="yellow")
        subprocess.check_call([SystemInfo.docker_compose_path(),
            "up", "-d", self.name],
            cwd=self.service.service_path)

    def stop(self):
        subprocess.check_call([SystemInfo.docker_compose_path(),
            "stop", self.name], cwd=self.service.service_path)

    @property
    def running(self):
        names = self.service._get_running_service_container_names()
        return (self.name in names)

    @property
    def service(self):
        """ The service this container belongs to. """
        return Service(self.service_name, self.service_path)

    @property
    def default_container_name(self):
        fpath = os.path.normpath(os.path.abspath(self.service_path))
        return (os.path.basename(fpath).replace("-", "")
            + "_" + str(self.name) + "_1")

    @property
    def image_name(self):
        if self._known_image_name != None:
            return self._known_image_name
        return self.default_container_name

    @property
    def current_running_instance_name(self):
        return self.default_container_name

    def _get_active_volume_directories(self, rw_only=False):
        """ Get the volumes active in this container. If the container hasn't
            been started before, this might raise a ValueError since this
            information is obtained via docker inspection of the container.
        """
        try:
            output = subprocess.check_output([SystemInfo.docker_path(),
                "inspect", self.current_running_instance_name],
                stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            raise ValueError("container not created - you might need to "+\
                "launch it first")
        result = json.loads(output.decode("utf-8", "ignore"))
        volumes_list = []
        if not rw_only:
            if "Volumes" in result[0]:
                if result[0]["Volumes"] != None:
                    for volume in result[0]["Volumes"]:
                        volumes_list.append(volume)
            else:
                if result[0]["Config"]["Volumes"] != None:
                    for volume in result[0]["Config"]["Volumes"]:
                        volumes_list.append(volume)
        if rw_only:
            if "Volumes" in result[0]:
                if not ("VolumesRW" in result[0]):
                    return []
                for volume in result[0]["Volumes"]:
                    if not result[0]["VolumesRW"][volume]:
                        volumes_list.remove(volume)
            else:
                if not ("VolumesRW" in result[0]["Config"]):
                    return []
                for volume in result[0]["Config"]["Volumes"]:
                    if not result[0]["Config"]["VolumesRW"][volume]:
                        volumes_list.remove(volume)
        return volumes_list

    def _map_host_dir_to_container_volume_dir(self, volume_dir):
        """ Attempts to find the volume mount information and return the host
            directory currently mapped to the given container volume path.
        """
        try:
            output = subprocess.check_output([SystemInfo.docker_path(),
                "inspect",
                self.current_running_instance_name],
                stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            raise ValueError("container not created - you might need to "+\
                "launch it first")
        result = json.loads(output.decode("utf-8", "ignore"))
        for mount in result[0]["Mounts"]:
            if os.path.normpath(mount["Destination"]) == \
                    os.path.normpath(volume_dir):
                return mount["Source"]
        return None

    def _active_volumes(self, rw_only=False):
        """ Get all the active volumes of this container. May return an empty
            or outdated list if the container is stopped.

            Returns a list of tuples (host file system path,
                container filesystem path).
        """

        try:
            return [(self._map_host_dir_to_container_volume_dir(
                volume), volume) \
                for volume in \
                self._get_active_volume_directories(rw_only=rw_only)]
        except ValueError:
            return []

    def _config_specified_volumes(self, rw_only=False):
        """ Attempt to parse and return all volumes used by this container
            from the according docker-compose.yml.

            Returns a list of tuples (host file system path,
                container filesystem path).
        """
        volumes = []
        def parse_volume_line(parts):
            """ Helper function to parse a single volume line """

            # Check if read-only or not:
            rwro = "rw"
            if len(parts) >= 3:
                if "ro" in [item.strip() for item in parts[2].split(",")]:
                    rwro = "ro"

            # Make sure path is absolute:
            if len(parts) >= 2:
                if (parts[0].startswith("/") or parts[0].startswith("./") or
                        parts[0].startswith("../")) \
                        and not os.path.isabs(parts[0]):
                    parts[0] = os.path.join(
                        os.path.realpath(self.service.service_path), parts[0])
                    parts[0] = os.path.normpath(os.path.abspath(parts[0]))

            # Add to list:
            if len(parts) >= 2 and (rwro == "rw" or not rw_only):
                volumes.append((parts[0], parts[1]))

        # Extract volume lines:
        yml = DockerComposeYml(self.service_path, self.name)
        if "volumes" in yml.data:
            for line in yml.data["volumes"]:
                parse_volume_line(line.split(":"))
        return volumes

    def _get_volumes(self, rw_only=False):
        volumes = []
        def add_volume(vol_line, from_yml=False):
            host_mount_path = None
            known_name = None
            if vol_line[0] != None:
                if vol_line[0].startswith("/") or \
                        vol_line[0].startswith("./") or \
                        vol_line[0].startswith("../"):
                    host_mount_path = vol_line[0]
                else:
                    known_name = vol_line[0]
            vol = DataVolume(known_host_mount=host_mount_path,
                id=known_name, name=known_name)
            vol._set_owning_container(self, vol_line[1])
            vol.specified_in_yml = from_yml
            volumes.append(vol)

        volumes_in_yml_by_normpath = set()
        vol_lines = {}
        for vol1 in self._active_volumes(rw_only=rw_only):
            if not vol1[0] in vol_lines or vol_lines[vol1[0]] == None:
                vol_lines[os.path.normpath(vol1[0])] = vol1[1]
        for vol2 in self._config_specified_volumes(rw_only=rw_only):
            if not vol2[0] in vol_lines or vol_lines[vol2[0]] == None:
                vol_lines[os.path.normpath(vol2[0])] = vol2[1]
            volumes_in_yml_by_normpath.add(os.path.normpath(vol2[0]))
        for vol_line_part0 in vol_lines:
            if os.path.normpath(vol_line_part0) in\
                    volumes_in_yml_by_normpath:
                add_volume((vol_line_part0, vol_lines[vol_line_part0]),
                    from_yml=True)
            else:
                add_volume((vol_line_part0, vol_lines[vol_line_part0]))
        return volumes

    @property
    def volumes(self):
        """ Returns a list of DataVolume instances representing all volumes
            which are in any sort of known connection to this container.
        """
        return self._get_volumes(rw_only=False)

    @property
    def rw_volumes(self):
        """ Returns a list of DataVolume instances representing all volumes
            which are in any sort of known connection to this container.
            Limited to all the volumes that are actually writable for the
            container, excluding the read-only ones.
        """
        return self._get_volumes(rw_only=True)

    @property
    def dependencies(self):
        # Collect all external_links and links container references:
        external_links = self.service._external_docker_compose_links(
            self.name)
        internal_links = self.service._internal_docker_compose_links(
            self.name)
        dependencies = []

        # Go through all external links
        for link in external_links:
            link_name = link.partition(":")[0]
            target_found = False
            # See if we can find the target service:
            for service in Service.all():
                for container in service.containers:
                    if (service == self.service and 
                            link_name == container.name) or \
                            link_name == container.default_container_name:
                        target_found = True
                        dependencies.append(ServiceDependency(
                            service.name, service.service_path,
                            container.name))
            if not target_found:
                dependencies.append(ServiceDependency(
                    None, None, link_name))

        # Go through all internal links:
        for link in internal_links:
            link_name = link.partition(":")[0]
            target_found = False
            # See if we can find the target service:
            for container in self.service.containers:
                 if link_name == container.name:
                    target_found = True
                    dependencies.append(ServiceDependency(
                        self.service_name, self.service_path,
                        container.name))
            if not target_found:
                dependencies.append(ServiceDependency(
                    None, None, link_name))
        return dependencies

class Service(object):
    """ This class holds all the info and helper functionality for managing
        a service, whereas a service is a group of containers with one single
        docker-compose.yml stored in a subfolder in the global service
        directory.
    """
    def __init__(self, service_name, service_path):
        self.name = service_name
        self.service_path = service_path

    def __repr__(self):
        return self.name + " service at " + str(self.service_path)

    def __eq__(self, other):
        if not hasattr(other, "name") and not hasattr(other,
                "service_path"):
            return False
        if self.name == other.name:
            if os.path.normpath(os.path.abspath(self.service_path)) == \
                    os.path.normpath(os.path.abspath(other.service_path)):
                return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name + "/" + os.path.normpath(
            os.path.abspath(self.service_path)))

    def is_running(self):
        return len(self._get_running_service_container_names()) > 0

    @staticmethod
    def find_by_name(name):
        for service in Service.all():
            if service.name == name:
                return service
        return None

    @staticmethod
    def all_containers():
        result = set()
        for service in Service.all():
            for container in service.containers:
                result.add(container)
        return list(result)

    @staticmethod
    def all():
        """ Get a global list of all detected services.
        """
        services = []
        scanned = set()
        def scan_dir(d):
            """ Scan a given directory for containers. """

            if not os.path.exists(d):
                return

            # Only scan directories:
            if not os.path.isdir(d):
                return

            # Duplicate avoidance:
            normd = os.path.normpath(os.path.abspath(
                os.path.realpath(d)))
            if normd in scanned:
                return
            scanned.add(normd)

            # Scan it:
            d = os.path.abspath(d)
            for f in os.listdir(d):
                if not os.path.isdir(os.path.join(d, f)):
                    continue
                if not os.path.exists(os.path.join((os.path.join(d, f)),
                        "docker-compose.yml")):
                    continue
                services.append(Service(f, 
                    os.path.normpath(os.path.join(d, f))))

        # Schedule some common places for scanning:
        scan_dir(os.getcwd())
        scan_dir("/usr/share/docker-services")
        scan_dir("/srv")
        return services

    def _fix_container_name(self, container_name):
        """ !! BIG HACK !!
            Sometimes docker-compose gives us just a shortened name for a
            container. While I am not fully aware of the algorithm, I assume
            it will usually still be unique. In this function, we try to get
            back the full unshortened name.
        """
        assert(container_name != None)
        for container in self.containers:
            if container.current_running_instance_name == container_name \
                    or container.default_container_name == container_name:
                return container.name
        matched_name = None
        for container in self.containers:
            if container.current_running_instance_name.startswith(
                    container_name):
                if matched_name != None:
                    raise RuntimeError("encountered unexpected non-unique " +\
                        "container label trying to find the full name for "+\
                         "\"" + str(container_name) + "\": variant A: " +\
                        str(matched_name) + ", variant B: " +\
                        str(container.name))
                matched_name = container.name
        return matched_name

    def _get_running_service_container_names(self):
        """ Get all running containers of the given service.
            Returns a list of container ids.
        """
        running_containers = []
        try:
            env = os.environ.copy()
            env["COLUMNS"] = "200"
            output = SubprocessHelpers.check_output_with_isolated_pty([
                SystemInfo.docker_compose_path(), "ps"],
                cwd=self.service_path, stderr=subprocess.STDOUT,
                timeout=10, env=env).\
                decode("utf-8", "ignore")
        except subprocess.CalledProcessError as e:
            output = e.output.decode("utf-8", "ignore")
            if output.find("client and server don't have same version") >= 0:
                print_msg("error: it appears docker-compose is " +\
                    "installed with " +\
                    "a version incompatible to docker.", color="red")
                sys.exit(1)
            raise e
        output = output.\
            replace("\r", "\n").replace("\n\n", "").split("\n")

        skipped_past_dashes = False
        for output_line in output:
            if len(output_line.strip()) == 0:
                continue
            if output_line.startswith("---------"):
                skipped_past_dashes = True
                continue
            output_line = output_line.strip()
            if len(output_line) > 0 and output_line.find(" Up") > 0:
                # this is a running container!
                space_pos = output_line.find(" ")
                running_containers.append(output_line[:space_pos])
        l = [self._fix_container_name(container) \
            for container in running_containers]
        return [name for name in l if name is not None]

    def _internal_docker_compose_links(self, container_name):
        """ Attempt to parse and return all containers referenced as external
            links used by a service from the respective docker-compose.yml of
            that service.
        """
        links = []

        # Get docker-compose.yml data:
        yml = DockerComposeYml(self.service_path, container_name)
        if yml.data == None:
            raise ValueError("no such container found: " +\
                str(container_name))

        # Extract link data:
        if not "links" in yml.data:
            return []
        return [entry.partition(":")[0] for entry in \
            yml.data["links"]]

    def _external_docker_compose_links(self, container_name):
        """ Attempt to parse and return all containers referenced as external
            links used by a service from the respective docker-compose.yml of
            that service.
        """
        external_links = []
        yml = DockerComposeYml(self.service_path, container_name)
        if yml.data == None:
            raise ValueError("no such container found: " +\
                str(container_name))
        if not "external_links" in yml.data:
            return []
        return [entry.partition(":")[0] for entry in \
            yml.data["external_links"]]

    def get_running_containers(self):
        """ Get only the containers of this service which are currently up and
            running.
        """
        running_names = self._get_running_service_container_names()
        running_containers = []
        for container in self.containers:
            if container.name in running_names:
                running_containers.append(container)
        return running_containers

    @property
    def rw_volumes(self):
        volume_set = set()
        for container in self.containers:
            for volume in container.rw_volumes:
                volume_set.add(volume)
        return list(volume_set)

    @staticmethod
    def global_clean_up(ask=True):
        """ This function will check the status of all docker containers, and
            then irrevocably delete all containers that aren't running.
            It will also delete all dangling volumes.
        """
        if ask:
            answer = input("\033[1m!! DANGER !!\033[0m\n"+\
                "This will irrevocably delete all "+\
                "stopped containers. "+\
                "It will also delete all volumes that are neither "+\
                "a host-mounted directory, nor are currently owned by any "+\
                "currently existing container (in short, all dangling "+\
                "volumes).\n\n(Avoid this warning next time " +\
                "with --force)\n\n" +\
                "Are you sure you want to continue? [Enter y/N]")
            if not answer == "y" and not answer == "Y":
                print("drudder: error: cleaning was aborted "+\
                    "by user.", file=sys.stderr)
                sys.exit(1)
        print_msg("cleaning up stopped containers...")
        output = subprocess.check_output([
            SystemInfo.docker_path(), "ps", "-a"])
        output = output.decode("utf-8", "ignore").\
            replace("\r", "\n").\
            replace("\n ", "\n").replace(" \n", "\n").replace("\n\n", "\n")
        while output.find("   ") >= 0:
            output = output.replace("   ", "  ")
        output = output.replace("  ", "\t")
        output = output.replace("\t ", "\t").replace(" \t", "\t")
        output = output.split("\n")
        for output_line in output:
            if len(output_line.strip()) == 0:
                continue
            parts = output_line.split("\t")
            if parts[0] == "CONTAINER ID":
                continue
            if len(parts) < 5 or (not parts[3].endswith("ago")):
                print_msg("WARNING: skipping container " + parts[0] +\
                    ", cannot locate STATUS column")
                continue
            if parts[0].find(" ") >= 0:
                print_msg("WARNING: skipping container with invalid " +\
                    "container id: " + parts[0])
                continue
            if len(parts) == 6 and parts[4].find("_") >= 0:
                parts = parts[:4] + [ '' ] + parts[4:]
            if parts[4] == "" or parts[4].startswith("Exited "):
                print_msg("deleting stopped container " + parts[0] + "...")
                subprocess.check_output([SystemInfo.docker_path(),
                    "rm", parts[0]])
        print_msg("cleaning up unneeded images...")
        subprocess.check_output([SystemInfo.docker_path(), "rmi",
            "$(docker images -aq"], shell=True)
        print_msg("cleaning up dangling volumes...")
        dangling_vols = subprocess.check_output([SystemInfo.docker_path(),
            "volume", "ls", "-qf", "dangling=true"])
        for vol in dangling_vols.splitlines():
            vol = vol.strip()
            if len(vol) == 0:
                continue
            subprocess.check_output([SystemInfo.docker_path(),
                "volume", "rm", vol])

    @property
    def containers(self):
        """ Get all containers specified for the given service's
            docker-compose.yml.
        """
        results = []

        # Assemble results:
        yml = DockerComposeYml(self.service_path)
        for container_name in yml.services:
            if not isinstance(yml.services[container_name], dict):
                continue
            for property_name in yml.services[container_name]:
                value = yml.services[container_name][property_name]
                if property_name == "image":
                    # This container is constructed from an image:
                    image_name = value
                    results.append(ServiceContainer(
                        self.name, self.service_path,
                        container_name,
                        image_name=image_name))
                elif property_name == "build":
                    # Built from a directory with Dockerfile:
                    results.append(ServiceContainer(
                        self.name, self.service_path,
                        container_name))
        return results

class FailedLaunchTracker(object):
    def __init__(self):
        self.access_lock = threading.Lock()
        self.contents = set()

    def __len__(self):
        self.access_lock.acquire()
        result = len(self.contents)
        self.access_lock.release()
        return result

    def __contains__(self, item):
        self.access_lock.acquire()
        result = (item in self.contents)
        self.access_lock.release()
        return result

    def add(self, item):
        self.access_lock.acquire()
        self.contents.add(item)
        self.access_lock.release()

class LaunchThreaded(threading.Thread):
    """ A helper to launch a service and wait for the launch only for a
        limited amount of time, and moving the launch into a background
        thread if it takes too long.
    """
    
    def __init__(self, container, failed_launch_tracker=None,
            force_container_recreation=False, do_on_success=None,
            do_on_failure=None):
        super().__init__()
        self.container = container
        self.failed_launch_tracker = failed_launch_tracker
        self.path = self.container.service.service_path
        self.force_container_recreation = force_container_recreation
        def do_nothing(self):
            pass
        self.do_on_success = do_on_success
        if self.do_on_success is None:
            self.do_on_success = do_nothing
        self.do_on_failure = do_on_failure
        if self.do_on_failure is None:
            self.do_on_failure = do_nothing

    def run(self):
        try:
            # Fix permissions if we have instructions for that:
            perms = Permissions(self.container.service)
            perm_info = perms.get_permission_info_from_yml()
            if ("owner" in perm_info["livedata-permissions"]) \
                    and os.path.exists(os.path.join(
                        self.path, "livedata")):
                owner = perm_info["livedata-permissions"]["owner"]
                try:
                    owner = int(owner)
                except TypeError:
                    # Must be a username.
                    try:
                        owner = getpwnam(owner).pw_uid
                    except KeyError:
                        print_msg("invalid user specified for permissions: "+\
                            "can't get uid for user: " + owner, color="red")
                        raise RuntimeError("invalid user")
                for root, dirs, files in os.walk(os.path.join(self.path, \
                        "livedata")):
                    for f in (dirs + files):
                        fpath = os.path.join(root, f)
                        os.chown(fpath, owner, -1, follow_symlinks=False)

            # Get dependencies and see if they have all been launched:
            waiting_msg = False
            for dependency in self.container.dependencies:
                if not dependency.container.running:
                    if not waiting_msg:
                        waiting_msg = True
                    time.sleep(5)
                    while not dependency.container.running:
                        if self.failed_launch_tracker != None:
                            if dependency.container in \
                                    self.failed_launch_tracker:
                                print_msg("launch aborted due to failed " +\
                                    "dependency launch: " +\
                                    str(dependency),
                                    service=self.container.service.name,
                                    container=self.container.name,
                                    color="red")
                                self.failed_launch_tracker.add(
                                    self.container)
                                self.do_on_failure()
                                return
                        time.sleep(5)

            # Launch the service:
            print_msg("launching...", service=self.container.service.name,
                container=self.container.name, color="blue")
            try:
                self.container.launch(
                    force_container_recreation=\
                    self.force_container_recreation)
                time.sleep(1)
                if not self.container.running:
                    print_msg("failed to launch. (nothing running after " +\
                        "1 second)",\
                        service=self.container.service.name,
                        container=self.container.name, color="red")
                    if self.failed_launch_tracker != None:
                        self.failed_launch_tracker.add(self.container)
                    self.do_on_failure()
                    return
                print_msg("now running.",
                    service=self.container.service.name,
                    container=self.container.name, color="green")
                self.do_on_success()
            except subprocess.CalledProcessError:
                print_msg("failed to launch. (error exit code)",\
                    service=self.container.service.name,
                    container=self.container.name,
                    color="red")
                if self.failed_launch_tracker != None:
                    self.failed_launch_tracker.add(self.container)
                self.do_on_failure()
            except Exception as e:
                print_msg("failed to launch. (unknown error)",\
                    service=self.container.service.name,
                    container=self.container.name,
                    color="red")
                if self.failed_launch_tracker != None:
                    self.failed_launch_tracker.add(self.container)
                self.do_on_failure()
                raise e
        except Exception as e:
            print("UNEXPECTED ERROR", file=sys.stderr)
            print("ERROR: " + str(e))
            traceback.print_exc()

    @staticmethod
    def attempt_launch(container, to_background_timeout=5,
            failed_launch_tracker=None, force_container_recreation=False,
            do_on_success=None, do_on_failure=None):
        """ Launch a given service and wait for it to run for a few seconds.
            If that isn't long enough for it to start running, return
            execution to possibly launch further services while this one is
            still busy launching.
        """

        # Start a new launch thread:
        launch_t = LaunchThreaded(container,
            failed_launch_tracker=failed_launch_tracker,
            do_on_success=do_on_success, do_on_failure=do_on_failure,
            force_container_recreation=force_container_recreation)
        launch_t.start()
        
        # Wait for it to complete:
        launch_t.join(to_background_timeout)
        if launch_t.isAlive():
            # This took too long, run in background:
            return launch_t
        return None

    @staticmethod
    def stop(container):
        """ Stop a service. """
        assert(container != None)
        container.stop()

    @staticmethod
    def wait_for_launches(threads):
        for launch_t in threads:
            if launch_t.isAlive():
                launch_t.join()

class Permissions:
    """ This represents permissions information parsed from a permssion.yml.
        This allows the administrator to specify permissions that should be
        applied to the live data folder (where usually read-write volumes are
        mounted).
    """
    def __init__(self, service):
        self.service = service

    def get_permission_info_from_yml(self):
        """ Get permission info for the given service
        """
        f = None
        try:
            f = open(os.path.join(self.service.service_path,
                "permissions.yml"), "rb")
        except Exception as e:
            return {"livedata-permissions" : {}}
        perm_dict = dict()
        current_area = None
        try:
            contents = f.read().decode("utf-8").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")
        finally:
            f.close()

        # Extract permission info from the YAML:
        parsed_obj = yaml.safe_load(contents)
        for k in parsed_obj:
            if k in set(["livedata-permissions"]):
                if isinstance(parsed_obj[k], dict):
                    perm_dict[k] = parsed_obj[k]
            else:
                print_msg("warning: unrecognized permissions.yml " +\
                    "section: " + str(k),
                    service=self.service.name,
                    color="red")

        # Make sure some stuff is present:
        if not "livedata-permissions" in perm_dict:
            perm_dict["livedata-permissions"] = dict()

        return perm_dict

parser = argparse.ArgumentParser(description=textwrap.dedent('''\
    drudder: a tool for managing a larger collection of
    docker-compose.yml-specified container groups simply.'''
    ),
    formatter_class=DoubleLineBreakFormatter)
parser.add_argument("action",
    help=textwrap.dedent('''\
    Possible values:
    
    "list": list all known services.

    "start": start the service specified as argument (or "all" for all).

    "stop": stop the service specified as argument (or "all" for all).

    "restart": restart the service specified as argument (or "all" for all).

    "info": show more detailed info for the given service (or "all for
            all).

    "logs": output the logs of all the docker containers of the service
            specified as argument (or "all" for all).

    "shell": start an interactive shell in the specified service's specified
             subservice. The argument is:
               <service>[/<subservice>]
               e.g.: drudder shell myownservice/datcoolsubservice
             The subservice can be omitted from the argument if the
             docker-compose.yml has just one container/subservice.

    "snapshot": store an atomic snapshot of the live data of the service
                specified as argument (from livedata/) in livedata-snapshots/

    "clean": clean up all stopped containers and all dangling volumes.
             THIS IS NOT REVERSIBLE. The docker images won't be touched.

    "dump-container-info" : dump all sorts of detailed container info as YAML.
                            Mainly useful if you want to process the combined
                            data computed by drudder with another tool
                            of yours.

    "install-tools" : install all required components and tools to run drudder
                      on this computer
    ''')

    )
parser.add_argument("argument", nargs="*", help="argument(s) to given action")
parser.add_argument("--version", help="show version and quit",
    default=False, action="store_true",
    dest="show_version")
parser.add_argument("--force",
    default=False, action="store_true",
    help="Can be used to override various safety warnings. Consult the "+\
        "respective warning prompted by an action to understand the "+\
        "consequences",
    dest="force")
if len(" ".join(sys.argv[1:]).strip()) == 0:
    parser.print_help()
    sys.exit(1)
for arg in sys.argv[1:]:
    if arg == "--version" or arg == "-v" or arg == "-V":
        print("drudder version " + str(TOOL_VERSION))
        sys.exit(0)
args = parser.parse_args()

ensure_docker = SystemInfo.docker_path()
ensure_docker_compose = SystemInfo.docker_compose_path()

class Snapshots(object):
    def __init__(self, service):
        self.service = service

    def check_running_snapshot_transaction(self):
        if os.path.exists(os.path.join(
                self.service.service_path, ".drudder-snapshot.lock")):
            output = subprocess.check_output(
                "ps aux | grep drudder | grep -v grep | wc -l",
                shell=True).decode("utf-8", "ignore")
            if output.strip() != "1":
                # Another copy still running??
                return True
            print_msg("warning: stale snapshot lock found but no process " +\
                "appears to be left around, removing.",
                color="yellow", service=self.service.name)
            # No longer running, remove file:
            os.remove(os.path.join(self.service.service_path,
                ".drudder-snapshot.lock"))
        return False

    @staticmethod
    def btrfs_tool_check():
        # Make sure the btrfs tool is working:
        if SystemInfo.btrfs_path() == None:
            print_msg("error: btrfs tool not found. " +\
                "Are btrfs-progs installed?",
                color="red")
            sys.exit(1)
        output = None
        try:
            output = subprocess.check_output([SystemInfo.btrfs_path(),
                "--version"],
                stderr=subprocess.STDOUT).decode("utf-8", "ignore")
        except subprocess.CalledProcessError as e:
            output = e.output.decode("utf-8", "ignore")
        if not output.lower().startswith("btrfs-progrs ") and \
                not output.lower().startswith("btrfs-progs ") and \
                not output.lower().startswith("btrfs "):
            print_msg("error: btrfs tool returned unexpected string. Are " +\
                "btrfs-progrs installed and working?",
                color="red")
            print_msg("Full btrfs tool output was: " + str(output))
            sys.exit(1)

    def subvolume_readiness_check(self, print_errors=True):
        """ Check if the given service is ready for snapshotting or still
            needs btrfs subvolume conversion. Print a warning if not.
        """
        fs = SystemInfo.filesystem_type_at_path(
            self.service.service_path)
        if fs != "btrfs":
            return False

        self.btrfs_tool_check()

        if os.path.exists(os.path.join(self.service.service_path,
                "livedata")):
            try:
                if not SystemInfo.is_btrfs_subvolume(os.path.join(
                        self.service.service_path, "livedata"))\
                        and len(self.service.rw_volumes) > 0:
                    if self.service.is_running():
                        if print_errors:
                            print_msg(
                                "the livedata/ dir of this service will " +\
                                "still need to be converted to " +\
                                "a subvolume to enable snapshots.\n" + \
                                "Fix it by doing this:\n" + \
                                "1. Stop the service with: drudder "+\
                                    "stop " +\
                                    self.service.name + "\n" + \
                                "2. Snapshot the service once with: " +\
                                    "drudder "+\
                                    "snapshot " + self.service.name + "\n",
                                service=self.service.name, color="yellow")
                    else:
                        if print_errors:
                            print_msg("the livedata/ dir of " +\
                                "this service still " +\
                                "needs conversion to btrfs subvolume.\n" +\
                                "Fix it by snapshotting it once with: " +\
                                "drudder "+\
                                "snapshot " + self.service.name + "\n",
                                service=self.service.name, color="yellow")
                    return False
            except ValueError:
                print_msg("there was a problem. btrfs snapshotting won't " +\
                          "work as intended.", \
                          service=self.service.name, color="red")
                return False
        # Everything seems fine so far.
        return True

    def do(self):
        """ Make a backup of the live data of the service.
        """

        self.btrfs_tool_check()

        # Make sure no snapshot is already in progress:
        if self.check_running_snapshot_transaction():
            print_msg("error: snapshot already in progress. " +\
                "try again later", service=self.service.name,
                color="red")
            print_msg("remove .drudder-snapshot.lock if you " +\
                "are sure that is incorrect", service=self.service.name)
            return False

        print_msg("considering for snapshot...",
            service=self.service.name, color="blue")

        # Check which volumes this service has:
        rw_volumes = set()
        for container in self.service.containers:
            for volume in container.rw_volumes:
                rw_volumes.add(volume)
        rw_volumes = list(rw_volumes)
        if len(rw_volumes) == 0:
            print_msg("service has no read-write volumes, nothing to do.",
                service=self.service.name, color="blue")
            return True
        
        # Check if we have livedata/:
        if not os.path.exists(os.path.join(
                self.service.service_path, "livedata")):
            print_msg("error: service has read-write volumes, " + \
                "but no livedata/ " +\
                "folder. Fix this to enable snapshots",
                service=self.service.name,
                color="red")
            return False

        # Check if we have any volumes which are actually in livedata/:
        empty_snapshot = True
        for volume in rw_volumes:
            for mount in volume.mounts:
                if mount.container == None:
                    continue
                if mount.container.service != self.service:
                    continue
                if mount.host_path == None:
                    # This volume mount is not a host mount!!
                    print_msg("warning: volume " + str(volume) + \
                        " used by " + str(mount.container) +\
                        " is NOT a host mount in livedata/" +\
                        " and won't be covered by the snapshot",
                        service=self.service.name, color="yellow")
                relpath = os.path.relpath(
                    os.path.realpath(mount.host_path),
                    os.path.realpath(os.path.join(
                    self.service.service_path, "livedata")),
                )
                if relpath.startswith(os.pardir + os.sep):
                    # This volume mount is not in livedata/!
                    print_msg("warning: volume " + str(volume) + \
                        " used by " + str(mount.container) +\
                        " is a host mount that is NOT mounted in" +\
                        " the livedata/ folder" +\
                        " and won't be covered by the snapshot: " +\
                        str(mount),
                        service=self.service.name, color="yellow")
                else:
                    empty_snapshot = False
        if empty_snapshot:
            print_msg("this snapshot would be empty because no read-write " +\
                "volumes are mounted to livedata/ - skipping.",
                service=self.service.name, color="blue")
            return True

        # Check if filesystem of livedata/ is actually btrfs:
        fs = SystemInfo.filesystem_type_at_path(
            os.path.join(self.service.service_path, "livedata"))
        if fs != "btrfs":
            print_msg("error: livedata/ has other filesystem " + str(fs) + \
                ", should be btrfs!")
            return fs

        actual_path = os.path.realpath(self.service.service_path)
        livedata_renamed_dir = os.path.join(
            actual_path, ".livedata-predeletion-renamed")
        livedata_dir = os.path.join(
            actual_path, "livedata")
        snapshot_dir = os.path.join(
            actual_path, ".btrfs-livedata-snapshot")
        tempvolume_dir = os.path.join(
            actual_path, ".btrfs-livedata-temporary-volume")
        tempdata_dir = os.path.join(
            actual_path, ".livedata-temporary-prevolume-copy")

        # Make sure the livedata/ dir is a btrfs subvolume:    
        if self.service.is_running():
            if not SystemInfo.is_btrfs_subvolume(livedata_dir):
                print_msg("error: can't do btrfs subvolume " +\
                    "conversion because "+\
                    "service is running. The first snapshot is " +\
                    "required to " +\
                    "be done when the service is stopped.",
                    service=self.service.name, color="red")
                return False

        lock_path = os.path.join(self.service.service_path,
            ".drudder-snapshot.lock")

        # Check there is still no transaction running:
        if self.check_running_snapshot_transaction():
            print_msg("error: snapshot already in progress. " +\
                "try again later", service=self.service.name,
                color="red")
            print_msg("remove .drudder-snapshot.lock if you are " +\
                "sure this is incorrect", service=self.service.name)
            return False

        # Add a transaction lock:
        transaction_id = str(uuid.uuid4())
        with open(lock_path, "wb") as f:
            f.write(transaction_id.encode("utf-8"))
        
        # Wait a short amount of time so other race condition writes will
        # be finished with a very high chance:
        time.sleep(0.5)

        # Verify we got the transaction lock:
        contents = None
        with open(lock_path, "rb") as f:
            contents = f.read().decode("utf-8", "ignore")
        if contents.strip() != transaction_id:
            print_msg("error: mid-air snapshot collision detected!! " + \
                "Did you call the script twice?",
                service=self.service.name, color="red")
            return False

        # Make sure the .livedata-predeletion-renamed isn't there:
        if os.path.exists(livedata_renamed_dir):
            if not os.path.exists(livedata_dir):
                print_msg("warning: .livedata-predeletion-renamed/ " + \
                    "is present and no livedata/ folder." +\
                    "Moving it back...",
                    service=self.service.name, color="yellow")
                shutil.move(livedata_renamed_dir, livedata_dir)
                assert(not os.path.exists(livedata_renamed_dir))
            else:
                print_msg("error: .livedata-predeletion-renamed/ " + \
                    "is still there, indicating a previously aborted " +\
                    "run, but livedata/ is also still around. " +\
                    "Please figure out which one you want to keep, and " +\
                    "delete one of the two.", service=self.service.name,
                    color="red")
                sys.exit(1)

        # Make sure the .livedata-temporary-prevolume-copy directory is unused:
        if os.path.exists(tempdata_dir):
            print_msg("warning: .livedata-temporary-prevolume-copy/ " + \
                "already present! " +\
                "This is probably a leftover from a previously " + \
                "aborted attempt. Will now attempt to delete it...",
                service=self.service.name, color="yellow")
            shutil.rmtree(tempdata_dir)
            assert(not os.path.exists(tempdata_dir))

        # Make sure the btrfs snapshot path is unused:
        if os.path.exists(snapshot_dir):
            if SystemInfo.btrfs_subvolume_stat_check(snapshot_dir):
                print_msg("warning: .btrfs-livedata-snapshot/ " \
                    + "already present! " \
                    + "This is probably a leftover from a previously " + \
                    "aborted attempt. Will now attempt to delete it...",
                    service=self.service.name, color="yellow")
                subprocess.check_output([SystemInfo.btrfs_path(),
                    "subvolume",
                    "delete", snapshot_dir])
                assert(not os.path.exists(snapshot_dir))
            else:
                print_msg("error: .btrfs-livedata-snapshot/ already " +\
                    "present, " \
                    + "but it is not a btrfs snapshot!! I don't know how " +\
                    "to deal with this, aborting.",
                    service=self.service.name, color="red")
                return False

        # Make sure the temporary btrfs subvolume path is unused:
        if os.path.exists(tempvolume_dir):
            print_msg("warning: .btrfs-livedata-temporary-volume/ already " +\
                "present! " \
                + "This is probably a leftover from a previously " + \
                "aborted attempt. Will now attempt to delete it...",
                service=self.service.name, color="yellow")
            output = subprocess.check_output([SystemInfo.btrfs_path(),
                "subvolume",
                "delete", tempvolume_dir])
            assert(not os.path.exists(tempvolume_dir))

        # If this isn't a btrfs subvolume, we will need to fix that first:
        if not SystemInfo.is_btrfs_subvolume(livedata_dir):
            print_msg("warning: initial subvolume conversion required. "+\
                "DON'T TOUCH livedata/ WHILE THIS HAPPENS!!",
                service=self.service.name, color="yellow")
            try:
                output = subprocess.check_output([SystemInfo.btrfs_path(),
                    "subvolume",
                    "create", tempvolume_dir])
            except Exception as e:
                os.remove(lock_path)
                raise e
            assert(SystemInfo.is_btrfs_subvolume(tempvolume_dir))

            # Copy all contents:
            assert(not os.path.exists(tempdata_dir))
            try:
                shutil.copytree(os.path.realpath(livedata_dir), tempdata_dir, symlinks=True)
            except shutil.Error as e:
                # Check for errors we care about:
                relevant_errors = []
                for err in e.args[0]:
                    src, dst, msg = err
                    if str(msg).startswith("[Errno 6]"):
                        # This occurs with Unix socekts. Check if it is one:
                        mode = os.stat(src).st_mode
                        if stat.S_ISSOCK(mode):
                            continue # It is. Skip it!
                    # This is an unknown error we most likely care about:
                    relevant_errors.append(err)
                if len(relevant_errors) > 0:
                    e.args[0] = relevant_errors
                    raise e

            assert(os.path.exists(tempdata_dir))
            for f in os.listdir(tempdata_dir):
                orig_path = os.path.join(tempdata_dir, f)
                new_path = os.path.join(tempvolume_dir, f)
                shutil.move(orig_path, new_path)

            # Do a superficial check if we copied all things:
            copy_failed = False
            for f in os.listdir(tempvolume_dir):
                if not os.path.exists(os.path.join(livedata_dir, f)):
                    copy_failed = True
                    break
            for f in os.listdir(livedata_dir):
                if not os.path.exists(os.path.join(tempvolume_dir, f)):
                    copy_failed = True
                    break
            if copy_failed:
                print_msg("error: files of old livedata/ directory and "+\
                    "new subvolume do not match. Did things get changed "+\
                    "during the process??",
                    service=self.service.name, color="red")
                return False

            # Remove old livedata/ dir:
            propagate_interrupt = None
            while True:
                try:
                    shutil.move(livedata_dir, livedata_renamed_dir)
                    shutil.move(tempvolume_dir, livedata_dir)
                    shutil.rmtree(livedata_renamed_dir)
                    break
                except KeyboardInterrupt as e:
                    propagate_interrupt = e
                    continue
            if propagate_interrupt != None:
                raise propagate_interrupt
            print_msg("conversion of livedata/ to btrfs subvolume complete.",
                service=self.service.name)

        snapshots_dir = os.path.join(self.service.service_path,
            "livedata-snapshots")

        # Create livedata-snapshots/ if not present:
        if not os.path.exists(snapshots_dir):
            os.mkdir(snapshots_dir)

        # Go ahead and snapshot:
        print_msg("initiating btrfs snapshot...",
            service=self.service.name)
        output = subprocess.check_output([SystemInfo.btrfs_path(),
            "subvolume", "snapshot",
            "-r", "--", livedata_dir, snapshot_dir])
        
        # Copy snapshot to directory:
        now = datetime.datetime.now()
        snapshot_base_name = str(now.year)
        if now.month < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.month)
        if now.day < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.day)
        snapshot_base_name += "-"
        if now.hour < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.hour)
        if now.minute < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.minute)
        if now.second < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.second)
        snapshot_name = snapshot_base_name + "00"
        i = 1
        while os.path.exists(os.path.join(snapshots_dir, snapshot_name)):
            snapshot_name = snapshot_base_name
            if i < 10:
                snapshot_name += "0"
            snapshot_name += str(i)
            i += 1
        snapshot_specific_dir = os.path.join(snapshots_dir,
            snapshot_name)
        print_msg("copying to " + snapshot_specific_dir,
            service=self.service.name)
        shutil.copytree(snapshot_dir, snapshot_specific_dir, symlinks=True)
        subprocess.check_output([SystemInfo.btrfs_path(), "subvolume",
            "delete", snapshot_dir])
        assert(not os.path.exists(snapshot_dir))
        print_msg("snapshot complete.", service=self.service.name,
            color="green")

        # Remove lock file:
        assert(os.path.exists(lock_path) and not os.path.isdir(lock_path))
        os.remove(lock_path)
        assert(not os.path.exists(lock_path))

        return True

class TargetsParser(object):
    """ A helper class to parse user input and turn it into container or
        services lists.
    """
    @staticmethod
    def split_targets(targets):
        # Split up the targets:
        return targets.strip().split(" ")

    @staticmethod
    def get_containers(targets, print_error=False):
        while targets.find("  ") >= 0:
            targets.replace("  ", " ")
        result = set()

        targets = TargetsParser.split_targets(targets)

        # Go through all targets in the list:
        for target in targets:
            negated = False
            if target.startswith("+"):
                target = target[1:]
            elif target.startswith("-"):
                target = target[1:]
                negated = True

            # Specific treatment of the "all" keyword:
            if target == "all":
                for service in Service.all():
                    for container in service.containers:
                        if not negated:
                            result.add(container)
                        else:
                            try:
                                result.remove(container)
                            except KeyError:
                                pass
                continue

            # Examine the current service/container entry:
            service_name = target.partition("/")[0]
            service = Service.find_by_name(service_name)
            if service == None:
                if print_error:
                    print("drudder: error: " + \
                        "no such service found: " + str(service_name),
                        file=sys.stderr)
                return None

            # See if a specific container is specified or jst all of them:
            container_name = target.partition("/")[2]
            if len(container_name) == 0: # all containers:
                containers = service.containers
                if len(containers) == 0:
                    print_msg("warning: specified service has no " +\
                        "containers", color="yellow", service=service.name)
            else: # a specific container. find it by name:
                containers = []
                for service_container in service.containers:
                    if service_container.name == container_name:
                        containers = [ service_container ]
                        break
                # Check if the container was found by name or not:
                if len(containers) == 0:
                    if print_error:
                        print("drudder: error: " + \
                            "no such container for service \"" +\
                            str(service_name) + "\" found: " +\
                            str(container_name),
                            file=sys.stderr)
                    return None
            # Add/remove all containers collected by this entry:
            for container in containers:
                if not negated:
                    result.add(container)
                else:
                    try:
                        result.remove(container)
                    except KeyError:
                        pass
        return list(result)

    @staticmethod
    def get_services(targets, print_error=False):
        while targets.find("  ") >= 0:
            targets.replace("  ", " ")
        result = set()
        targets = TargetsParser.split_targets(targets)

        # Go through all targets in the list:
        for target in targets:
            negated = False
            if target.startswith("+"):
                target = target[1:]
            elif target.startswith("-"):
                target = target[1:]
                negated = True

            # Specific treatment of the "all" keyword:
            if target == "all":
                for service in Service.all():
                    if not negated:
                        result.add(service)
                    else:
                        try:
                            result.remove(service)
                        except KeyError:
                            pass
                return list(result)
            service = Service.find_by_name(target.partition("/")[0])
            if service == None:
                if print_error:
                    print("drudder: error: " + \
                        "no such service found: " + str(
                        target.partition("/")[0]),
                        file=sys.stderr)
                return None
            if not negated:
                result.add(service)
            else:
                try:
                    result.remove(service)
                except KeyError:
                    pass
        return list(result)

class ContainerLinkDependencyNode(object):
    """ A node in the dependency graph of container dependencies.
        Has incoming and outgoing link edges, and has an associated container.
    """
    def __init__(self, container, incoming_links, outgoing_links, graph):
        self.incoming_links = incoming_links
        self.outgoing_links = outgoing_links
        self.container = container
        self.graph = graph

    def __repr__(self):
        return "<ContainerLinkDependencyNode " + str(self.container) +\
            " (" +\
            ", ".join([str(self.container) + " -> " + str(link.container)\
                for link in self.outgoing_links]) + ")>"

    def __hash__(self):
        return hash(str(self.container.name))

    def __eq__(self, other):
        if other == None:
            return False
        if not hasattr(other, "container") or \
                not hasattr(other, "incoming_links") or \
                not hasattr(other, "outgoing_links") or \
                not hasattr(other, "graph"):
            return False
        if other.container == self.container:
            if other.graph != self.graph:
                return False
            return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def fulfills_dependencies(self, containers):
        nodes = [self.graph[container] for container in running_containers]
        for outgoing_link in self.outgoing_links:
            if not outgoing_link in nodes:
                return False
        return True

    def is_depending_on(self, containers):
        nodes = [self.graph[container] for container in running_containers]
        for outgoing_link in self.outgoing_links:
            if outgoing_link in nodes:
                return True
        return False

    def is_dependency_of(self, containers):
        nodes = [self.graph[container] for container in running_containers]
        for incoming_link in self.outgoing_links:
            if incoming_link in nodes:
                return True
        return False

class DoAlongDependencyEdges(object):
    """ This helper class operates on the graph provided by a
        ContainerLinkDependencyResolution object.

        It constructs a subgraph according to the following rules:

        - the graph will be made of the specified initial_containers list,
          and then gradually grown along all the outgoing edges or,
          alternatively with along_incoming=True, along all the incoming
          edges to the maximum extent

        You can then yield containers in this subgraph in the partial order
        laid out by the connections it was grown along, starting with the root
        nodes, then their children, etc.

        This allows you to apply a function to all yielded containers in
        order, e.g. launching them.

        See the iterators DoAlongDependencyEdges.unprocessed_containers() or
        DoAlongDependencyEdges.failed_containers() for details on how the
        containers of this subgraph can be yielded.
    """
    def __init__(self, dependency_resolution_object,
            initial_containers,
            along_incoming=False, expand_initial_set=True,
            reverse_yield_order=False):
        self.access_lock = threading.Lock()

        self.expand_initial_set = expand_initial_set
        self.along_incoming = along_incoming
        self.graph = dependency_resolution_object.graph
        self.containers = initial_containers
        self.start_set = set()
        for container in initial_containers:
            self.start_set.add(self.graph[container])
        self.marked_done = set()
        self.marked_failed = set()
        self.yielded_nodes = set()
        self.yielded_failed_nodes = set()
        self.reverse_yield_order = reverse_yield_order

        # Grow to the relevant subgraph which can be possibly returned:
        expand_queue = queue.Queue()
        self.subgraph = {}
        for container in initial_containers: # queue up initial containers
            self.subgraph[container] = self.graph[container]
            expand_queue.put(self.subgraph[container])
        if expand_initial_set:
            while not expand_queue.empty(): # crawl through graph and grow it
                expand_node = expand_queue.get()
                expanded_nodes = []
                if along_incoming:
                    expanded_nodes = expand_node.incoming_links
                else:
                    expanded_nodes = expand_node.outgoing_links
                for expanded_node in expanded_nodes:
                    if not expanded_node.container in self.subgraph:
                        self.subgraph[expanded_node.container] = expanded_node
                        expand_queue.put(expanded_node)

    def __iter__(self):
        return self

    def get_next_candidates(self):
        """ Internal function. Don't call before obtaining access lock!!

            Returns a set of candidates that can currently be returned as
            unprocessed nodes. Ignores which ones have already been returned
            by the iterator, but it ensures only those reachable through
            success nodes are actually returned.
        """
        start_nodes = set()
        if not self.reverse_yield_order:
            # Get root nodes in relevant sub graph:
            for node in self.subgraph.values():
                if self.along_incoming:
                    if len([node for node in node.outgoing_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
                else:
                    if len([node for node in node.incoming_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
            if len(start_nodes) == 0:
                raise RuntimeError("invalid graph, no root nodes present")
        else:
            # Get leaf nodes in relevant sub graph:
            for node in self.subgraph.values():
                if self.along_incoming:
                    if len([node for node in node.incoming_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
                else:
                    if len([node for node in node.outgoing_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
            if len(start_nodes) == 0:
                raise RuntimeError("invalid graph, no leaf nodes present")

        # Prepare some stuff for gradually crawling the graph:
        examine_queue = queue.Queue()
        def get_successors(node):
            if self.along_incoming:
                if not self.reverse_yield_order:
                    return node.incoming_links
                else:
                    return node.outgoing_links
            else:
                if not self.reverse_yield_order:
                    return node.outgoing_links
                else:
                    return node.incoming_links
        def get_predecessors(node):
            if self.along_incoming:
                if not self.reverse_yield_order:
                    return node.outgoing_links
                else:
                    return node.incoming_links
            else:
                if not self.reverse_yield_order:
                    return node.incoming_links
                else:
                    return node.outgoing_links
        for node in start_nodes:
            examine_queue.put(node)

        # Start advancing through graph to next unprocessed nodes:
        seen = set()
        candidates = []
        while not examine_queue.empty():
            next_node = examine_queue.get()
            if not next_node in self.marked_done and \
                    not next_node in self.marked_failed:
                # This node is unprocessed. Check if all predecessors are
                # actually marked as done so we can return it:
                predecessors = get_predecessors(next_node)
                reachable = True
                for predecessor in predecessors:
                    if not predecessor.container in self.subgraph:
                        continue
                    if not predecessor in self.marked_done:
                        # Nope, at least one predecessor isn't done yet.
                        reachable = False
                        break
                    if predecessor in self.marked_failed:
                        # Whoops, this should also be marked failed.
                        self.marked_failed.add(next_node)
                        reachable = False
                        break
                if reachable:
                    candidates.append(next_node)
            else:
                # This node is either failed or done/succeeded. See which:
                if not next_node in self.marked_failed:
                    # It's a succeeded node, therefore look at the successors:
                    for successor in get_successors(next_node):
                        if not successor.container in self.subgraph:
                            continue
                        if not successor in seen:
                            seen.add(successor)
                            examine_queue.put(successor)

        # Return assembled candidates:
        return candidates

    def mark_success(self, container):
        """ Mark a container as succeeded. This opens up all the direct
            dependencies of this container to be returned from
            unprocessed_containers() as next unprocessed containers to be
            taken care of.
        """
        self.access_lock.acquire()
        self.marked_done.add(self.graph[container])
        self.access_lock.release()

    def mark_failure(self, container):
        """ Mark a container as failed. No container depending on this one
            will be considered for yielding for unprocessed_containers()
            anymore.
        """
        self.access_lock.acquire()
        self.marked_failed.add(self.graph[container])
        self.access_lock.release()

    def _next_unprocessed_node(self):
        """ Note: internal function, use unprocessed_nodes() iterator instead.

            Get the next node that is still unprocessed.
            
            Calling this function continuously will gradually yield all
            unprocessed nodes that are obtainable (see
            unprocessed_containers() for details), and None if there is
            currently no unprocessed node that can be returned.
        """

        self.access_lock.acquire()
        # Get all the candidates:
        candidates = self.get_next_candidates()

        # Check which one we haven't already returned:
        actual_candidates = []
        for candidate in candidates:
            if not candidate in self.yielded_nodes:
                actual_candidates.append(candidate)

        # If no real candidate remains, return None:
        if len(actual_candidates) == 0:
            self.access_lock.release()
            return None

        # Pick a random choice of the remaining candidates:
        yielded_node = random.choice(actual_candidates)
        self.yielded_nodes.add(yielded_node)
        result = yielded_node
        self.access_lock.release()
        return result

    def _all_nodes_processed(self):
        """ Check if all nodes are either marked as success or failure.
            Returns True if all are marked, False if some are still unmarked.
        """
        result = True
        self.access_lock.acquire()

        def get_predecessors(node):
            if self.along_incoming:
                if not self.reverse_yield_order:
                    return node.outgoing_links
                else:
                    return node.incoming_links
            else:
                if not self.reverse_yield_order:
                    return node.incoming_links
                else:
                    return node.outgoing_links

        # Check for completely unmarked nodes:
        for node in self.subgraph.values():
            if not node in self.marked_failed and \
                    not node in self.marked_done:
                # Make sure to propagate failures, eventually:
                predecessors = get_predecessors(node) 
                for predecessor in predecessors:
                    if predecessor in self.marked_failed:
                        self.marked_failed.add(node)
                        break

                # Stop since we found an unmarked node:
                result = False
                break

        self.access_lock.release()
        return result

    def unprocessed_containers(self):
        """ This function returns an iterator which yields the items according
            to the order specified in the DoAlongDependencyEdges class
            description.

            However, it introduces two additional criteria (while maintaing
            the order implicated by the subgraph):

            - a container can only be yielded as soon as all the direct
              predecessors are marked as "succeeded" with mark_success().

            - all containers where any direct or indirect predecessors were
              marked as "failed" with mark_failure() will be ignored and not
              yielded at all.

            If containers cannot be yielded because the first criterion
            isn't fulfilled and they weren't discarded through the second
            criterion yet and if all containers that could have been yielded
            so far have been yielded, the iterator will hang and wait for
            this situation to be resolved through further mark_success()/
            mark_failure() calls.

            As a result, this iterator will eventually hang unless you make
            sure to eventually mark all returned items with "mark_success" or
            "mark_failure". If you fail to do that, the iterator will remain
            stuck.
        """

        class UnprocessedContainersIterator(object):
            def __init__(self, doalongobj):
                self.doalongobj = doalongobj

            def __iter__(self):
                return self

            def __next__(self):
                skipped = True
                while skipped:
                    skipped = False

                    # Get next node or wait until we can get it:
                    node = self.doalongobj._next_unprocessed_node()
                    while node == None and \
                            not self.doalongobj._all_nodes_processed():
                        time.sleep(0.2)
                        node = self.doalongobj._next_unprocessed_node()
                    if node == None:
                        raise StopIteration

                    # Make sure we only expand beyond initial set if we were
                    # supposed to do that:
                    if not self.doalongobj.expand_initial_set and \
                            not node.container in self.doalongobj.containers:
                        # Skip this, it would expand beyond initial set.
                        skipped = True
                        continue
                    break
                return node.container
        return UnprocessedContainersIterator(self)

    def failed_containers(self):
        """ This function returns an iterator which yields containers
            in no particular order which fulfill the following criterion:

            - containers which have been marked as "failed" or where any
              direct or indirect predecessors were marked as "failed" with
              mark_failure() will be yielded

            If containers have neither met by this criterion, nor been marked
            as succeeded and if all containers that could have been yielded so
            far have been yielded already, this iterator will hang and wait
            for this situation to be resolved through further mark_success()/
            mark_failure() calls.

            As a result, this iterator will eventually hang unless you make
            sure to eventually mark all returned items with "mark_success" or
            "mark_failure". If you fail to do that, the iterator will remain
            stuck.
        """
        raise NotImplementedError("not implemented at this point")

class ContainerLinkDependencyResolution(object):
    """ This class builds a graph based on the container.dependencies (which
        are themselves constructed from the docker links of each container).

        This graph can then be used with DoAlongDependencyEdges to do things
        according to the dependency order.
    """
    def __init__(self, containers):
        self.containers = containers
        self.graph = dict()

        # Add all containers as nodes without arcs:
        for container in self.containers:
            if not container in self.graph:
                self.graph[container] = ContainerLinkDependencyNode(
                    container, [], [], self.graph)

        # Go through containers again and add arcs:
        node_added = True
        while node_added:
            node_added = False
            for container in self.containers:
                source_node = self.graph[container]
                for dependency in container.dependencies:
                    dep_container = None
                    try:
                        dep_container = dependency.container
                    except ValueError:
                        print("drudder: error: dependency for " +\
                            str(container) + " is not in known services, " +\
                            "can't resolve dependency chain: " +\
                            str(dependency), file=sys.stderr)
                        sys.exit(1)

                    # Make sure the target for our new arc is in the graph:
                    if not dep_container in self.graph:
                        self.graph[dep_container] = \
                            ContainerLinkDependencyNode(
                            dep_container, [], [], self.graph)
                        node_added = True

                    # Obtain target node ref:
                    target_node = self.graph[dep_container]

                    # Add arcs in both directions, if any is missing:
                    if not target_node in source_node.incoming_links:
                        source_node.outgoing_links.append(target_node)
                    if not source_node in target_node.outgoing_links:
                        target_node.incoming_links.append(source_node)
                if node_added:
                    break
        # Done! Graph complete

    def print_graph_debug(self):
        print("Graph:")
        seen = set()
        for container in self.graph:
            node = self.graph[container]
            for outgoing in node.outgoing_links:
                assert(node in outgoing.incoming_links)
                print(str(node.container) + " -> " + str(outgoing.container))
                seen.add(node.container)
                seen.add(outgoing.container)
        for container in self.graph:
            if not container in seen:    
                print(str(container))
        print("End of Graph.")

class ContainerStatusInfo(object):
    """ A class to compute a dictionary of all sorts of extra information for
        a container. Used by the "info" command.
    """
    def __init__(self, container):
        self.container = container

    def get(self, ordered=False):
        dict_class = dict
        if ordered:
            dict_class = OrderedDict
        info = dict_class()
        info["Canonical docker container name"] = \
            self.container.default_container_name 
        info["Owning service"] = OrderedDict()
        info["Owning service"]["Name"] = self.container.service.name
        info["Owning service"]["Location"] = \
            self.container.service.service_path
        info["Running"] = self.container.running
        info["Dependencies"] = OrderedDict()
        resolution = ContainerLinkDependencyResolution(
            Service.all_containers())
        info["Dependencies"]["Pre-Start"] = []
        pre_start = \
            DoAlongDependencyEdges(resolution, [self.container],
            along_incoming=False, reverse_yield_order=True)
        for container in pre_start.unprocessed_containers():
            if container == self.container:
                pre_start.mark_success(container)
                continue
            info["Dependencies"]["Pre-Start"].append(
                container)
            pre_start.mark_success(container)
        info["Dependencies"]["Pre-Stop-Post-Restart"] = []
        pre_stop = \
            DoAlongDependencyEdges(resolution, [self.container],
            along_incoming=True, reverse_yield_order=True)
        for container in pre_stop.unprocessed_containers():
            if container == self.container:
                pre_stop.mark_success(container)
                continue
            info["Dependencies"]["Pre-Stop-Post-Restart"].append(
                container)
            pre_stop.mark_success(container)

        # Collect volumes:
        potentially_lost = self.container.potentially_lost_volumes
        info["Volumes"] = list()
        for volume in self.container.volumes:
            vol_info = dict_class()
            if volume.id != None:
                vol_info["Id"] = str(volume.id)
            if volume.name != None:
                vol_info["Name"] = volume.name
                vol_info["Unnamed"] = False
            else:
                vol_info["Unnamed"] = True
            vol_info["Specified in YAML"] = volume.specified_in_yml
            if volume in potentially_lost:
                vol_info["Potentially lost on container recreation"] = True
            mounts = volume.mounts
            if len(mounts) > 0:
                vol_info["Mounts"] = list()
                for mount in mounts:
                    mount_info = dict_class() 
                    if mount.host_path != None:
                        mount_info["Host mount path"] = mount.host_path
                    if mount.mount_container_filesystem_path != None:
                        mount_info["Container mount path"] =\
                            mount.mount_container_filesystem_path
                    vol_info["Mounts"].append(mount_info)
            info["Volumes"].append(vol_info)
        return info

def main():
    if not yaml_available:
        yaml_error()
        sys.exit(1)

    parser = argparse.ArgumentParser(description=textwrap.dedent('''\
        drudder: a tool for managing a larger collection of
        docker-compose.yml-specified container groups simply.'''
        ),
        formatter_class=DoubleLineBreakFormatter)
    parser.add_argument("action",
        help=textwrap.dedent('''\
        Possible values:

        "install-tools": install and upgrade all required tools like
                         docker-machine and docker-compose on the local
                         computer. This will automatically fetch and provide
                         anything to require drudder.
 
        "list": list all known services.

        "start": start the service specified as argument (or "all" for all).

        "stop": stop the service specified as argument (or "all" for all).

        "restart": restart the service specified as argument (or "all" for all).

        "info": show more detailed info for the given service (or "all for
                all).

        "logs": output the logs of all the docker containers of the service
                specified as argument (or "all" for all).

        "shell": start an interactive shell in the specified service's specified
                 subservice (parameters: <service> <subservice>) - the subservice
                 is optional if the docker-compose.yml has just one
                 container/subservice.

        "snapshot": store an atomic snapshot of the live data of the service
                    specified as argument (from livedata/) in livedata-snapshots/

        "clean": clean up all stopped containers. THIS IS NOT REVERSIBLE. The
                 docker images of course won't be touched.

        "dump-container-info" : dump all sorts of detailed container info as YAML.
                                Mainly useful if you want to process the combined
                                data computed by drudder with another tool
                                of yours.
        ''')

        )
    parser.add_argument("argument", nargs="*",
        help="argument(s) to given action")
    parser.add_argument("--version", help="show version and quit",
        default=False, action="store_true",
        dest="show_version")
    parser.add_argument("--force",
        default=False, action="store_true",
        help="Can be used to override various safety warnings. Consult the "+\
            "respective warning prompted by an action to understand the "+\
            "consequences",
        dest="force")
    if len(" ".join(sys.argv[1:]).strip()) == 0:
        parser.print_help()
        sys.exit(1)
    for arg in sys.argv[1:]:
        if arg == "--version" or arg == "-v" or arg == "-V":
            print("drudder version " + str(TOOL_VERSION))
            sys.exit(0)
    args = parser.parse_args()

    ensure_docker = SystemInfo.docker_path()
    ensure_docker_compose = SystemInfo.docker_compose_path()


    def unknown_action(hint=None):
        """ Print an error that the given action to drudder is invalid,
            with a possible hint to suggest another action.
        """
        print("drudder: error: unknown action: " + \
            args.action, file=sys.stderr)
        if hint != None:
            print("Did you mean: " + str(hint) + "?")
        sys.exit(1)

    # Ensure the docker main service is running:
    error_output = None
    try:
        subprocess.check_output([SystemInfo.docker_path(), "ps"],
            stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        error_output = e.output.decode("utf-8", "ignore")
    if error_output != None:
        # Old-style error message:
        if error_output.find("dial unix") >= 0 and \
                error_output.find("no such file or directory") >= 0:
            print("drudder: error: " + \
                "docker daemon appears to be not running." +\
                " Please start it and ensure it is reachable.")
            sys.exit(1)
        # Newer error message:
        elif error_output.find("Cannot connect to the Docker daemon") >= 0:
            print("drudder: error: " + \
                "docker daemon appears to be not running." +\
                " Please start it and ensure it is reachable.")
            sys.exit(1)
        else:
            print("drudder: error: " + \
                "there appears to be some unknown problem with " + \
                "docker! (test run of \"docker ps\" returned error code)")
            sys.exit(1)

    # Check if services are btrfs ready, and give warning if not:
    if args.action != "snapshot" and args.action != "info":
        for service in Service.all():
            snapshots = Snapshots(service)
            snapshots.subvolume_readiness_check()

    # --- Main handling of actions here:

    if args.action == "list":
        all_services = Service.all()
        print("\033[1mService list (" + str(len(all_services)) +\
            " service(s)):\033[0m")
        for service in all_services:
            state = ""
            running_names = service._get_running_service_container_names()
            all_names = [container.name for container in service.containers]
            if len(all_names) == 0:
                state = "\033[1;33mservice has no containers" +\
                    "\033[0m"
            elif len(running_names) == len(all_names):
                state = "\033[1;32mrunning\033[0m"
            elif len(running_names) > 0:
                state = "\033[1;33mpartial (running: " +\
                    ", ".join(running_names) + ", not running: " +\
                    ", ".join([name for name in all_names\
                        if name not in running_names]) + ")\033[0m"
            else:
                state = "\033[1;31mstopped\033[0m"
            print("\033[0m  - \033[1m" + service.name + "\033[0m (" + \
                service.service_path + "): " + state)
    elif args.action == "help":
        parser.print_help()
        sys.exit(1)
    elif args.action == "logs":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service for which docker logs shold be printed, "+\
                "or \"all\"", file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        for container in containers:
            print_msg("printing log of container " + str(container),
                service=container.service.name, color='blue')
            try:
                retcode = subprocess.call([SystemInfo.docker_path(), "logs",
                        container.current_running_instance_name],
                        stderr=subprocess.STDOUT)
                if retcode != 0:
                    raise subprocess.CalledProcessError(
                        retcode, " ".join([SystemInfo.docker_path(), "logs"]))
            except subprocess.CalledProcessError:
                print_msg("failed printing logs. " +\
                    "Maybe container has no logs yet?",
                    service=container.service.name,
                    container=container.name, color='yellow')
                pass
    elif args.action == "rebuild":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service for which an interactive shell " +\
                "should be started",
                file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        for container in containers:
            if container.running:
                print_msg("cannot safely recreate container, " +\
                    "one or more of the specified " +\
                    "containers is/are currently running: " +\
                    str(", ".join([
                        str(container) for container in containers])),
                    color="red")
                print("drudder: error: aborted rebuilding")
                sys.exit(1)
        some_failed = False
        for container in containers:
            print_msg("rebuilding container with no cache",
                service=container.service.name, container=container.name,
                color="blue")
            try:
                subprocess.check_call([SystemInfo.docker_compose_path(),
                    "build",
                    "--no-cache", "--pull", container.name],
                    cwd=container.service.service_path)
            except subprocess.CalledProcessError as e:
                some_failed = True
                print_msg("Build failure (exit code " +\
                    str(e.returncode) + ")", color="red")
        if some_failed:
            print_msg("Rebuilding completed with errors.", color="red")
            sys.exit(1)
        print_msg("Rebuilding complete.", color="green")
        sys.exit(0)
    elif args.action == "shell":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service for which an interactive " +\
                "shell should be started",
                file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        if len(containers) != 1:
            print("drudder: error: this command can only be used " + \
                "on a single container, but the given argument \"" +\
                " ".join(args.argument) +\
                "\" matches multiple " +\
                "containers: " +\
                ", ".join([str(container) for container in containers]),
                file=sys.stderr)
            sys.exit(1)
        if containers[0].running:
            cname = containers[0].current_running_instance_name
            print_msg("attaching to running container " + str(cname),
                service=containers[0].service.name,
                color="blue")
            subprocess.call([SystemInfo.docker_path(), "exec", "-t", "-i",
                    str(cname), "/bin/bash"],
                stderr=subprocess.STDOUT)
        else:
            print_msg("launching container " + str(containers[0]) + \
                " with shell",
                service=containers[0].service.name,
                color="blue")
            image_name = containers[0].image_name
            subprocess.call([SystemInfo.docker_compose_path(), "build",
                containers[0].name], cwd=containers[0].service.service_path)
            subprocess.call([SystemInfo.docker_compose_path(), "run",
                containers[0].name, "/bin/bash"],
                cwd=containers[0].service.service_path)
    elif args.action == "start" or args.action == "restart":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service to be started, or \"all\"", file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        if len(containers) == 0:
            sys.exit(0)

        resolution = ContainerLinkDependencyResolution(
            Service.all_containers())

        tracker = FailedLaunchTracker()
        scheduled_action = "restart"
        if args.action == "start":
            scheduled_action = "start"

        # Stop all indirect dependencies and the containers themselves:
        restart_dependencies = set()
        if scheduled_action == "restart":
            unreversed_stop_before_restart = \
                DoAlongDependencyEdges(resolution, containers,
                along_incoming=True)
            stop_before_restart = []
            for container in \
                    unreversed_stop_before_restart.unprocessed_containers():
                stop_before_restart.append(container)
                unreversed_stop_before_restart.mark_success(container)
            stop_before_restart = list(reversed(stop_before_restart))
            for container in stop_before_restart:
                if not container in containers:
                    print_msg("stopping... " +\
                        "(this container is depending on 1+ " +\
                        "of the container(s) scheduled for restart)",
                        service=container.service.name,
                        container=container.name,
                        color="blue")
                    restart_dependencies.add(container)
                else:
                    print_msg("stopping... " +\
                        "(this container was scheduled for " +\
                        scheduled_action + ")",
                        service=container.service.name,
                        container=container.name,
                        color="blue")
                LaunchThreaded.stop(container)

        # Helper function to launch a service:
        threads = []
        def start_container(container, is_last=False,
                do_on_failure=None, do_on_success=None):
            if container.running:
                if container in containers:
                    print_msg("already running. (this container was " +\
                            "scheduled for " +\
                            scheduled_action + ")",
                        service=container.service.name,\
                        container=container.name,
                        color="green")
                elif container in restart_dependencies:
                    print_msg("already running. " +\
                            "(this container was " +\
                            "an external dependency on the " +\
                            scheduled_action +\
                            "ed containers) - " +\
                            "weird, didn't we stop it before??",
                        service=container.service.name,\
                        container=container.name,
                        color="yellow")
                else:
                    print_msg("already running. (this container is " +\
                            "a dependency of a container scheduled for " +\
                            scheduled_action + ")",
                        service=container.service.name,\
                        container=container.name,
                        color="green")
                if do_on_success != None:
                    do_on_success()
                return
            if not is_last:
                t = LaunchThreaded.attempt_launch(container,
                    failed_launch_tracker=tracker,
                    force_container_recreation=(args.force is True),
                    do_on_failure=do_on_failure,
                    do_on_success=do_on_success)
            else:
                t = LaunchThreaded.attempt_launch(container,
                    failed_launch_tracker=tracker,
                    to_background_timeout=None,
                    force_container_recreation=(args.force is True),
                    do_on_failure=do_on_failure,
                    do_on_success=do_on_success)
            if t != None:
                threads.append(t)

        def mark_success_func(net, container):
            return lambda: net.mark_success(container)

        def mark_failure_func(net, container):
            return lambda: net.mark_failure(container)

        # Start the dependencies of our container launch set:
        start_dependency_containers = \
            DoAlongDependencyEdges(resolution, containers,
            along_incoming=False, expand_initial_set=True,
            reverse_yield_order=True)
        for container in start_dependency_containers.unprocessed_containers():
            if container in containers: # not a dependency
                start_dependency_containers.mark_success(container)
                continue
            # Launch it:
            start_container(container, is_last=False,
                do_on_failure=mark_failure_func(
                    start_dependency_containers, container),
                do_on_success=mark_success_func(
                    start_dependency_containers, container))

        # Start everything again:
        start_containers = \
            DoAlongDependencyEdges(resolution, set(containers).union(\
                restart_dependencies),
                along_incoming=False, expand_initial_set=False,
                reverse_yield_order=True)
        for container in start_containers.unprocessed_containers():
            start_container(container, is_last=False,
                do_on_failure=mark_failure_func(
                    start_containers, container),
                do_on_success=mark_success_func(
                    start_containers, container))

        # Wait until launches are finished:
        LaunchThreaded.wait_for_launches(threads)
        if len(tracker) > 0:
            print("drudder: error: some launches failed.",
                file=sys.stderr)
            sys.exit(1)
        else:
            sys.exit(0)
    elif args.action == "stop":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service to be stopped, or \"all\"", file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        for container in containers:
            assert(container != None)
            if container.running:
                print_msg("stopping...", service=container.service.name,
                    container=container.name,
                    color="blue")
                LaunchThreaded.stop(container)
                print_msg("stopped.", service=container.service.name,
                    container=container.name, color="green")
            else:
                print_msg("not currently running.",
                    service=container.service.name,
                    container=container.name,
                    color="blue")
        sys.exit(0)
    elif args.action == "snapshot":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service to be snapshotted, or \"all\"",
                file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        return_error = False
        for service in services:
            fs = SystemInfo.filesystem_type_at_path(service.service_path)
            if fs != "btrfs":
                print_msg("cannot snapshot service. filesystem " +\
                    "is " + fs + ", would need to be btrfs",
                    service=service.name, color="red")
                return_error = True
                continue
            snapshots = Snapshots(service)
            if not snapshots.do():
                return_error = True
        if return_error:
            print("drudder: error: some snapshots failed.",
                file=sys.stderr)
            sys.exit(1)
        else:
            sys.exit(0)
    elif args.action == "info":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service or container to print info for, " +\
                "or \"all\"", file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        def print_info(key, value=None, color=None, bold=False,
                no_line_break=False):
            if hasattr(value, "append"):
                if len(value) == 0:
                    value = "<empty list>"
                else:
                    value = ", ".join([str(item) for item in value])
            if bold:
                t = "\033[1m" + key
            else:
                t = key
            if value == None:
                if bold:
                    t += "\033[0m"
                print(t)
                return
            if (len(value) > 25 or value.find("\n") >= 0) and \
                    not no_line_break:
                value = "\n  " + "\n  ".join(
                    [line for line in value.split("\n")])
            if bold:
                t += "\033[0m: "
            else:
                t += ": "
            if color == "red":
                t += "\033[31m\033[1m" + value
            elif color == "yellow":
                t += "\033[33m\033[1m" + value
            elif color == "green":
                t += "\033[32m\033[1m" + value
            elif color == "blue":
                t += "\033[34m\033[1m" + value
            else:
                t += value
            if color != None:
                t += "\033[0m"
            print(t)
        first = True
        for container in containers:
            if first:
                first = False
            else:
                print("")
            print_info("Info for", str(container), bold=True,
                no_line_break=True)
            print("-" * (len("Info for: ") + len(str(container))))
            info = ContainerStatusInfo(container).get()
            if info["Running"]:
                print_info("Running", "yes", color="green")
            else:
                print_info("Running", "no", color="red")
            print_info("Canonical docker container name",
                info["Canonical docker container name"])
            print_info("Configuration file",
                os.path.join(container.service.service_path,
                "docker-compose.yml"))
            snapshots = Snapshots(container)
            if not snapshots.subvolume_readiness_check():
                print_info("Live data snapshot support",
                    "no",
                    color="red")
                print("  Hint: try \"drudder snapshot " +\
                    str(container.service.name) + "\" for details")
            else:
                print_info("Live data snapshot support", "ok", color="green")
            print("This container depends on "+\
                "(ensured to run if this one starts):")
            l = info["Dependencies"]["Pre-Start"]
            if len(l) == 0:
                print("  <no dependencies>")
            else:
                for item in l:
                    print(" - " + str(item))
            print("Other containers depending on this " +\
                "(restarted along with this one):")
            l = info["Dependencies"]["Pre-Stop-Post-Restart"]
            if len(l) == 0:
                print("  <no dependencies>")
            else:
                for item in l:
                    print(" - " + str(item))
            have_potentially_lost_volumes = False
            for volume in info["Volumes"]:
                if "Potentially lost on container recreation" in volume:
                    if volume["Potentially lost on container recreation"] \
                            is True:
                        have_potentially_lost_volumes = True
                        break
            if have_potentially_lost_volumes:
                potentially_lost = container.potentially_lost_volumes
                if len(potentially_lost) > 0:
                    print("\033[33m\033[1mWarning: some volumes might be " +\
                        "lost on container recreation:\n         "+\
                        ", ".join([str(vol) for vol in potentially_lost]) +\
                        "\n" +\
                        "         Make sure to specify them in your " +\
                            "docker-compose.yml!\033[0m")
        sys.exit(0)
    elif args.action == "dump-container-info":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service or container to print info for, " +\
                "or \"all\"", file=sys.stderr)
            sys.exit(1)
        containers = TargetsParser.get_containers(" ".join(args.argument),
            print_error=True)
        if containers == None:
            sys.exit(1)
        for container in containers:
            info = ContainerStatusInfo(container).get(ordered=True)
            print("# --- Container: " + str(container))
            print("\"" + str(container) + "\":")
            def serialize(item, indent=2):
                t = ""
                if isinstance(item, dict) or isinstance(item, OrderedDict):
                    for k in item:
                        t = t + " " * indent + k + ":"
                        t2 = serialize(item[k], indent=2)
                        if t2.find("\n") >= 0:
                            if t2.endswith("\n"):
                                t2 = t2[:-1]
                            t = t + "\n" + \
                                "\n".join([" " * (indent) + line for line in \
                                t2.split("\n")]) + "\n"
                        else:
                            t = t + " " + t2[2:] + "\n"
                elif isinstance(item, list):
                    for value in item:
                        t2 = serialize(value, indent=(indent))
                        t2 = t2.lstrip()
                        if t2.endswith("\n"):
                            t2 = t2[:-1]
                        t = t + "- " +\
                            t2 + "\n"
                else:
                    return " " * indent + str(item)
                return t
            print(serialize(info))
        sys.exit(0)
    elif args.action == "clean":
        Service.global_clean_up(args.force != True)
    else:
        unknown_action()

if __name__ == "__main__":
    main()
