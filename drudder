#!/usr/bin/env python3

"""
Copyright (c) 2015-2016  Jonas Thiem et al.

This software is provided 'as-is', without any express or implied
warranty. In no event will the authors be held liable for any damages
arising from the use of this software.

Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:

1. The origin of this software must not be misrepresented; you must not
   claim that you wrote the original software. If you use this software
   in a product, an acknowledgement in the product documentation would be
   appreciated but is not required.
2. Altered source versions must be plainly marked as such, and must not be
   misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution.
"""

"""
    # What is this


    ## Motivation

    **docker-rudder** is a tool which operates on top of docker and
    docker-compose. It doesn't add any notable new features (other than btrfs
    snapshot handling): instead it tries to offer a well-thought-out interface
    to simplify all the required daily tasks needed for managing your docker
    containers.

    It unifies some tasks requiring multiple commands with the regular docker
    interfaces in single quick commands with sensible defaults and satefy
    checks.


    ## Features

    - easily start/stop and manage multiple docker-compose controlled
      services in one go

    - clean up all stopped containers and dangling volumes

    - creation of atomic snapshots of all your read-write volumes/live data
      without shutting down or pausing your containers (for backup purposes)

    - one self-contained file



    # Basic usage

    Usage:

    ```
      drudder list                - list all known services
      drudder start <service>     - starts the specified service
      drudder stop <service>      - stops the specified service
      drudder restart <service>   - restarts the given service.
                                    **WARNING**: the containers will *always*
                                    get rebuilt and recreated by this command
                                    (unless this would result in dangling
                                    volumes).
                                    All data in the containers outside of
                                    volumes will be reset!
      drudder rebuild <service>   - force rebuild of the service from the
                                    newest Dockerfile and/or image. Please note
                                    this is only required if you want to force
                                    a rebuild from the ground up, the (re)start
                                    actions will already update the container 
                                    if any of the relevant Dockerfiles were
                                    changed.
      drudder info <service>[/subservice] - show extended info about the
                                            service
      drudder logs <service>      - print logs of all docker containers of the
                                    service
      drudder shell <service>[/<subservice>]  - run a shell in the specified
                                                subservice's container
      drudder snapshot <service>  - makes a snapshot of the live data if
                                    enabled. (optional) This feature requires
                                    btrfs
      drudder clean               - deletes all containers that aren't running
                                    and all dangling volumes
    ```

    **Hint**: You can always use "all" as service target if you want to apply
    an action to all services on your machine.



    # Installation

    Copy the drudder script to /usr/bin/ and set execution bit (chmod +x)



    # HOW TO add your service

    docker-rudder expects services grouped with docker-compose /
    docker-compose.yml. The script will scan the following locations for
    services subfolders with a docker-compose.yml in them:

    - the current working directory when running the script
    - /usr/share/docker-services/
    - /srv/

    Each service folder inside one of those locatoins should contain:

    - docker-compose.yml to launch it. (folders without this file are skipped)
    - livedata/ subfolder where all read-write volumes are mounted to
                                (recommended, see snapshots as described below)

    To list all currently recognized services, type: `drudder list`

    Congratulations, you can now manage launch your service(s) with
    docker-rudder!



    # HOW TO backup

    You should backup all your services. docker-rudder provides snapshot
    functionality to help with this. While you could simply copy your service
    folder with all the mounted volumes in it, this can lead to corrupt copies
    when doing this while some services are operating (SQL databases etc.).

    To use docker-rudder snapshots of your writable volumes during service
    operation, do this:

    1. Enable snapshots as described below

    2. Always run "docker-rudder snapshot all" before you make your backup
       to get consistent snapshots of your writable volumes in a subfolder
       named livedata-snapshpots/ in each respective service folder.



    # HOW TO enable snapshots (optional)

    This feature allows you to easily copy all the live data of your read-write
    mounted volumes your containers as atomic snapshots even while your
    services are running and continue to write data.

    The snapshots will be atomic, therefore they should be suitable even for
    database realtime operations while the database is running and writing to
    the volume(s).


    ## Enable snapshots for a specific service

    How to enable snapshots for a service:

    1. Make sure your services folder is on a btrfs file system (not ext4).

    2. Each of your snapshot enabled services needs to have a subfolder
       livedata/ where all read-write volumes of it are located.


    ## Test/do it

    **Before you go into production, make sure to test snapshots for your
    service(s) at least once!**

    Calling:
       ``` drudder snapshot <service>|"all" ```
  
    will now use btrfs functionality to add a time-stamped folder with an
    atomic snapshot of livedata/ of the specified service(s) into a new
    livedata-snapshots/ subfolder - while your service can continue using the
    volume thanks to btrfs' copy-on-write snapshot functionality.


    ## Restore a snapshot

    You can easily restore such a snapshot by shutting down your service
    temporarily, copying back a snapshot into livedata/ and turning your
    service back on.

"""

""" Copyright (C) Jonas Thiem et al., 2015-2016
"""

TOOL_VERSION="0.1"

import argparse
from argparse import RawTextHelpFormatter, HelpFormatter
import base64
from collections import OrderedDict
import copy
import datetime
import json
import multiprocessing
import os
import pty
import queue
import subprocess
import random
import re
import shutil
import sys
import textwrap
import threading
import time
import traceback
import uuid

yaml_available = False
try:
    import yaml
except ImportError:
    print("IMPORTANT WARNING: using YAML fallback hack. This will work for "+\
        "most generic configs, but lead to errors for advanced YAML files. "+\
        "Please install pyyaml to use proper parsing.", file=sys.stderr)
    pass

class DoubleLineBreakFormatter(HelpFormatter):
    """ Retains double line breaks/paragraphs """
    def _split_lines(self, text, width):
        return self._fill_text(text, width, "").splitlines(False)

    def _fill_text(self, t, width, indent):
        t = " ".join([s for s in t.replace("\t", " ").strip("\t ").split(" ")\
            if len(s) > 0]).replace("\n ", "\n").replace(" \n", " ")
        ts = re.sub("([^\n])\n([^\n])", "\\1 \\2", t).split("\n\n")
        result = [textwrap.fill(paragraph, width,
            initial_indent=indent, subsequent_indent=indent)\
            for paragraph in ts]
        return "\n\n".join(result)

class SystemInfo(object):
    """ This class provides system info of various sorts about the installed
        docker versions, btrfs and more.
    """
    __cached_docker_path = None

    @staticmethod
    def locate_binary(name):
        """ Locate a binary of some name in the common system-wide places
            and return the full path, or return None if it can't be found.
        """
        badchars = '\'"$<> %|&():*/\\{}#!?=\n\r\t[]\033'
        for char in badchars:
            if name.find(char) >= 0:
                raise ValueError("dangerous character in binary name")
        output = None
        try:
            output = subprocess.check_output("which " + name, shell=True,
                stderr=subprocess.STDOUT).\
                decode("utf-8", "ignore").strip()
        except subprocess.CalledProcessError:
            pass
        if output == None or len(output) == 0:
            return None
        return output

    @staticmethod
    def check_output_with_isolated_pty(cmd, shell=False, cwd=None,
            stdout=None, stderr=None, timeout=None, env=None):
        """ This is a re-implementation of subprocess.check_output() which
            will isolate the child process in a pseudo-TTY to prevent it from
            accessing the main terminal itself where docker-rudder itself is
            running from.
        """
        if cwd == None:
            cwd = os.getcwd()
        if stderr == None:
            stderr = subprocess.DEVNULL
        if env == None:
            env = os.environ.copy()

        parent_conn, child_conn = multiprocessing.Pipe()
        pid, fd = pty.fork()

        # Child with pseudo TTY (since some tools like to detect the terminal
        # size and greatly mess with our parsing):
        if pid == 0:
            result_dict = dict()
            error_happened = None
            result = None
            # Run actual command in forked process with fake TTY:
            try:
                if stdout != None:
                    result = subprocess.check_output(cmd, shell=shell,
                        cwd=cwd, stdout=stdout, stderr=stderr,
                        timeout=timeout, env=env)
                else:
                    result = subprocess.check_output(cmd, shell=shell,
                        cwd=cwd, stderr=stderr, timeout=timeout,
                        env=env)
            except Exception as e:
                if isinstance(e, CalledProcessError):
                    result_dict["result"] = base64.b64encode(e.output).decode(
                        "utf-8", "ignore")
                error_happened = e

            # Collect result and send back to parent:
            result_dict["error_happened"] = \
                None if error_happened is None else str(error_happened)
            if result != None:
                result_dict["result"] = base64.b64encode(result).decode(
                    "utf-8", "ignore")
            dumped = json.dumps(result_dict)
            child_conn.send(dumped)
            child_conn.close()

            # We're done. Parent will receive result and handle things:
            os.exit(0)
        else:
            # Wait for child to execute process and report result:
            result_dict = json.loads(parent_conn.recv())
            parent_conn.close()
            if result_dict["error_happened"] != None:
                e = subprocess.CalledProcessError(cmd, 1)
                if result_dict["result"] != None:
                    e.output = base64.b64decode(result_dict["result"])
                raise e
            return base64.b64decode(result_dict["result"])

    @staticmethod
    def docker_path():
        """ Locate docker binary and return its path, or print an error
            and exit the program with sys.exit(1) if not available.
        """
        if SystemInfo.__cached_docker_path != None:
            return SystemInfo.__cached_docker_path
        
        def behaves_like_docker(binary_path):
            output = None
            try:
                output = subprocess.check_output([binary_path, "--version"],
                    stderr=subprocess.STDOUT)
            except subprocess.CalledProcessError as e:
                output = e.output
            output = output.decode("utf-8", "ignore")
            return (output.lower().startswith("docker version "))

        test_names = [ "docker.io", "docker" ]

        for test_name in test_names:
            bin_path = SystemInfo.locate_binary(test_name)
            if bin_path == None:
                continue
            if behaves_like_docker(bin_path):
                return bin_path
        print("docker-rudder: error: no docker found. Is it installed?")
        sys.exit(1)
    
    @staticmethod
    def is_btrfs_subvolume(path):
        """ Check if the given path is a btrfs subvolume. Returns True if
            if it is, or False if not. May print out various warnings if there
            were problems detecting this. May raise a ValueError if the path
            couldn't be properly examined for some reason.
        """
        path = os.path.realpath(path)
        nontrivial_error = "error: failed to map a btrs subvolume "+\
            "to its POSIX path. This seems to be a non-trivial setup."+\
            " You should do your snapshotting manually!!"

        # First, get the containing mount point:
        base_path = os.path.normpath(path + "/../")
        mount = SystemInfo.get_fs_mount_root(base_path)
        if mount == None:
            print("docker-rudder: warning: internal problem: " +\
                "mount point of " +\
                str(base_path) + " was returned as: None")
            details = "cannot check path. Mount point search failed: " +\
                str(path)
            print_msg(nontrivial_error + "\n\nError details: " + details,
                color="red")
            raise ValueError(details)

        # Get btrfs subvolume list:
        output = subprocess.check_output([
            SystemInfo.locate_binary("btrfs"),
            "subvolume", "list", path]).\
            decode("utf-8", "ignore").strip().split("\n")
        for line in output:
            if not line.startswith("ID ") or line.find(" path ") < 0:
                raise RuntimeError("unexpected btrfs tool output - " + \
                    "maybe incompatible tool version? Please report this." +\
                    " Full output: " + str(output))
            line = line[line.find(" path ")+len(" path "):].strip()
            full_path_guess = mount + line
            if not os.path.exists(full_path_guess):
                if line == "DELETED":
                    continue
                print_msg(nontrivial_error, color="red")
                return False
            if os.path.normpath(os.path.abspath(full_path_guess)) == \
                    os.path.normpath(os.path.abspath(path)):
                try:
                    output = subprocess.check_output([
                        SystemInfo.locate_binary("stat"),
                        "-c", "%i", path]).decode('utf-8', 'ignore').strip()
                except subprocess.CalledProcessError as e:
                    # Stat failed, although btrfs subvolume list lists it!
                    details = "btrfs volume listed by btrfs tool " +\
                        "cannot be stat'ed: " + str(path)
                    print_msg(nontrivial_error + "\n\nError details: " +\
                        details, color="red")
                    raise ValueError(details)
                if output != "256":
                    # Not a subvolume, although btrfs subvolume list lists it!
                    details = "btrfs volume listed by btrfs tool " +\
                        "doesn't appear to be " +\
                        "an actual btrfs volume according to stat: " +\
                         str(path)
                    print_msg(nontrivial_error + "\n\nError details: " +\
                        details, color="red")
                    raise ValueError(details)
                return True
        try:
            output = subprocess.check_output([
                SystemInfo.locate_binary("stat"),
                "-c", "%i", path]).decode('utf-8', 'ignore').strip()
        except subprocess.CalledProcessError as e:
            pass
        if output == "256":
            # Stat says it's a subvolume, although we don't think it is!
            details = "stat reports path as btrfs volume, but " +\
                "we didn't detect respective entry from btrfs tool: " +\
                str(path)
            print_msg(nontrivial_error + "\n\nError details: " + \
                details, color="red")
            raise ValueError(details)
        return False


    @staticmethod
    def docker_compose_path():
        """ Locate docker-compose binary and return its path, or exit process
            with error if not available.
        """
        bin_path = SystemInfo.locate_binary("docker-compose")
        if bin_path != None:
            return bin_path
        print("docker-rudder: error: no docker-compose found. " + \
            "Is it installed?")
        sys.exit(1)

    @staticmethod
    def btrfs_path():
        """ Locate btrfs helper tool binary and return its path, or return
            None if not found.
        """
        bin_path = SystemInfo.locate_binary("btrfs")
        if bin_path != None:
            return bin_path
        return None

    @staticmethod
    def get_fs_mount_root(path):
        """ Find out where the mount point of the filesystem is located of the
            given file path. (e.g. for a path "/home/myuser/somefile"
            located inside a home partition mounted at /home/, this would
            return "/home/") The information is probed using the "df" tool.
        """
        # Make the path absolute and make sure it leads to an existing thing:
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise ValueError("given path does not exist: " + str(path))

        # If this is a symlink itself, make sure the parent folder is
        # converted to the actual disk path (-> all parent symlinks are
        # resolved to actual disk paths):
        if os.path.islink(path):
            element_name = os.path.basename(os.path.normpath(path))
            parent = os.path.normpath(os.path.join(
                os.path.normpath(path), ".."))
            path = os.path.join(os.path.normpath(parent), element_name)
        else:
            # Not a symlink itself. Simply make sure it's the actual real
            # path on disk (-> all symlinks in the path are resolved):
            path = os.path.realpath(path)

        # Check again to make sure the real disk path is an existing thing:
        if not os.path.exists(path):
            raise ValueError(
                "converted real path does not exist: " + str(path))

        # Run "df" to find the mount point:
        output = subprocess.check_output([
            SystemInfo.locate_binary("df"), path]).\
            decode("utf-8", "ignore").strip()

        # Skip first line:
        if output.find("\n") <= 0:
            raise RuntimeError("failed to parse df output")
        output = output[output.find("\n")+1:]

        # Skip past first entry:
        skip_pos = output.find(" ")
        if skip_pos <= 0 or skip_pos >= len(output):
            raise RuntimeError("failed to parse df output")
        output = output[skip_pos+1:].strip()

        # Skip past all entries not starting with /
        while True:
            fwslash = output.find("/")
            spacepos = output.find(" ")
            if fwslash < 0:
                raise RuntimeError("failed to parse df output")
            if spacepos >= 0 and spacepos < fwslash:
                output = output[spacepos+1:].strip()
                continue
            break

        if not output.startswith("/"):
            raise RuntimeError("failed to parse df output")
        return output

    @staticmethod
    def filesystem_type_at_path(path):
        """ Find out the filesystem a given directory or file is on and return
            the name (e.g. "ext4", "btrfs", ...). The information is probed
            using the "df" tool.
        """
        if not os.path.exists(path):
            raise ValueError("given path does not exist: " + str(path))
        output = subprocess.check_output([
            SystemInfo.locate_binary("df"), path]).\
            decode("utf-8", "ignore")

        # Skip first line:
        if output.find("\n") <= 0:
            raise RuntimeError("failed to parse df output")
        output = output[output.find("\n")+1:]

        # Get first word being the FS of the path:
        end_pos = output.find(" ")
        if end_pos <= 0:
            raise RuntimeError("failed to parse df output")
        device_of_path = output[:end_pos]
        if device_of_path == "-":
            # We can't find out the filesystem of this path.
            # -> try to find out the parent!
            get_parent = os.path.normpath(path + "/../")
            if get_parent == os.path.normpath(path) or path == "/":
                # We are already at the root.
                return None
            return SystemInfo.filesystem_type_at_path(os.path.normpath(
                path + "/../"))

        output = subprocess.check_output([
            SystemInfo.locate_binary("mount")]).\
            decode("utf-8", "ignore")
        for line in output.split("\n"):
            if not line.startswith(device_of_path + " "):
                continue
            i = len(line) - 1
            while not line[i:].startswith(" type ") and i > 0:
                i -= 1
            if not line[i:].startswith(" type "):
                raise RuntimeError("failed to parse mount output")
            fs_type = line[i + len(" type "):].strip()
            if fs_type.find(" ") > 0:
                fs_type = fs_type[:fs_type.find(" ")].strip()
            return fs_type
        raise RuntimeError('failed to find according mount entry')

    @staticmethod
    def btrfs_subvolume_stat_check(path):
        """ Checks whether something that is supposedly a btrfs volume is
            also identified as such by the "stat" tool (returns True) or not
            (returns False).
        """
        output = subprocess.check_output([locate_binary("stat"),
            "-c", "%i", path]).decode('utf-8', 'ignore').strip()
        return (output == "256")

def print_msg(text, service=None, container=None, color="blue"):
    """ Print out a nicely formatted message prefixed with
        [docker-rudder] and/or possibly a service or container name.

        You can specify a signal color, with red and yellow changing the
        displayed message type from INFO to ERROR or WARNING respectively.
    """
    info_msg = "INFO"
    if color == "red":
        info_msg = "\033[1;31mERROR"
    elif color == "yellow":
        info_msg = "\033[1;33mWARNING"
    elif color == "green":
        info_msg = "SUCCESS"

    def color_code():
        part = "\033[1;"
        if color == "blue":
            part += "34"
        elif color == "red":
            part += "31"
        elif color == "yellow":
            part += "33"
        elif color == "green":
            part += "32"
        elif color == "white":
            part += "37"
        return part + "m"

    service_part = ""
    if service != None and len(service) > 0:
        service_part = "\033[0m" + color_code() + service
        if container != None:
            service_part = service_part + "/" + container

    docker_services_part = ""
    if service == None or len(service) == 0:
        docker_services_part = color_code() + "docker-rudder"

    initial_length = len("[docker-rudder")
    if service != None:
        initial_length = len("[" + service)
        if container != None:
            initial_length += len("/" + container)
    initial_length += len("] ") 

    text_color = "\033[0m"
    if color == "yellow" or color == "red":
        text_color += color_code()

    # Split lines according to \n\n:
    lines = [line.strip() for line in text.split("\n\n")]

    # First line with complicated lead up:
    print_text = "\033[0m\033[1m[\033[0m" + docker_services_part + \
        service_part + "\033[0m\033[1m] " + info_msg +\
        " " + text_color
    first_line = True
    for line in lines:
        if first_line:
            print_text += textwrap.fill(line, width=70,
                initial_indent=(" " * initial_length),
                subsequent_indent=(" " * initial_length))[
                initial_length:]
            first_line = False
        else:
            print_text += "\n" + textwrap.fill(line, width=70,
                initial_indent=(" " * initial_length),
                subsequent_indent=(" " * initial_length))
    print_text += "\033[0m"
    print(print_text)

class DataVolumeMount(object):
    """ This represents a docker volume mount with the known information
        about this mount. This is used by the DataVolume class below to
        identify the various places it is mounted on the host's disk
    """
    def __init__(self, host_path=None,
            mount_container=None, mount_container_filesystem_path=None):
        self.host_path = host_path
        self.container = mount_container
        self.mount_container_filesystem_path = mount_container_filesystem_path

    def __repr__(self):
        if self.host_path != None:
            return self.host_path
        return "<unspecified volume mount of " + str(container) + ">"

class DataVolume(object):
    """ This specifies the known information about a data volume. A volume
        can be either with no host directory mount and system-wide/reusable
        with a name, no host directory mount and system-wide/reusable with a
        random id, or with a host directory mount (which means it is usually
        "owned" by just one specific containers).
    """
    def __init__(self, known_host_mount=None, id=None, name=None):
        self.known_host_mount = known_host_mount
        self.id = id
        self.name = None
        self.owning_container = None
        self.owning_container_path = None
        self.specified_in_yml = False

    def __hash__(self):
        return hash(str(self.id) + str(self.owning_container_path) +\
            str(self.known_host_mount))

    def __repr__(self):
        if self.id != None:
            return str(self.id)
        if self.owning_container_path != None:
            return str(self.owning_container_path)
        if self.known_host_mount != None:
            return str(self.known_host_mount)
        return "<DataVolume object with unknown mount and unknown id>"

    def __eq__(self, other):
        if other == None:
            return False
        if not hasattr(other, "id") and not hasattr(other, "name"):
            return False
        if not hasattr(other, "known_host_mount"):
            return False
        if self.id != None and other.id == self.id:
            return True
        if self.known_host_mount != None and \
                self.known_host_mount == other.known_host_mount:
            return True
        if self.owning_container != None and \
                self.owning_container_path != None:
            if self.owning_container == other.owning_container and \
                    os.path.normpath(self.owning_container_path) ==\
                    os.path.normpath(other.owning_container_path):
                return True
        return False

    def _set_owning_container(self, container, container_fs_path=None):
        """ Set the container this volume is associated with. Only for
            internal use. This is called for volumes that are specified as
            host mounts in docker-compose.yml and don't have a proper
            system-wide volume identifier/name for use in other containers.

            DEVELOPER NOTE (Jonas Thiem):
            In theory, a volume that is purely a host directory mount can be
            used by multiple containers of course. However, right now
            docker-rudder only builds up volume information per container, and
            therefore this container-centric approach is sufficient for now.
            However, later this should be changed to allow multiple owning
            containers.
        """
        self.owning_container = container
        if container_fs_path != None:
            self.owning_container_path = container_fs_path

    @property
    def mounts(self):
        """ Return the known places this DataVolume is mounted at. Returns
            a list of DataVolumeMount instances (or an empty list).
        """
        if self.known_host_mount != None:
            if self.owning_container != None:
                return [ DataVolumeMount(host_path=self.known_host_mount,
                    mount_container=self.owning_container,
                    mount_container_filesystem_path=\
                    self.owning_container_path) ]
            else:
                return [ DataVolumeMount(host_path=self.known_host_mount) ]
        elif self.owning_container != None:
            return [ DataVolumeMount(mount_container=self.owning_container,
                mount_container_filesystem_path=\
                self.owning_container_path) ]
        else:
            return []

class ServiceDependency(object):
    """ This class holds the info describing the dependency to another
        service's container.
    """
    def __init__(self, other_service_name, other_service_path,
            other_service_container_name):
        self.service_name = other_service_name
        self.service_path = other_service_path
        self.container_name = other_service_container_name

    @property
    def container(self):
        """ The actual container instance to start/stop the container which
            is the target of this dependency.

            Please note this might be unavailable if the container doesn't
            belong to any known service, in which case accessing this property
            will raise ValueError.
        """
        if self.service_name == None:
            raise ValueError("the service that provides this dependency " +\
                "isn't known")
        return ServiceContainer(self.service_name, self.service_path,
            self.container_name)

    def __repr__(self):
        if self.service_name != None:
            return self.service_name + "/" + self.container_name
        return self.container_name + " (unknown service!!)"

class ServiceContainer(object):
    """ An instance of this class holds the info for a service's container.
        It can be used to e.g. obtain the system-wide docker container name,
        or the directory for the respective docker-compose.yml where
        docker-compose commands can be run.
    """
    def __init__(self, service_name, service_path, container_name,
            image_name=None):
        self.service_name = service_name
        self.service_path = service_path
        self.name = container_name
        self._known_image_name = image_name

    def __repr__(self):
        return self.service_name + "/" + self.name

    def __hash__(self):
        return hash(self.service_name + \
            os.path.normpath(os.path.abspath(self.service_path)) + \
            self.name)

    def __eq__(self, other):
        if other is None:
            return False
        if not hasattr(other, "service_name") or not hasattr(other,
                "service_path") or not hasattr(other, "name"):
            return False
        if other.service_name != self.service_name:
            return False
        if (os.path.normpath(os.path.abspath(self.service_path)) !=
                os.path.normpath(os.path.abspath(other.service_path))):
            return False
        if other.name != self.name:
            return False
        return True

    def __neq__(self, other):
        return not self.__eq__(other)

    @property
    def potentially_lost_volumes(self):
        # Find volumes that will be dangling and not re-attached by
        # docker-compose on container recreation:
        potentially_lost_volumes = []
        for volume in self.volumes:
            if volume.name != None and volume.specified_in_yml:
                # Named volume specified in .yml -> we should fine
                continue
            host_mounted = False
            for mount in volume.mounts:
                if mount.host_path != None:
                    host_mounted = True
                    break
            if host_mounted and volume.specified_in_yml:
                # Host mount specified in .yml -> we should be fine
                continue
            potentially_lost_volumes.append(volume)
        return potentially_lost_volumes

    def launch(self, force_container_recreation=False):
        if self.running:
            return

        # Warn about volumes that might be dangling after recreation
        potentially_lost_volumes = self.potentially_lost_volumes
        if len(potentially_lost_volumes) == 0 or \
                force_container_recreation:
            if len(potentially_lost_volumes) > 0:
                print_msg("some volumes might be potentially dangling, " +\
                    "but container recreation was forced by user",
                    service=self.service.name, container=self.name,
                    color="yellow")
            subprocess.check_call([SystemInfo.docker_compose_path(),
                "rm", "-f", self.name],
                cwd=self.service.service_path)
            subprocess.check_call([SystemInfo.docker_compose_path(),
                "build", self.name],
                cwd=self.service.service_path)
        else:
            for vol in potentially_lost_volumes:
                print_msg("ONLY LIMITED RESTART, CONTAINER WONT BE "+\
                    "UPDATED TO NEWEST IMAGE. " +\
                    "Cannot recreate container safely: " +\
                    "container has a volume that might be lost " +\
                    "or dangling after recreation: "+\
                    "" + str(vol),
                    service=self.service.name,
                    container=self.name, color="yellow")
        subprocess.check_call([SystemInfo.docker_compose_path(),
            "up", "-d", self.name],
            cwd=self.service.service_path)

    def stop(self):
        subprocess.check_call([SystemInfo.docker_compose_path(),
            "stop", self.name], cwd=self.service.service_path)

    @property
    def running(self):
        names = self.service._get_running_service_container_names()
        return (self.name in names)

    @property
    def service(self):
        """ The service this container belongs to. """
        return Service(self.service_name, self.service_path)

    @property
    def default_container_name(self):
        fpath = os.path.normpath(os.path.abspath(self.service_path))
        return (os.path.basename(fpath).replace("-", "")
            + "_" + str(self.name) + "_1")

    @property
    def image_name(self):
        if self._known_image_name != None:
            return self._known_image_name
        return self.default_container_name

    @property
    def current_running_instance_name(self):
        return self.default_container_name

    def _get_active_volume_directories(self, rw_only=False):
        """ Get the volumes active in this container. If the container hasn't
            been started before, this might raise a ValueError since this
            information is obtained via docker inspection of the container.
        """
        try:
            output = subprocess.check_output([SystemInfo.docker_path(),
                "inspect", self.current_running_instance_name],
                stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            raise ValueError("container not created - you might need to "+\
                "launch it first")
        result = json.loads(output.decode("utf-8", "ignore"))
        volumes_list = []
        if not rw_only:
            if "Volumes" in result[0]:
                if result[0]["Volumes"] != None:
                    for volume in result[0]["Volumes"]:
                        volumes_list.append(volume)
            else:
                if result[0]["Config"]["Volumes"] != None:
                    for volume in result[0]["Config"]["Volumes"]:
                        volumes_list.append(volume)
        if rw_only:
            if "Volumes" in result[0]:
                if not ("VolumesRW" in result[0]):
                    return []
                for volume in result[0]["Volumes"]:
                    if not result[0]["VolumesRW"][volume]:
                        volumes_list.remove(volume)
            else:
                if not ("VolumesRW" in result[0]["Config"]):
                    return []
                for volume in result[0]["Config"]["Volumes"]:
                    if not result[0]["Config"]["VolumesRW"][volume]:
                        volumes_list.remove(volume)
        return volumes_list

    def _map_host_dir_to_container_volume_dir(self, volume_dir):
        """ Attempts to find the volume mount information and return the host
            directory currently mapped to the given container volume path.
        """
        try:
            output = subprocess.check_output([SystemInfo.docker_path(),
                "inspect",
                self.current_running_instance_name],
                stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            raise ValueError("container not created - you might need to "+\
                "launch it first")
        result = json.loads(output.decode("utf-8", "ignore"))
        for mount in result[0]["Mounts"]:
            if os.path.normpath(mount["Destination"]) == \
                    os.path.normpath(volume_dir):
                return mount["Source"]
        return None

    def _active_volumes(self, rw_only=False):
        """ Get all the active volumes of this container. May return an empty
            or outdated list if the container is stopped.

            Returns a list of tuples (host file system path,
                container filesystem path).
        """

        try:
            return [(self._map_host_dir_to_container_volume_dir(
                volume), volume) \
                for volume in \
                self._get_active_volume_directories(rw_only=rw_only)]
        except ValueError:
            return []

    def _config_specified_volumes(self, rw_only=False):
        """ Attempt to parse and return all volumes used by this container
            from the according docker-compose.yml.

            Returns a list of tuples (host file system path,
                container filesystem path).
        """
        volumes = []
        def parse_volume_line(parts):
            # Check if read-only or not:
            rwro = "rw"
            if len(parts) >= 3:
                if "ro" in [item.strip() for item in parts[2].split(",")]:
                    rwro = "ro"

            # Make sure path is absolute:
            if len(parts) >= 2:
                if (parts[0].startswith("/") or parts[0].startswith("./") or
                        parts[0].startswith("../")) \
                        and not os.path.isabs(parts[0]):
                    parts[0] = os.path.join(
                        os.path.realpath(self.service.service_path), parts[0])
                    parts[0] = os.path.normpath(os.path.abspath(parts[0]))

            # Add to list:
            if len(parts) >= 2 and (rwro == "rw" or not rw_only):
                volumes.append((parts[0], parts[1]))

        f = open(os.path.join(self.service_path,
            "docker-compose.yml"), "rb")
        try:
            contents = f.read().decode("utf-8", "ignore").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")
        finally:
            f.close()
        try:
            parsed_obj = yaml.safe_load(contents)
            if self.name in parsed_obj and \
                    "volumes" in parsed_obj[self.name]:
                for line in parsed_obj[self.name]["volumes"]:
                    parse_volume_line(parts.split(":")) 
        except NameError:
            pass

        in_relevant_container_section = False
        in_volume_list = False
        for line in contents.split("\n"):
            if line.strip() == "":
                continue

            # Figure out whether we are entering/leaving the relevant container
            # section:
            if line.startswith(self.name + ":"):
                in_relevant_container_section = True
            elif not line.startswith(" ") and \
                    not line.startswith("\t") and \
                    line.endswith(":"):
                in_relevant_container_section = False

            if line.startswith(" ") and (line.strip().startswith("volumes ")
                    or line.strip().startswith("volumes:")):
                in_volume_list = True

                # Parse remaining stuff in line:
                i = line.find("volumes")
                line = line[i+len("volumes"):].strip()
                if line.startswith(":"):
                    line = line[1:].strip()
                if len(line) == 0:
                    continue

            if in_volume_list:
                if not line.strip().startswith("-"):
                    in_volume_list = False
                    continue
                line = line[line.find("-")+1:].strip()
                if line.startswith("\"") and line.endswith("\""):
                    line = line[1:-1]
                parts = line.split(":")
                parse_volume_line(parts) 
        return volumes

    def _get_volumes(self, rw_only=False):
        volumes = []
        def add_volume(vol_line, from_yml=False):
            host_mount_path = None
            known_name = None
            if vol_line[0] != None:
                if vol_line[0].startswith("/") or \
                        vol_line[0].startswith("./") or \
                        vol_line[0].startswith("../"):
                    host_mount_path = vol_line[0]
                else:
                    known_name = vol_line[0]
            vol = DataVolume(known_host_mount=host_mount_path,
                id=known_name, name=known_name)
            vol._set_owning_container(self, vol_line[1])
            vol.specified_in_yml = from_yml
            volumes.append(vol)

        volumes_in_yml = set()
        vol_lines = {}
        for vol1 in self._active_volumes(rw_only=rw_only):
            if not vol1[0] in vol_lines or vol_lines[vol1[0]] == None:
                vol_lines[vol1[0]] = vol1[1]
        for vol2 in self._config_specified_volumes(rw_only=rw_only):
            if not vol2[0] in vol_lines or vol_lines[vol2[0]] == None:
                vol_lines[vol2[0]] = vol2[1]
            volumes_in_yml.add(vol2[0])
        for vol_line in vol_lines:
            if vol_line in volumes_in_yml:
                add_volume((vol_line, vol_lines[vol_line]), from_yml=True)
            else:
                add_volume((vol_line, vol_lines[vol_line]))
        return volumes

    @property
    def volumes(self):
        """ Returns a list of DataVolume instances representing all volumes
            which are in any sort of known connection to this container.
        """
        return self._get_volumes(rw_only=False)

    @property
    def rw_volumes(self):
        """ Returns a list of DataVolume instances representing all volumes
            which are in any sort of known connection to this container.
            Limited to all the volumes that are actually writable for the
            container, excluding the read-only ones.
        """
        return self._get_volumes(rw_only=True)

    @property
    def dependencies(self):
        # Collect all external_links and links container references:
        external_links = self.service._external_docker_compose_links(
            self.name)
        internal_links = self.service._internal_docker_compose_links(
            self.name)
        dependencies = []

        # Go through all external links
        for link in external_links:
            link_name = link.partition(":")[0]
            target_found = False
            # See if we can find the target service:
            for service in Service.all():
                for container in service.containers:
                    if (service == self.service and 
                            link_name == container.name) or \
                            link_name == container.default_container_name:
                        target_found = True
                        dependencies.append(ServiceDependency(
                            service.name, service.service_path,
                            container.name))
            if not target_found:
                dependencies.append(ServiceDependency(
                    None, None, link_name))

        # Go through all internal links:
        for link in internal_links:
            link_name = link.partition(":")[0]
            target_found = False
            # See if we can find the target service:
            for container in self.service.containers:
                 if link_name == container.name:
                    target_found = True
                    dependencies.append(ServiceDependency(
                        self.service_name, self.service_path,
                        container.name))
            if not target_found:
                dependencies.append(ServiceDependency(
                    None, None, link_name))
        return dependencies

class Service(object):
    """ This class holds all the info and helper functionality for managing
        a service, whereas a service is a group of containers with one single
        docker-compose.yml stored in a subfolder in the global service
        directory.
    """
    def __init__(self, service_name, service_path):
        self.name = service_name
        self.service_path = service_path

    def __repr__(self):
        return self.name + " service at " + str(self.service_path)

    def __eq__(self, other):
        if not hasattr(other, "name") and not hasattr(other,
                "service_path"):
            return False
        if self.name == other.name:
            if os.path.normpath(os.path.abspath(self.service_path)) == \
                    os.path.normpath(os.path.abspath(other.service_path)):
                return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name + "/" + os.path.normpath(
            os.path.abspath(self.service_path)))

    def is_running(self):
        return len(self._get_running_service_container_names()) > 0

    @staticmethod
    def find_by_name(name):
        for service in Service.all():
            if service.name == name:
                return service
        return None

    @staticmethod
    def all_containers():
        result = set()
        for service in Service.all():
            for container in service.containers:
                result.add(container)
        return list(result)

    @staticmethod
    def all():
        """ Get a global list of all detected services.
        """
        services = []
        def scan_dir(d):
            if not os.path.isdir(d):
                return
            d = os.path.abspath(d)
            for f in os.listdir(d):
                if not os.path.isdir(os.path.join(d, f)):
                    continue
                if not os.path.exists(os.path.join((os.path.join(d, f)),
                        "docker-compose.yml")):
                    continue
                services.append(Service(f, 
                    os.path.normpath(os.path.join(d, f))))
        scan_dir(os.getcwd())
        if os.path.exists("/usr/share/docker-services"):
            scan_dir("/usr/share/docker-services")
        if os.path.exists("/srv"):
            scan_dir("/srv")
        return services

    def _fix_container_name(self, container_name):
        """ !! BIG HACK !!
            Sometimes docker-compose gives us just a shortened name for a
            container. While I am not fully aware of the algorithm, I assume
            it will usually still be unique. In this function, we try to get
            back the full unshortened name.
        """
        assert(container_name != None)
        for container in self.containers:
            if container.current_running_instance_name == container_name \
                    or container.default_container_name == container_name:
                return container.name
        matched_name = None
        for container in self.containers:
            if container.current_running_instance_name.startswith(
                    container_name):
                if matched_name != None:
                    raise RuntimeError("encountered unexpected non-unique " +\
                        "container label trying to find the full name for "+\
                         "\"" + str(container_name) + "\": variant A: " +\
                        str(matched_name) + ", variant B: " +\
                        str(container.name))
                matched_name = container.name
        return matched_name

    def _get_running_service_container_names(self):
        """ Get all running containers of the given service.
            Returns a list of container ids.
        """
        running_containers = []
        try:
            env = os.environ.copy()
            env["COLUMNS"] = "200"
            output = SystemInfo.check_output_with_isolated_pty([
                SystemInfo.docker_compose_path(), "ps"],
                cwd=self.service_path, stderr=subprocess.STDOUT,
                timeout=10, env=env).\
                decode("utf-8", "ignore")
        except subprocess.CalledProcessError as e:
            output = e.output.decode("utf-8", "ignore")
            if output.find("client and server don't have same version") >= 0:
                print_msg("error: it appears docker-compose is " +\
                    "installed with " +\
                    "a version incompatible to docker.", color="red")
                sys.exit(1)
            raise e
        output = output.\
            replace("\r", "\n").replace("\n\n", "").split("\n")

        skipped_past_dashes = False
        for output_line in output:
            if len(output_line.strip()) == 0:
                continue
            if output_line.startswith("---------"):
                skipped_past_dashes = True
                continue
            output_line = output_line.strip()
            if len(output_line) > 0 and output_line.find(" Up") > 0:
                # this is a running container!
                space_pos = output_line.find(" ")
                running_containers.append(output_line[:space_pos])
        l = [self._fix_container_name(container) \
            for container in running_containers]
        return l

    def _internal_docker_compose_links(self, container_name):
        """ Attempt to parse and return all containers referenced as external
            links used by a service from the respective docker-compose.yml of
            that service.
        """
        links = []
        with open(os.path.join(self.service_path,
                "docker-compose.yml"), "rb") as f:
            contents = f.read().decode("utf-8", "ignore").\
                replace("\r\n", "\n").\
                replace("\r", "\n")
        try:
            parsed_obj = yaml.safe_load(contents)
            if not container_name in parsed_obj:
                raise ValueError("no such container found: " +\
                    str(container_name))
            if not "links" in parsed_obj[container_name]:
                return []
            return [entry.partition(":")[0] for entry in \
                parsed_obj[container_name]["links"]]
        except NameError:
            # Continue with the YAML parsing hack:
            pass
        in_relevant_container_section = False
        in_links_list = False
        for line in contents.split("\n"):
            if line.strip() == "":
                continue

            # Figure out whether we are entering/leaving the relevant container
            # section:
            if line.startswith(container_name + ":"):
                in_relevant_container_section = True
            elif not line.startswith(" ") and \
                    not line.startswith("\t") and \
                    line.endswith(":"):
                in_relevant_container_section = False

            # Detect external_links: subsection:
            if line.startswith(" ") and (line.strip().\
                    startswith("links ")
                    or line.strip().startswith("links:")):
                in_links_list = True

                # Parse remaining stuff in line:
                i = line.find("links")
                line = line[i+len("links"):].strip()
                if line.startswith(":"):
                    line = line[1:].strip()
                if len(line) == 0:
                    continue

            # Parse entries:
            if in_links_list:
                if not line.strip().startswith("-"):
                    in_links_list = False
                    continue
                line = line[line.find("-")+1:].strip()
                if line.startswith("\"") and line.endswith("\""):
                    line = line[1:-1].strip()
                parts = line.split(":")
                if in_relevant_container_section:
                    links.append(parts[0])
        return links


    def _external_docker_compose_links(self, container_name):
        """ Attempt to parse and return all containers referenced as external
            links used by a service from the respective docker-compose.yml of
            that service.
        """
        external_links = []
        with open(os.path.join(self.service_path,
                "docker-compose.yml"), "rb") as f:
            contents = f.read().decode("utf-8", "ignore").\
                replace("\r\n", "\n").\
                replace("\r", "\n")
        try:
            parsed_obj = yaml.safe_load(contents)
            if not container_name in parsed_obj:
                raise ValueError("no such container found: " +\
                    str(container_name))
            if not "external_links" in parsed_obj[container_name]:
                return []
            return [entry.partition(":")[0] for entry in \
                parsed_obj[container_name]["external_links"]]
        except NameError:
            # Continue with the YAML parsing hack:
            pass
        in_relevant_container_section = False
        in_external_links_list = False
        for line in contents.split("\n"):
            if line.strip() == "":
                continue

            # Figure out whether we are entering/leaving the relevant container
            # section:
            if line.startswith(container_name + ":"):
                in_relevant_container_section = True
            elif not line.startswith(" ") and \
                    not line.startswith("\t") and \
                    line.endswith(":"):
                in_relevant_container_section = False

            # Detect external_links: subsection:
            if line.startswith(" ") and (line.strip().\
                    startswith("external_links ")
                    or line.strip().startswith("external_links:")):
                in_external_links_list = True

                # Parse remaining stuff in line:
                i = line.find("external_links")
                line = line[i+len("external_links"):].strip()
                if line.startswith(":"):
                    line = line[1:].strip()
                if len(line) == 0:
                    continue

            # Parse entries:
            if in_external_links_list:
                if not line.strip().startswith("-"):
                    in_external_links_list = False
                    continue
                line = line[line.find("-")+1:].strip()
                if line.startswith("\"") and line.endswith("\""):
                    line = line[1:-1].strip()
                parts = line.split(":")
                if in_relevant_container_section:
                    external_links.append(parts[0])
        return external_links

    def get_running_containers(self):
        """ Get only the containers of this service which are currently up and
            running.
        """
        running_names = self._get_running_service_container_names()
        running_containers = []
        for container in self.containers:
            if container.name in running_names:
                running_containers.append(container)
        return running_containers

    @property
    def rw_volumes(self):
        volume_set = set()
        for container in self.containers:
            for volume in container.rw_volumes:
                volume_set.add(volume)
        return list(volume_set)

    @staticmethod
    def global_clean_up(ask=True):
        """ This function will check the status of all docker containers, and
            then irrevocably delete all containers that aren't running.
            It will also delete all dangling volumes.
        """
        if ask:
            answer = input("\033[1m!! DANGER !!\033[0m\n"+\
                "This will irrevocably delete all "+\
                "stopped containers. "+\
                "It will also delete all volumes that are neither "+\
                "a host-mounted directory, nor are currently owned by any "+\
                "currently existing container (in short, all dangling "+\
                "volumes).\n\n(Avoid this warning next time " +\
                "with --force)\n\n" +\
                "Are you sure you want to continue? [Enter y/N]")
            if not answer == "y" and not answer == "Y":
                print("docker-rudder: error: cleaning was aborted "+\
                    "by user.", file=sys.stderr)
                sys.exit(1)
        print_msg("cleaning up stopped containers...")
        output = subprocess.check_output([
            SystemInfo.docker_path(), "ps", "-a"])
        output = output.decode("utf-8", "ignore").\
            replace("\r", "\n").\
            replace("\n ", "\n").replace(" \n", "\n").replace("\n\n", "\n")
        while output.find("   ") >= 0:
            output = output.replace("   ", "  ")
        output = output.replace("  ", "\t")
        output = output.replace("\t ", "\t").replace(" \t", "\t")
        output = output.split("\n")
        for output_line in output:
            if len(output_line.strip()) == 0:
                continue
            parts = output_line.split("\t")
            if parts[0] == "CONTAINER ID":
                continue
            if len(parts) < 5 or (not parts[3].endswith("ago")):
                print_msg("WARNING: skipping container " + parts[0] +\
                    ", cannot locate STATUS column")
                continue
            if parts[0].find(" ") >= 0:
                print_msg("WARNING: skipping container with invalid " +\
                    "container id: " + parts[0])
                continue
            if len(parts) == 6 and parts[4].find("_") >= 0:
                parts = parts[:4] + [ '' ] + parts[4:]
            if parts[4] == "" or parts[4].startswith("Exited "):
                print_msg("deleting stopped container " + parts[0] + "...")
                subprocess.check_output([SystemInfo.docker_path(),
                    "rm", parts[0]])
        print_msg("cleaning up unneeded images...")
        subprocess.check_output([SystemInfo.docker_path(), "rmi",
            "$(docker images -aq"], shell=True)
        print_msg("cleaning up dangling volumes...")
        dangling_vols = subprocess.check_output([SystemInfo.docker_path(),
            "volume", "ls", "-qf", "dangling=true"])
        for vol in dangling_vols.splitlines():
            vol = vol.strip()
            if len(vol) == 0:
                continue
            subprocess.check_output([SyStemInfo.docker_path(),
                "volume", "rm", vol])

    @property
    def containers(self):
        """ Get all containers specified for the given service's
            docker-compose.yml.
        """
        with open(os.path.join(self.service_path,
                "docker-compose.yml"), "rb") as f:
            contents = f.read().decode("utf-8", "ignore").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")

        # See if YAML parsing works (depending on pyyaml being installed):
        parsed_obj = None
        try:
            parsed_obj = yaml.safe_load(contents)
        except NameError:
            pass

        results = []

        # Assemble results if YAML parsing worked:
        if parsed_obj != None:
            for container_name in parsed_obj:
                for property_name in parsed_obj[container_name]:
                    value = parsed_obj[container_name][property_name]
                    if property_name == "image":
                        # This container is constructed from an image:
                        image_name = value

                        # This is the resulting container name:
                        results.append(ServiceContainer(
                            self.name, self.service_path,
                            container_name,
                            image_name=image_name))
                    elif property_name == "build":
                        results.append(ServiceContainer(
                            self.name, self.service_path,
                            container_name))
            return results

        # Parse with our hacky YAML pseudo-parsing:
        smallest_observed_indentation = 999
        def count_indentation(line):
            i = 0
            while i < len(line) and line[i] == " ":
                i += 1
            return i
        current_container_name = None
        i = 0
        for line in contents.split("\n"):
            next_line = None
            i += 1
            if i < len(contents):
                next_line = contents[i]

            if not (line.startswith(" ")):
                # This is the container name:
                current_container_name = line.partition(":")[0].strip().\
                    partition(" ")[0]
            elif count_indentation(line) <= smallest_observed_indentation:
                # This is a line inside the service declaration.
                keyword = line.partition(":")[0].strip().partition(" ")[0]
                value = line.partition(":")[2].strip()
                if len(value) == 0:
                    value = next_line.strip()
                if keyword == "build":
                    # This service build from a directory.
                    # Get the name of the directory:
                    build_path = os.path.normpath(\
                        os.path.join(service_path, value))
                    while build_path.endswith("/"):
                        build_path = build_path[:-1]
                    build_name = os.path.basename(build_path).replace("-", "")
                    
                    # This is the resulting container name:
                    results.append(ServiceContainer(
                        self.name, self.service_path,
                        current_container_name))
                elif keyword == "image":
                    # This container is constructed from an image:
                    image_name = value

                    # This is the resulting container name:
                    results.append(ServiceContainer(
                        self.name, self.service_path,
                        current_container_name,
                        image_name=image_name))
        return results

class FailedLaunchTracker(object):
    def __init__(self):
        self.access_lock = threading.Lock()
        self.contents = set()

    def __len__(self):
        self.access_lock.acquire()
        result = len(self.contents)
        self.access_lock.release()
        return result

    def __contains__(self, item):
        self.access_lock.acquire()
        result = (item in self.contents)
        self.access_lock.release()
        return result

    def add(self, item):
        self.access_lock.acquire()
        self.contents.add(item)
        self.access_lock.release()

class LaunchThreaded(threading.Thread):
    """ A helper to launch a service and wait for the launch only for a
        limited amount of time, and moving the launch into a background
        thread if it takes too long.
    """
    
    def __init__(self, container, failed_launch_tracker=None,
            force_container_recreation=False, do_on_success=None,
            do_on_failure=None):
        super().__init__()
        self.container = container
        self.failed_launch_tracker = failed_launch_tracker
        self.path = self.container.service.service_path
        self.force_container_recreation = force_container_recreation
        def do_nothing(self):
            pass
        self.do_on_success = do_on_success
        if self.do_on_success is None:
            self.do_on_success = do_nothing
        self.do_on_failure = do_on_failure
        if self.do_on_failure is None:
            self.do_on_failure = do_nothing

    def run(self):
        try:
            # Fix permissions if we have instructions for that:
            perms = Permissions(self.container.service)
            perm_info = perms.get_permission_info_from_yml()
            if ("owner" in perm_info["livedata-permissions"]) \
                    and os.path.exists(os.path.join(
                        self.path, "livedata")):
                owner = perm_info["livedata-permissions"]["owner"]
                try:
                    owner = int(owner)
                except TypeError:
                    # Must be a username.
                    try:
                        owner = getpwnam(owner).pw_uid
                    except KeyError:
                        print_msg("invalid user specified for permissions: "+\
                            "can't get uid for user: " + owner, color="red")
                        raise RuntimeError("invalid user")
                for root, dirs, files in os.walk(os.path.join(self.path, \
                        "livedata")):
                    for f in (dirs + files):
                        fpath = os.path.join(root, f)
                        os.chown(fpath, owner, -1, follow_symlinks=False)

            # Get dependencies and see if they have all been launched:
            waiting_msg = False
            for dependency in self.container.dependencies:
                if not dependency.container.running:
                    if not waiting_msg:
                        waiting_msg = True
                    time.sleep(5)
                    while not dependency.container.running:
                        if self.failed_launch_tracker != None:
                            if dependency.container in \
                                    self.failed_launch_tracker:
                                print_msg("launch aborted due to failed " +\
                                    "dependency launch: " +\
                                    str(dependency),
                                    service=self.container.service.name,
                                    container=self.container.name,
                                    color="red")
                                self.failed_launch_tracker.add(
                                    self.container)
                                self.do_on_failure()
                                return
                        time.sleep(5)

            # Launch the service:
            print_msg("launching...", service=self.container.service.name,
                container=self.container.name, color="blue")
            try:
                self.container.launch(
                    force_container_recreation=\
                    self.force_container_recreation)
                time.sleep(1)
                if not self.container.running:
                    print_msg("failed to launch. (nothing running after " +\
                        "1 second)",\
                        service=self.container.service.name,
                        container=self.container.name, color="red")
                    if self.failed_launch_tracker != None:
                        self.failed_launch_tracker.add(self.container)
                    self.do_on_failure()
                    return
                print_msg("now running.",
                    service=self.container.service.name,
                    container=self.container.name, color="green")
                self.do_on_success()
            except subprocess.CalledProcessError:
                print_msg("failed to launch. (error exit code)",\
                    service=self.container.service.name,
                    container=self.container.name,
                    color="red")
                if self.failed_launch_tracker != None:
                    self.failed_launch_tracker.add(self.container)
                self.do_on_failure()
            except Exception as e:
                print_msg("failed to launch. (unknown error)",\
                    service=self.container.service.name,
                    container=self.container.name,
                    color="red")
                if self.failed_launch_tracker != None:
                    self.failed_launch_tracker.add(self.container)
                self.do_on_success()
                raise e
        except Exception as e:
            print("UNEXPECTED ERROR", file=sys.stderr)
            print("ERROR: " + str(e))
            traceback.print_exc()

    @staticmethod
    def attempt_launch(container, to_background_timeout=5,
            failed_launch_tracker=None, force_container_recreation=False,
            do_on_success=None, do_on_failure=None):
        """ Launch a given service and wait for it to run for a few seconds.
            If that isn't long enough for it to start running, return
            execution to possibly launch further services while this one is
            still busy launching.
        """

        # Start a new launch thread:
        launch_t = LaunchThreaded(container,
            failed_launch_tracker=failed_launch_tracker,
            do_on_success=do_on_success, do_on_failure=do_on_failure)
        launch_t.start()
        
        # Wait for it to complete:
        launch_t.join(to_background_timeout)
        if launch_t.isAlive():
            # This took too long, run in background:
            return launch_t
        return None

    @staticmethod
    def stop(container):
        """ Stop a service. """
        assert(container != None)
        container.stop()

    @staticmethod
    def wait_for_launches(threads):
        for launch_t in threads:
            if launch_t.isAlive():
                launch_t.join()

class Permissions:
    """ This represents permissions information parsed from a permssion.yml.
        This allows the administrator to specify permissions that should be
        applied to the live data folder (where usually read-write volumes are
        mounted).
    """
    def __init__(self, service):
        self.service = service

    def get_permission_info_from_yml(self):
        """ Get permission info for the given service
        """
        f = None
        try:
            f = open(os.path.join(self.service.service_path,
                "permissions.yml"), "rb")
        except Exception as e:
            return {"livedata-permissions" : {}}
        perm_dict = dict()
        current_area = None
        try:
            contents = f.read().decode("utf-8", "ignore").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")
        finally:
            f.close()

        # Try proper parsing with the YAML parser:
        try:
            parsed_obj = yaml.safe_load(contents)
            for k in parsed_obj:
                print_msg("warning: unrecognized permissions.yml " +\
                        "section: " + str(line),
                        service=self.service.name,
                        color="red")
            if not "livedata-permissions" in parsed_obj:
                parsed_obj["livedata-permissions"] = dict()
            return parsed_obj
        except NameError:
            pass

        # Hack for unavailable YAML parser:
        for line in contents.split("\n"):
            if line.strip() == "":
                continue
            if not line.startswith(" ") and not line.startswith("\t"):
                line = line.strip()
                if not line.endswith(":"):
                    print_msg("error: syntax error in permissions.yml: " +\
                        "colon expected",
                        service=self.service.name, color="red")
                    raise RuntimeError("invalid syntax")
                line = line[:-1]
                # this specifies the current area
                if line == "livedata-permissions":
                    current_area = "livedata-permissions"
                else:
                    print_msg("warning: unrecognized permissions.yml " +\
                        "section: " + str(line),
                        service=self.service.name,
                        color="red")
                    continue
                if not current_area in perm_dict:
                    perm_dict[current_area] = dict()
                continue
            elif line.startswith(" ") or line.startswith("\t"):
                if current_area == None:
                    print_msg("error: syntax error in permissions.yml: " +\
                        "unexpected value outside of block",
                        service=self.service.name, color="red")
                    raise RuntimeError("invalid syntax")
                k = line.partition(":")[0].strip()
                v = line.partition(":")[2].strip()
                perm_dict[current_area][k] = v

                continue
        
        # Make sure some stuff is present:
        if not "livedata-permissions" in perm_dict:
            perm_dict["livedata-permissions"] = dict()

        return perm_dict

parser = argparse.ArgumentParser(description=textwrap.dedent('''\
    docker-rudder: a tool to simply managing a larger collection of
    docker-compose.yml-specified container groups.'''
    ),
    formatter_class=DoubleLineBreakFormatter)
parser.add_argument("action",
    help=textwrap.dedent('''\
    Possible values:
    
    "list": list all known services.

    "start": start the service specified as argument (or "all" for all).

    "stop": stop the service specified as argument (or "all" for all).

    "restart": restart the service specified as argument (or "all" for all).

    "status": show more detailed status of the given service (or "all for
              all).

    "logs": output the logs of all the docker containers of the service
            specified as argument (or "all" for all).

    "shell": start an interactive shell in the specified service's specified
             subservice (parameters: <service> <subservice>) - the subservice
             is optional if the docker-compose.yml has just one
             container/subservice.

    "snapshot": store an atomic snapshot of the live data of the service
                specified as argument (from livedata/) in livedata-snapshots/

    "clean": clean up all stopped containers. THIS IS NOT REVERSIBLE. The
             docker images of course won't be touched.

    "dump-container-info" : dump all sorts of detailed container info as YAML.
                            Mainly useful if you want to process the combined
                            data computed by docker-rudder with another tool
                            of yours.
    ''')

    )
parser.add_argument("argument", nargs="*", help="argument(s) to given action")
parser.add_argument("--version", help="show version and quit",
    default=False, action="store_true",
    dest="show_version")
parser.add_argument("--force",
    default=False, action="store_true",
    help="Can be used to override various safety warnings. Consult the "+\
        "respective warning prompted by an action to understand the "+\
        "consequences",
    dest="force")
if len(" ".join(sys.argv[1:]).strip()) == 0:
    parser.print_help()
    sys.exit(1)
for arg in sys.argv[1:]:
    if arg == "--version" or arg == "-v" or arg == "-V":
        print("docker-rudder version " + str(TOOL_VERSION))
        sys.exit(0)
args = parser.parse_args()

ensure_docker = SystemInfo.docker_path()
ensure_docker_compose = SystemInfo.docker_compose_path()

def verify_service_names(names):
    """ This will evaluate all service names passed as command line arguments,
        and make sure they actually exist (and find out their path). "all" is
        interpreted and turned into a list of all services. The resulting
        validated list is returned.
    """
    specified_services = []
    for specified_service in names:
        if specified_service == "all":
            return list(services)
        found = None
        for service in services:
            if service['name'] == specified_service:
                found = service
                break
        if found == None:
            print("docker-rudder: error: no such service found: " +\
                specified_service, file=sys.stderr)
            sys.exit(1)
        specified_services.append(found)
    return specified_services

class Snapshots(object):
    def __init__(self, service):
        self.service = service

    def check_running_snapshot_transaction(self):
        if os.path.exists(os.path.join(
                self.service.service_path, ".docker-rudder-snapshot.lock")):
            output = subprocess.check_output(
                "ps aux | grep docker-rudder | grep -v grep | wc -l",
                shell=True).decode("utf-8", "ignore")
            if output.strip() != "1":
                # Another copy still running??
                return True
            print_msg("warning: stale snapshot lock found but no process " +\
                "appears to be left around, removing.",
                color="yellow", service=self.service.name)
            # No longer running, remove file:
            os.remove(os.path.join(self.service.service_path,
                ".docker-rudder-snapshot.lock"))
        return False

    @staticmethod
    def btrfs_tool_check():
        # Make sure the btrfs tool is working:
        if SystemInfo.btrfs_path() == None:
            print_msg("error: btrfs tool not found. " +\
                "Are btrfs-progs installed?",
                color="red")
            sys.exit(1)
        output = None
        try:
            output = subprocess.check_output([SystemInfo.btrfs_path(),
                "--version"],
                stderr=subprocess.STDOUT).decode("utf-8", "ignore")
        except subprocess.CalledProcessError as e:
            output = e.output.decode("utf-8", "ignore")
        if not output.lower().startswith("btrfs-progrs ") and \
                not output.lower().startswith("btrfs-progs ") and \
                not output.lower().startswith("btrfs "):
            print_msg("error: btrfs tool returned unexpected string. Are " +\
                "btrfs-progrs installed and working?",
                color="red")
            print_msg("Full btrfs tool output was: " + str(output))
            sys.exit(1)

    def subvolume_readiness_check(self, print_errors=True):
        """ Check if the given service is ready for snapshotting or still
            needs btrfs subvolume conversion. Print a warning if not.
        """
        fs = SystemInfo.filesystem_type_at_path(
            self.service.service_path)
        if fs != "btrfs":
            return False

        self.btrfs_tool_check()

        if os.path.exists(os.path.join(self.service.service_path,
                "livedata")):
            try:
                if not SystemInfo.is_btrfs_subvolume(os.path.join(
                        self.service.service_path, "livedata"))\
                        and len(self.service.rw_volumes) > 0:
                    if self.service.is_running():
                        if print_errors:
                            print_msg(
                                "the livedata/ dir of this service will " +\
                                "still need to be converted to " +\
                                "a subvolume to enable snapshots.\n" + \
                                "Fix it by doing this:\n" + \
                                "1. Stop the service with: docker-rudder "+\
                                    "stop " +\
                                    self.service.name + "\n" + \
                                "2. Snapshot the service once with: " +\
                                    "docker-rudder "+\
                                    "snapshot " + self.service.name + "\n",
                                service=self.service.name, color="yellow")
                    else:
                        if print_errors:
                            print_msg("the livedata/ dir of " +\
                                "this service still " +\
                                "needs conversion to btrfs subvolume.\n" +\
                                "Fix it by snapshotting it once with: " +\
                                "docker-rudder "+\
                                "snapshot " + self.service.name + "\n",
                                service=self.service.name, color="yellow")
                    return False
            except ValueError:
                print_msg("there was a problem. btrfs snapshotting won't " +\
                          "work as intended.", \
                          service=self.service.name, color="red")
                return False
        # Everything seems fine so far.
        return True

    def do(self):
        """ Make a backup of the live data of the service.
        """

        self.btrfs_tool_check()

        # Make sure no snapshot is already in progress:
        if self.check_running_snapshot_transaction():
            print_msg("error: snapshot already in progress. " +\
                "try again later", service=self.service.name,
                color="red")
            print_msg("remove .docker-rudder-snapshot.lock if you " +\
                "are sure that is incorrect", service=self.service.name)
            return False

        print_msg("considering for snapshot...",
            service=self.service.name, color="blue")

        # Check which volumes this service has:
        rw_volumes = set()
        for container in self.service.containers:
            for volume in container.rw_volumes:
                rw_volumes.add(volume)
        rw_volumes = list(rw_volumes)
        if len(rw_volumes) == 0:
            print_msg("service has no read-write volumes, nothing to do.",
                service=self.service.name, color="blue")
            return True
        
        # Check if we have livedata/:
        if not os.path.exists(os.path.join(
                self.service.service_path, "livedata")):
            print_msg("error: service has read-write volumes, " + \
                "but no livedata/ " +\
                "folder. Fix this to enable snapshots",
                service=self.service.name,
                color="red")
            return False

        # Check if we have any volumes which are actually in livedata/:
        empty_snapshot = True
        for volume in rw_volumes:
            for mount in volume.mounts:
                if mount.container == None:
                    continue
                if mount.container.service != self.service:
                    continue
                if mount.host_path == None:
                    # This volume mount is not a host mount!!
                    print_msg("warning: volume " + str(volume) + \
                        " used by " + str(mount.container) +\
                        " is NOT a host mount in livedata/" +\
                        " and won't be covered by the snapshot",
                        service=self.service.name, color="yellow")
                relpath = os.path.relpath(
                    os.path.realpath(mount.host_path),
                    os.path.realpath(os.path.join(
                    self.service.service_path, "livedata")),
                )
                if relpath.startswith(os.pardir + os.sep):
                    # This volume mount is not in livedata/!
                    print_msg("warning: volume " + str(volume) + \
                        " used by " + str(mount.container) +\
                        " is a host mount that is NOT mounted in" +\
                        " the livedata/ folder" +\
                        " and won't be covered by the snapshot: " +\
                        str(mount),
                        service=self.service.name, color="yellow")
                else:
                    empty_snapshot = False
        if empty_snapshot:
            print_msg("this snapshot would be empty because no read-write " +\
                "volumes are mounted to livedata/ - skipping.",
                service=self.service.name, color="blue")
            return True

        # Check if filesystem of livedata/ is actually btrfs:
        fs = SystemInfo.filesystem_type_at_path(
            os.path.join(self.service.service_path, "livedata"))
        if fs != "btrfs":
            print_msg("error: livedata/ has other filesystem " + str(fs) + \
                ", should be btrfs!")
            return fs

        livedata_renamed_dir = os.path.join(
            self.service.service_path, ".livedata-predeletion-renamed")
        livedata_dir = os.path.join(
            self.service.service_path, "livedata")
        snapshot_dir = os.path.join(
            self.service.service_path, ".btrfs-livedata-snapshot")
        tempvolume_dir = os.path.join(
            self.service.service_path, ".btrfs-livedata-temporary-volume")
        tempdata_dir = os.path.join(
            self.service.service_path, ".livedata-temporary-prevolume-copy")

        # Make sure the livedata/ dir is a btrfs subvolume:    
        if self.service.is_running():
            if not SystemInfo.is_btrfs_subvolume(livedata_dir):
                print_msg("error: can't do btrfs subvolume " +\
                    "conversion because "+\
                    "service is running. The first snapshot is " +\
                    "required to " +\
                    "be done when the service is stopped.",
                    service=self.service.name, color="red")
                return False

        lock_path = os.path.join(self.service.service_path,
            ".docker-rudder-snapshot.lock")

        # Check there is still no transaction running:
        if self.check_running_snapshot_transaction():
            print_msg("error: snapshot already in progress. " +\
                "try again later", service=self.service.name,
                color="red")
            print_msg("remove .docker-rudder-snapshot.lock if you are " +\
                "sure this is incorrect", service=self.service.name)
            return False

        # Add a transaction lock:
        transaction_id = str(uuid.uuid4())
        with open(lock_path, "wb") as f:
            f.write(transaction_id.encode("utf-8"))
        
        # Wait a short amount of time so other race condition writes will
        # be finished with a very high chance:
        time.sleep(0.5)

        # Verify we got the transaction lock:
        contents = None
        with open(lock_path, "rb") as f:
            contents = f.read().decode("utf-8", "ignore")
        if contents.strip() != transaction_id:
            print_msg("error: mid-air snapshot collision detected!! " + \
                "Did you call the script twice?",
                service=self.service.name, color="red")
            return False

        # Make sure the .livedata-predeletion-renamed isn't there:
        if os.path.exists(livedata_renamed_dir):
            if not os.path.exists(livedata_dir):
                print_msg("warning: .livedata-predeletion-renamed/ " + \
                    "is present and no livedata/ folder." +\
                    "Moving it back...",
                    service=self.service.name, color="yellow")
                shutil.move(livedata_renamed_dir, livedata_dir)
                assert(not os.path.exists(livedata_renamed_dir))
            else:
                print_msg("error: .livedata-predeletion-renamed/ " + \
                    "is still there, indicating a previously aborted " +\
                    "run, but livedata/ is also still around. " +\
                    "Please figure out which one you want to keep, and " +\
                    "delete one of the two.", service=self.service.name,
                    color="red")
                sys.exit(1)

        # Make sure the .livedata-temporary-prevolume-copy directory is unused:
        if os.path.exists(tempdata_dir):
            print_msg("warning: .livedata-temporary-prevolume-copy/ " + \
                "already present! " +\
                "This is probably a leftover from a previously " + \
                "aborted attempt. Will now attempt to delete it...",
                service=self.service.name, color="yellow")
            shutil.rmtree(tempdata_dir)
            assert(not os.path.exists(tempdata_dir))

        # Make sure the btrfs snapshot path is unused:
        if os.path.exists(snapshot_dir):
            if SystemInfo.btrfs_subvolume_stat_check(snapshot_dir):
                print_msg("warning: .btrfs-livedata-snapshot/ " \
                    + "already present! " \
                    + "This is probably a leftover from a previously " + \
                    "aborted attempt. Will now attempt to delete it...",
                    service=self.service.name, color="yellow")
                subprocess.check_output([SystemInfo.btrfs_path(),
                    "subvolume",
                    "delete", snapshot_dir])
                assert(not os.path.exists(snapshot_dir))
            else:
                print_msg("error: .btrfs-livedata-snapshot/ already " +\
                    "present, " \
                    + "but it is not a btrfs snapshot!! I don't know how " +\
                    "to deal with this, aborting.",
                    service=self.service.name, color="red")
                return False

        # Make sure the temporary btrfs subvolume path is unused:
        if os.path.exists(tempvolume_dir):
            print_msg("warning: .btrfs-livedata-temporary-volume/ already " +\
                "present! " \
                + "This is probably a leftover from a previously " + \
                "aborted attempt. Will now attempt to delete it...",
                service=self.service.name, color="yellow")
            output = subprocess.check_output([SystemInfo.btrfs_path(),
                "subvolume",
                "delete", tempvolume_dir])
            assert(not os.path.exists(tempvolume_dir))

        # If this isn't a btrfs subvolume, we will need to fix that first:
        if not SystemInfo.is_btrfs_subvolume(livedata_dir):
            print_msg("warning: initial subvolume conversion required. "+\
                "DON'T TOUCH livedata/ WHILE THIS HAPPENS!!",
                service=self.service.name, color="yellow")
            try:
                output = subprocess.check_output([SystemInfo.btrfs_path(),
                    "subvolume",
                    "create", tempvolume_dir])
            except Exception as e:
                os.remove(lock_path)
                raise e
            assert(btrfs_is_subvolume(tempvolume_dir))

            # Copy all contents:
            assert(not os.path.exists(tempdata_dir))
            shutil.copytree(livedata_dir, tempdata_dir, symlinks=True)
            assert(os.path.exists(tempdata_dir))
            for f in os.listdir(tempdata_dir):
                orig_path = os.path.join(tempdata_dir, f)
                new_path = os.path.join(tempvolume_dir, f)
                shutil.move(orig_path, new_path)

            # Do a superficial check if we copied all things:
            copy_failed = False
            for f in os.listdir(tempvolume_dir):
                if not os.path.exists(os.path.join(livedata_dir, f)):
                    copy_failed = True
                    break
            for f in os.listdir(livedata_dir):
                if not os.path.exists(os.path.join(tempvolume_dir, f)):
                    copy_failed = True
                    break
            if copy_failed:
                print_msg("error: files of old livedata/ directory and "+\
                    "new subvolume do not match. Did things get changed "+\
                    "during the process??",
                    service=self.service.name, color="red")
                return False

            # Remove old livedata/ dir:
            propagate_interrupt = None
            while True:
                try:
                    shutil.move(livedata_dir, livedata_renamed_dir)
                    shutil.move(tempvolume_dir, livedata_dir)
                    shutil.rmtree(livedata_renamed_dir)
                    break
                except KeyboardInterrupt as e:
                    propagate_interrupt = e
                    continue
            if propagate_interrupt != None:
                raise propagate_interrupt
            print_msg("conversion of livedata/ to btrfs subvolume complete.",
                service=self.service.name)

        snapshots_dir = os.path.join(self.service.service_path,
            "livedata-snapshots")

        # Create livedata-snapshots/ if not present:
        if not os.path.exists(snapshots_dir):
            os.mkdir(snapshots_dir)

        # Go ahead and snapshot:
        print_msg("initiating btrfs snapshot...",
            service=self.service.name)
        output = subprocess.check_output([SystemInfo.btrfs_path(),
            "subvolume", "snapshot",
            "-r", "--", livedata_dir, snapshot_dir])
        
        # Copy snapshot to directory:
        now = datetime.datetime.now()
        snapshot_base_name = str(now.year)
        if now.month < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.month)
        if now.day < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.day)
        snapshot_base_name += "-"
        if now.hour < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.hour)
        if now.minute < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.minute)
        if now.second < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.second)
        snapshot_name = snapshot_base_name + "00"
        i = 1
        while os.path.exists(os.path.join(snapshots_dir, snapshot_name)):
            snapshot_name = snapshot_base_name
            if i < 10:
                snapshot_name += "0"
            snapshot_name += str(i)
            i += 1
        snapshot_specific_dir = os.path.join(snapshots_dir,
            snapshot_name)
        print_msg("copying to " + snapshot_specific_dir,
            service=self.service.name)
        shutil.copytree(snapshot_dir, snapshot_specific_dir, symlinks=True)
        subprocess.check_output([SystemInfo.btrfs_path(), "subvolume",
            "delete", snapshot_dir])
        assert(not os.path.exists(snapshot_dir))
        print_msg("snapshot complete.", service=self.service.name,
            color="green")

        # Remove lock file:
        assert(os.path.exists(lock_path) and not os.path.isdir(lock_path))
        os.remove(lock_path)
        assert(not os.path.exists(lock_path))

        return True

class TargetsParser(object):
    """ A helper class to parse user input and turn it into container or
        services lists.
    """

    @staticmethod
    def get_containers(targets, print_error=False):
        while targets.find("  ") >= 0:
            targets.replace("  ", " ")
        result = set()
        targets = targets.strip()

        # Go through all targets in the list:
        for target in targets.split(" "):
            # Specific treatment of the "all" keyword:
            if target == "all":
                for service in Service.all():
                    for container in service.containers:
                        result.add(container)
                return list(result)

            # Examine the current service/container entry:
            service_name = target.partition("/")[0]
            service = Service.find_by_name(service_name)
            if service == None:
                if print_error:
                    print("docker-rudder: error: " + \
                        "no such service found: " + str(service_name),
                        file=sys.stderr)
                return None

            # See if a specific container is specified or jst all of them:
            container_name = target.partition("/")[2]
            if len(container_name) == 0: # all containers:
                containers = service.containers
                if len(containers) == 0:
                    print_msg("warning: specified service has no " +\
                        "containers", color="yellow", service=service.name)
            else: # a specific container. find it by name:
                containers = []
                for service_container in service.containers:
                    if service_container.name == container_name:
                        containers = [ service_container ]
                        break
                # Check if the container was found by name or not:
                if len(containers) == 0:
                    if print_error:
                        print("docker-rudder: error: " + \
                            "no such container for service \"" +\
                            str(service_name) + "\" found: " +\
                            str(container_name),
                            file=sys.stderr)
                    return None
            # Add all containers collected by this entry:
            for container in containers:
                result.add(container)
        return list(result)

    @staticmethod
    def get_services(targets, print_error=False):
        while targets.find("  ") >= 0:
            targets.replace("  ", " ")
        result = set()
        targets = targets.strip()

        # Go through all targets in the list:
        for target in targets.split(" "):
            # Specific treatment of the "all" keyword:
            if target == "all":
                for service in Service.all():
                    result.add(service)
                return list(result)
            service = Service.find_by_name(target.partition("/")[0])
            if service == None:
                if print_error:
                    print("docker-rudder: error: " + \
                        "no such service found: " + str(
                        target.partition("/")[0]),
                        file=sys.stderr)
                return None
            result.add(service)
        return list(result)

class ContainerLinkDependencyNode(object):
    """ A node in the dependency graph of container dependencies.
        Has incoming and outgoing link edges, and has an associated container.
    """
    def __init__(self, container, incoming_links, outgoing_links, graph):
        self.incoming_links = incoming_links
        self.outgoing_links = outgoing_links
        self.container = container
        self.graph = graph

    def __repr__(self):
        return "<ContainerLinkDependencyNode " + str(self.container) +\
            " (" +\
            ", ".join([str(self.container) + " -> " + str(link.container)\
                for link in self.outgoing_links]) + ")>"

    def __hash__(self):
        return hash(str(self.container.name))

    def __eq__(self, other):
        if other == None:
            return False
        if not hasattr(other, "container") or \
                not hasattr(other, "incoming_links") or \
                not hasattr(other, "outgoing_links") or \
                not hasattr(other, "graph"):
            return False
        if other.container == self.container:
            if other.graph != self.graph:
                return False
            return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def fulfills_dependencies(self, containers):
        nodes = [self.graph[container] for container in running_containers]
        for outgoing_link in self.outgoing_links:
            if not outgoing_link in nodes:
                return False
        return True

    def is_depending_on(self, containers):
        nodes = [self.graph[container] for container in running_containers]
        for outgoing_link in self.outgoing_links:
            if outgoing_link in nodes:
                return True
        return False

    def is_dependency_of(self, containers):
        nodes = [self.graph[container] for container in running_containers]
        for incoming_link in self.outgoing_links:
            if incoming_link in nodes:
                return True
        return False

class DoAlongDependencyEdges(object):
    """ This helper class operates on the graph provided by a
        ContainerLinkDependencyResolution object.

        It constructs a subgraph according to the following rules:

        - the graph will be made of the specified initial_containers list,
          and then gradually grown along all the outgoing edges or,
          alternatively with along_incoming=True, along all the incoming
          edges to the maximum extent

        You can then yield containers in this subgraph in the partial order
        laid out by the connections it was grown along, starting with the root
        nodes, then their children, etc.

        This allows you to apply a function to all yielded containers in
        order, e.g. launching them.

        See the iterators DoAlongDependencyEdges.unprocessed_containers() or
        DoAlongDependencyEdges.failed_containers() for details on how the
        containers of this subgraph can be yielded.
    """
    def __init__(self, dependency_resolution_object,
            initial_containers,
            along_incoming=False, expand_initial_set=True):
        self.access_lock = threading.Lock()

        self.expand_initial_set = expand_initial_set
        self.along_incoming = along_incoming
        self.graph = dependency_resolution_object.graph
        self.containers = initial_containers
        self.start_set = set()
        for container in initial_containers:
            self.start_set.add(self.graph[container])
        self.marked_done = set()
        self.marked_failed = set()
        self.yielded_nodes = set()
        self.yielded_failed_nodes = set()

        # Grow to the relevant subgraph which can be possibly returned:
        expand_queue = queue.Queue()
        self.subgraph = {}
        for container in initial_containers: # queue up initial containers
            self.subgraph[container] = self.graph[container]
            expand_queue.put(self.subgraph[container])
        if expand_initial_set:
            while not expand_queue.empty(): # crawl through graph and grow it
                expand_node = expand_queue.get()
                expanded_nodes = []
                if along_incoming:
                    expanded_nodes = expand_node.incoming_links
                else:
                    expanded_nodes = expand_node.outgoing_links
                for expanded_node in expanded_nodes:
                    if not expanded_node.container in self.subgraph:
                        self.subgraph[expanded_node.container] = expanded_node
                        expand_queue.put(expanded_node)

    def __iter__(self):
        return self

    def get_next_candidates(self):
        """ Internal function. Don't call before obtaining access lock!!

            Returns a set of candidates that can currently be returned as
            unprocessed nodes. Ignores which ones have already been returned
            by the iterator, but it ensures only those reachable through
            success nodes are actually returned.
        """
        # Get root nodes in relevant sub graph:
        start_nodes = set()
        for node in self.subgraph.values():
            if self.along_incoming:
                if len([node for node in node.outgoing_links\
                        if node in self.subgraph.values()]) == 0:
                    start_nodes.add(node)
            else:
                if len([node for node in node.incoming_links\
                        if node in self.subgraph.values()]) == 0:
                    start_nodes.add(node)
        if len(start_nodes) == 0:
            raise RuntimeError("invalid graph, no root nodes present")

        # Prepare some stuff for gradually crawling the graph:
        examine_queue = queue.Queue()
        def get_successors(node):
            if self.along_incoming:
                return node.incoming_links
            else:
                return node.outgoing_links
        def get_predecessors(node):
            if self.along_incoming:
                return node.outgoing_links
            else:
                return node.incoming_links
        for node in start_nodes:
            examine_queue.put(node)

        # Start advancing through graph to next unprocessed nodes:
        seen = set()
        candidates = []
        while not examine_queue.empty():
            next_node = examine_queue.get()
            if not next_node in self.marked_done and \
                    not next_node in self.marked_failed:
                # This node is unprocessed. Check if all predecessors are
                # actually marked as done so we can return it:
                predecessors = get_predecessors(next_node)
                reachable = True
                for predecessor in predecessors:
                    if not predecessor.container in self.subgraph:
                        continue
                    if not predecessor in self.marked_done:
                        # Nope, at least one predecessor isn't done yet.
                        reachable = False
                        break
                if reachable:
                    candidates.append(next_node)
            else:
                # This node is either failed or done/succeeded. See which:
                if not next_node in self.marked_failed:
                    # It's a succeeded node, therefore look at the successors:
                    for successor in get_successors(next_node):
                        if not successor in seen:
                            seen.add(successor)
                            examine_queue.put(successor)

        # Return assembled candidates:
        return candidates

    def mark_success(self, container):
        """ Mark a container as succeeded. This opens up all the direct
            dependencies of this container to be returned from
            unprocessed_containers() as next unprocessed containers to be
            taken care of.
        """
        self.marked_done.add(self.graph[container])

    def mark_failure(self, container):
        """ Mark a container as failed. No container depending on this one
            will be considered for yielding for unprocessed_containers()
            anymore.
        """
        self.marked_failed.add(self.graph[container])

    def _next_unprocessed_node(self):
        """ Note: internal function, use unprocessed_nodes() iterator instead.

            Get the next node that is still unprocessed.
            
            Calling this function continuously will gradually yield all
            unprocessed nodes that are obtainable (see
            unprocessed_containers() for details), and None if there is
            currently no unprocessed node that can be returned.
        """

        self.access_lock.acquire()
        # Get all the candidates:
        candidates = self.get_next_candidates()

        # Check which one we haven't already returned:
        actual_candidates = []
        for candidate in candidates:
            if not candidate in self.yielded_nodes:
                actual_candidates.append(candidate)

        # If no real candidate remains, return None:
        if len(actual_candidates) == 0:
            self.access_lock.release()
            return None

        # Pick a random choice of the remaining candidates:
        yielded_node = random.choice(actual_candidates)
        self.yielded_nodes.add(yielded_node)
        result = yielded_node
        self.access_lock.release()
        return result

    def _all_nodes_processed(self):
        """ Check if all nodes are either marked as success or failure.
            Returns True if all are marked, False if some are still unmarked.
        """
        result = True
        self.access_lock.acquire()
        for node in self.subgraph.values():
            if not node in self.marked_failed and \
                    not node in self.marked_done:
                result = False
                break
        self.access_lock.release()
        return result

    def unprocessed_containers(self):
        """ This function returns an iterator which yields the items according
            to the order specified in the DoAlongDependencyEdges class
            description.

            However, it introduces two additional criteria (while maintaing
            the order implicated by the subgraph):

            - a container can only be yielded as soon as all the direct
              predecessors are marked as "succeeded" with mark_success().

            - all containers where any direct or indirect predecessors were
              marked as "failed" with mark_failure() will be ignored and not
              yielded at all.

            If containers cannot be yielded because the first criterion
            isn't fulfilled and they weren't discarded through the second
            criterion yet and if all containers that could have been yielded
            so far have been yielded, the iterator will hang and wait for
            this situation to be resolved through further mark_success()/
            mark_failure() calls.

            As a result, this iterator will eventually hang unless you make
            sure to eventually mark all returned items with "mark_success" or
            "mark_failure". If you fail to do that, the iterator will remain
            stuck.
        """

        class UnprocessedContainersIterator(object):
            def __init__(self, doalongobj):
                self.doalongobj = doalongobj

            def __iter__(self):
                return self

            def __next__(self):
                skipped = True
                while skipped:
                    skipped = False

                    # Get next node or wait until we can get it:
                    node = self.doalongobj._next_unprocessed_node()
                    while node == None and \
                            not self.doalongobj._all_nodes_processed():
                        time.sleep(0.2)
                        node = self.doalongobj._next_unprocessed_node()
                    if node == None:
                        raise StopIteration

                    # Make sure we only expand beyond initial set if we were
                    # supposed to do that:
                    if not self.doalongobj.expand_initial_set and \
                            not node.container in self.doalongobj.containers:
                        # Skip this, it would expand beyond initial set.
                        skipped = True
                        continue
                    break
                return node.container
        return UnprocessedContainersIterator(self)

    def failed_containers(self):
        """ This function returns an iterator which yields containers
            in no particular order which fulfill the following criterion:

            - containers which have been marked as "failed" or where any
              direct or indirect predecessors were marked as "failed" with
              mark_failure() will be yielded

            If containers have neither met by this criterion, nor been marked
            as succeeded and if all containers that could have been yielded so
            far have been yielded already, this iterator will hang and wait
            for this situation to be resolved through further mark_success()/
            mark_failure() calls.

            As a result, this iterator will eventually hang unless you make
            sure to eventually mark all returned items with "mark_success" or
            "mark_failure". If you fail to do that, the iterator will remain
            stuck.
        """
        raise NotImplementedError("not implemented at this point")

class ContainerLinkDependencyResolution(object):
    """ This class builds a graph based on the container.dependencies (which
        are themselves constructed from the docker links of each container).

        This graph can then be used with DoAlongDependencyEdges to do things
        according to the dependency order.
    """
    def __init__(self, containers):
        self.containers = containers
        self.graph = dict()

        # Add all containers as nodes without arcs:
        for container in self.containers:
            if not container in self.graph:
                self.graph[container] = ContainerLinkDependencyNode(
                    container, [], [], self.graph)

        # Go through containers again and add arcs:
        node_added = True
        while node_added:
            node_added = False
            for container in self.containers:
                source_node = self.graph[container]
                for dependency in container.dependencies:
                    dep_container = None
                    try:
                        dep_container = dependency.container
                    except ValueError:
                        print("docker-rudder: error: dependency for " +\
                            str(container) + " is not in known services, " +\
                            "can't resolve dependency chain: " +\
                            str(dependency), file=sys.stderr)
                        sys.exit(1)

                    # Make sure the target for our new arc is in the graph:
                    if not dep_container in self.graph:
                        self.graph[dep_container] = \
                            ContainerLinkDependencyNode(
                            dep_container, [], [], self.graph)
                        node_added = True

                    # Obtain target node ref:
                    target_node = self.graph[dep_container]

                    # Add arcs in both directions, if any is missing:
                    if not target_node in source_node.incoming_links:
                        source_node.outgoing_links.append(target_node)
                    if not source_node in target_node.outgoing_links:
                        target_node.incoming_links.append(source_node)
                if node_added:
                    break
        # Done! Graph complete

    def print_graph_debug(self):
        print("Graph:")
        seen = set()
        for container in self.graph:
            node = self.graph[container]
            for outgoing in node.outgoing_links:
                assert(node in outgoing.incoming_links)
                print(str(node.container) + " -> " + str(outgoing.container))
                seen.add(node.container)
                seen.add(outgoing.container)
        for container in self.graph:
            if not container in seen:    
                print(str(container))
        print("End of Graph.")

class ContainerStatusInfo(object):
    """ A class to compute a dictionary of all sorts of extra information for
        a container. Used by the "status" command.
    """
    def __init__(self, container):
        self.container = container

    def get(self, ordered=False):
        dict_class = dict
        if ordered:
            dict_class = OrderedDict
        info = dict_class()
        info["Canonical docker container name"] = \
            self.container.default_container_name 
        info["Owning service"] = OrderedDict()
        info["Owning service"]["Name"] = self.container.service.name
        info["Owning service"]["Location"] = \
            self.container.service.service_path
        info["Running"] = self.container.running
        info["Dependencies"] = OrderedDict()
        resolution = ContainerLinkDependencyResolution(
            Service.all_containers())
        info["Dependencies"]["Pre-Start"] = []
        pre_start = \
            DoAlongDependencyEdges(resolution, [self.container],
            along_incoming=False)
        for container in pre_start.unprocessed_containers():
            if container == self.container:
                pre_start.mark_success(container)
                continue
            info["Dependencies"]["Pre-Start"].append(
                container)
            pre_start.mark_success(container)
        info["Dependencies"]["Pre-Stop-Post-Restart"] = []
        pre_stop = \
            DoAlongDependencyEdges(resolution, [self.container],
            along_incoming=True)
        for container in pre_stop.unprocessed_containers():
            if container == self.container:
                pre_stop.mark_success(container)
                continue
            info["Dependencies"]["Pre-Stop-Post-Restart"].append(
                container)
            pre_stop.mark_success(container)
        info["Dependencies"]["Pre-Stop-Post-Restart"] = \
            list(reversed(info["Dependencies"]["Pre-Stop-Post-Restart"]))

        # Collect volumes:
        potentially_lost = self.container.potentially_lost_volumes
        info["Volumes"] = list()
        for volume in self.container.volumes:
            vol_info = dict_class()
            if volume.id != None:
                vol_info["Id"] = str(volume.id)
            if volume.name != None:
                vol_info["Name"] = volume.name
                vol_info["Unnamed"] = False
            else:
                vol_info["Unnamed"] = True
            vol_info["Specified in YAML"] = volume.specified_in_yml
            if volume in potentially_lost:
                vol_info["Potentially lost on container recreation"] = True
            mounts = volume.mounts
            if len(mounts) > 0:
                vol_info["Mounts"] = list()
                for mount in mounts:
                    mount_info = dict_class() 
                    if mount.host_path != None:
                        mount_info["Host mount path"] = mount.host_path
                    if mount.mount_container_filesystem_path != None:
                        mount_info["Container mount path"] =\
                            mount.mount_container_filesystem_path
                    vol_info["Mounts"].append(mount_info)
            info["Volumes"].append(vol_info)
        return info

def unknown_action(hint=None):
    """ Print an error that the given action to docker-rudder is invalid,
        with a possible hint to suggest another action.
    """
    print("docker-rudder: error: unknown action: " + \
        args.action, file=sys.stderr)
    if hint != None:
        print("Did you mean: " + str(hint) + "?")
    sys.exit(1)

# Ensure the docker main service is running:
error_output = None
try:
    subprocess.check_output([SystemInfo.docker_path(), "ps"],
        stderr=subprocess.STDOUT)
except subprocess.CalledProcessError as e:
    error_output = e.output.decode("utf-8", "ignore")
if error_output != None:
    # Old-style error message:
    if error_output.find("dial unix") >= 0 and \
            error_output.find("no such file or directory") >= 0:
        print("docker-rudder: error: " + \
            "docker daemon appears to be not running." +\
            " Please start it and ensure it is reachable.")
        sys.exit(1)
    # Newer error message:
    elif error_output.find("Cannot connect to the Docker daemon") >= 0:
        print("docker-rudder: error: " + \
            "docker daemon appears to be not running." +\
            " Please start it and ensure it is reachable.")
        sys.exit(1)
    else:
        print("docker-rudder: error: " + \
            "there appears to be some unknown problem with " + \
            "docker! (test run of \"docker ps\" returned error code)")
        sys.exit(1)

# Check if services are btrfs ready, and give warning if not:
if args.action != "snapshot" and args.action != "info":
    for service in Service.all():
        snapshots = Snapshots(service)
        snapshots.subvolume_readiness_check()

# --- Main handling of actions here:

if args.action == "list":
    all_services = Service.all()
    print("\033[1mService list (" + str(len(all_services)) +\
        " service(s)):\033[0m")
    for service in all_services:
        state = ""
        running_names = service._get_running_service_container_names()
        all_names = [container.name for container in service.containers]
        if len(running_names) == len(all_names):
            state = "\033[1;32mrunning\033[0m"
        elif len(running_names) > 0:
            state = "\033[1;33mpartial (running: " +\
                ", ".join(running_names) + ", not running: " +\
                ", ".join([name for name in all_names\
                    if name not in running_names]) + ")\033[0m"
        else:
            state = "\033[1;31mstopped\033[0m"
        print("\033[0m  - \033[1m" + service.name + "\033[0m (" + \
            service.service_path + "): " + state)
elif args.action == "help":
    parser.print_help()
    sys.exit(1)
elif args.action == "logs":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service for which docker logs shold be printed, "+\
            "or \"all\"", file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    for container in containers:
        print_msg("printing log of container " + str(container),
            service=container.service.name, color='blue')
        try:
            retcode = subprocess.call([SystemInfo.docker_path(), "logs",
                    container.current_running_instance_name],
                    stderr=subprocess.STDOUT)
            if retcode != 0:
                raise subprocess.CalledProcessError(
                    retcode, " ".join([SystemInfo.docker_path(), "logs"]))
        except subprocess.CalledProcessError:
            print_msg("failed printing logs. " +\
                "Maybe container has no logs yet?",
                service=container.service.name,
                container=container.name, color='yellow')
            pass
elif args.action == "rebuild":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service for which an interactive shell should be started",
            file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    for container in containers:
        if container.running:
            print_msg("cannot safely recreate container, " +\
                "one or more of the specified " +\
                "containers is/are currently running: " +\
                str(", ".join([
                    str(container) for container in containers])),
                color="red")
            print("docker-rudder: error: aborted rebuilding")
            sys.exit(1)
    for container in containers:
        print_msg("rebuilding container with no cache",
            service=container.service.name, container=container.name,
            color="blue")
        subprocess.call([SystemInfo.docker_compose_path(), "build",
            "--no-cache", "--pull", container.name],
            cwd=container.service.service_path)
    print_msg("Rebuilding complete.", color="green")
    sys.exit(0)
elif args.action == "shell":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service for which an interactive shell should be started",
            file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    if len(containers) != 1:
        print("docker-rudder: error: this command can only be used " + \
            "on a single container, but the given argument \"" +\
            " ".join(args.argument) +\
            "\" matches multiple " +\
            "containers: " +\
            ", ".join([str(container) for container in containers]),
            file=sys.stderr)
        sys.exit(1)
    if containers[0].running:
        cname = containers[0].current_running_instance_name
        print_msg("attaching to running container " + str(cname),
            service=containers[0].service.name,
            color="blue")
        subprocess.call([SystemInfo.docker_path(), "exec", "-t", "-i",
                str(cname), "/bin/bash"],
            stderr=subprocess.STDOUT)
    else:
        print_msg("launching container " + str(containers[0]) + \
            " with shell",
            service=containers[0].service.name,
            color="blue")
        image_name = containers[0].image_name
        subprocess.call([SystemInfo.docker_compose_path(), "build",
            containers[0].name], cwd=containers[0].service.service_path)
        subprocess.call([SystemInfo.docker_compose_path(), "run",
            image_name, "/bin/bash"], cwd=containers[0].service.service_path)
elif args.action == "start" or args.action == "restart":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service to be started, or \"all\"", file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    if len(containers) == 0:
        sys.exit(0)

    resolution = ContainerLinkDependencyResolution(
        Service.all_containers())

    tracker = FailedLaunchTracker()
    scheduled_action = "restart"
    if args.action == "start":
        scheduled_action = "start"

    # Stop all indirect dependencies and the containers themselves:
    restart_dependencies = set()
    if scheduled_action == "restart":
        unreversed_stop_before_restart = \
            DoAlongDependencyEdges(resolution, containers,
            along_incoming=True)
        stop_before_restart = []
        for container in \
                unreversed_stop_before_restart.unprocessed_containers():
            stop_before_restart.append(container)
            unreversed_stop_before_restart.mark_success(container)
        stop_before_restart = list(reversed(stop_before_restart))
        for container in stop_before_restart:
            if not container in containers:
                print_msg("stopping... (this container is depending on 1+ " +\
                    "of the container(s) scheduled for restart)",
                    service=container.service.name, container=container.name,
                    color="blue")
                restart_dependencies.add(container)
            else:
                print_msg("stopping... (this container was scheduled for " +\
                    scheduled_action + ")",
                    service=container.service.name, container=container.name,
                    color="blue")
            LaunchThreaded.stop(container)

    # Helper function to launch a service:
    threads = []
    def start_container(container, is_last=False,
            do_on_failure=None, do_on_success=None):
        if container.running:
            if container in containers:
                print_msg("already running. (this container was " +\
                        "scheduled for " +\
                        scheduled_action + ")",
                    service=container.service.name,\
                    container=container.name,
                    color="green")
            elif container in restart_dependencies:
                print_msg("already running. (this container was " +\
                        "an external dependency on the " + scheduled_action +\
                        "ed containers) - weird, didn't we stop it before??",
                    service=container.service.name,\
                    container=container.name,
                    color="yellow")
            else:
                print_msg("already running. (this container is " +\
                        "a dependency of a container scheduled for " +\
                        scheduled_action + ")",
                    service=container.service.name,\
                    container=container.name,
                    color="green")
            if do_on_success != None:
                do_on_success()
            return
        if not is_last:
            t = LaunchThreaded.attempt_launch(container,
                failed_launch_tracker=tracker,
                force_container_recreation=(args.force is True),
                do_on_failure=do_on_failure,
                do_on_success=do_on_success)
        else:
            t = LaunchThreaded.attempt_launch(container,
                failed_launch_tracker=tracker,
                to_background_timeout=None,
                force_container_recreation=(args.force is True),
                do_on_failure=do_on_failure,
                do_on_success=do_on_success)
        if t != None:
            threads.append(t)

    # Start the dependencies of our container launch set:
    start_dependency_containers = \
        DoAlongDependencyEdges(resolution, containers,
        along_incoming=False, expand_initial_set=True)
    for container in start_dependency_containers.unprocessed_containers():
        current_container = container
        if current_container in containers: # not a dependency
            start_dependency_containers.mark_success(current_container)
            continue
        # Launch it:
        start_container(container, is_last=False,
            do_on_failure=lambda: \
            start_dependency_containers.mark_failure(current_container),
            do_on_success=lambda: \
            start_dependency_containers.mark_success(current_container))

    # Start everything again:
    start_containers = \
        DoAlongDependencyEdges(resolution, set(containers).union(\
            restart_dependencies),
        along_incoming=False, expand_initial_set=False)
    for container in start_containers.unprocessed_containers():
        start_container(container, is_last=False,
            do_on_failure=lambda: \
            start_containers.mark_failure(container),
            do_on_success=lambda: \
            start_containers.mark_success(container))
    LaunchThreaded.wait_for_launches(threads)
    if len(tracker) > 0:
        print("docker-rudder: error: some launches failed.",
            file=sys.stderr)
        sys.exit(1)
    else:
        sys.exit(0)
elif args.action == "stop":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service to be stopped, or \"all\"", file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    for container in containers:
        assert(container != None)
        if container.running:
            print_msg("stopping...", service=container.service.name,
                container=container.name,
                color="blue")
            LaunchThreaded.stop(container)
            print_msg("stopped.", service=container.service.name,
                container=container.name, color="green")
        else:
            print_msg("not currently running.",
                service=container.service.name,
                container=container.name,
                color="blue")
    sys.exit(0)
elif args.action == "snapshot":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service to be snapshotted, or \"all\"", file=sys.stderr)
        sys.exit(1)
    services = TargetsParser.get_services(" ".join(args.argument),
        print_error=True)
    return_error = False
    for service in services:
        fs = SystemInfo.filesystem_type_at_path(service.service_path)
        if fs != "btrfs":
            print_msg("cannot snapshot service. filesystem " +\
                "is " + fs + ", would need to be btrfs",
                service=service.name, color="red")
            return_error = True
            continue
        snapshots = Snapshots(service)
        if not snapshots.do():
            return_error = True
    if return_error:
        print("docker-rudder: error: some snapshots failed.",
            file=sys.stderr)
        sys.exit(1)
    else:
        sys.exit(0)
elif args.action == "status":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service or container to print info for, " +\
            "or \"all\"", file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    def print_info(key, value=None, color=None, bold=False,
            no_line_break=False):
        if hasattr(value, "append"):
            if len(value) == 0:
                value = "<empty list>"
            else:
                value = ", ".join([str(item) for item in value])
        if bold:
            t = "\033[1m" + key
        else:
            t = key
        if value == None:
            if bold:
                t += "\033[0m"
            print(t)
            return
        if (len(value) > 25 or value.find("\n") >= 0) and \
                not no_line_break:
            value = "\n  " + "\n  ".join(
                [line for line in value.split("\n")])
        if bold:
            t += "\033[0m: "
        else:
            t += ": "
        if color == "red":
            t += "\033[31m\033[1m" + value
        elif color == "yellow":
            t += "\033[33m\033[1m" + value
        elif color == "green":
            t += "\033[32m\033[1m" + value
        elif color == "blue":
            t += "\033[34m\033[1m" + value
        else:
            t += value
        if color != None:
            t += "\033[0m"
        print(t)
    first = True
    for container in containers:
        if first:
            first = False
        else:
            print("")
        print_info("Status for", str(container), bold=True,
            no_line_break=True)
        print("-" * (len("Status for: ") + len(str(container))))
        info = ContainerStatusInfo(container).get()
        print_info("Canonical docker container name",
            container.default_container_name)
        if container.running:
            print_info("Running", "yes", color="green")
        else:
            print_info("Running", "no", color="red")
        print_info("Configuration file",
            os.path.join(container.service.service_path,
            "docker-compose.yml"))
        snapshots = Snapshots(container)
        if not snapshots.subvolume_readiness_check():
            print_info("Live data snapshot support",
                "no, errors found.\nTry \"drudder snapshot " +\
                str(container.service.name) + "\" for details",
                color="red")
        else:
            print_info("Live data snapshot support", "ok", color="green")
        print("This container depends on "+\
            "(ensured to run if this one starts):")
        l = info["Dependencies"]["Pre-Start"]
        if len(l) == 0:
            print("  <no dependencies>")
        else:
            for item in l:
                print(" - " + str(item))
        print("Other containers depending on this " +\
            "(restarted along with this one):")
        l = info["Dependencies"]["Pre-Stop-Post-Restart"]
        if len(l) == 0:
            print("  <no dependencies>")
        else:
            for item in l:
                print(" - " + str(item))
        potentially_lost = container.potentially_lost_volumes
        if len(potentially_lost) > 0:
            print("\033[33m\033[1mWarning: some volumes might be " +\
                "lost on container recreation:\n         "+\
                ", ".join([str(vol) for vol in potentially_lost]) + "\n" +\
                "         Make sure to specify them in your " +\
                    "docker-compose.yml!\033[0m")
    sys.exit(0)
elif args.action == "dump-container-info":
    if len(args.argument) == 0:
        print("docker-rudder: error: please specify the name " + \
            "of the service or container to print info for, " +\
            "or \"all\"", file=sys.stderr)
        sys.exit(1)
    containers = TargetsParser.get_containers(" ".join(args.argument),
        print_error=True)
    if containers == None:
        sys.exit(1)
    for container in containers:
        info = ContainerStatusInfo(container).get(ordered=True)
        print("# --- Container: " + str(container))
        print("\"" + str(container) + "\":")
        def serialize(item, indent=2):
            t = ""
            if isinstance(item, dict) or isinstance(item, OrderedDict):
                for k in item:
                    t = t + " " * indent + k + ":"
                    t2 = serialize(item[k], indent=2)
                    if t2.find("\n") >= 0:
                        if t2.endswith("\n"):
                            t2 = t2[:-1]
                        t = t + "\n" + \
                            "\n".join([" " * (indent) + line for line in \
                            t2.split("\n")]) + "\n"
                    else:
                        t = t + " " + t2[2:] + "\n"
            elif isinstance(item, list):
                for value in item:
                    t2 = serialize(value, indent=(indent))
                    t2 = t2.lstrip()
                    if t2.endswith("\n"):
                        t2 = t2[:-1]
                    t = t + "- " +\
                        t2 + "\n"
            else:
                return " " * indent + str(item)
            return t
        print(serialize(info))
    sys.exit(0)
elif args.action == "clean":
    Service.global_clean_up(args.force != True)
else:
    unknown_action()

