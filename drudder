#!/usr/bin/env python3

"""
Copyright (c) 2015-2016  Jonas Thiem et al.

This software is provided 'as-is', without any express or implied
warranty. In no event will the authors be held liable for any damages
arising from the use of this software.

Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:

1. The origin of this software must not be misrepresented; you must not
   claim that you wrote the original software. If you use this software
   in a product, an acknowledgement in the product documentation would be
   appreciated but is not required.
2. Altered source versions must be plainly marked as such, and must not be
   misrepresented as being the original software.
3. This notice may not be removed or altered from any source distribution.
"""

"""
    # What is this


    ## Motivation

    **drudder** is a tool which operates on top of docker and docker-compose to
    introduce a simpler and more powerful API.
    It doesn't add any notable new features (other than btrfs snapshot
    handling): instead it tries to offer a well-thought-out interface to
    simplify all the required daily tasks needed for managing your docker
    containers.

    **drudder** is not a fully fledged orchestration tool and is mainly useful
    if you have a single smaller production server where all software runs on.
    If you want to control a huge docker cloud on multiple physical machines,
    go elsewhere.


    ## Features

    - easily start/stop and manage multiple docker-compose controlled
      services in one go

    - creation of atomic snapshots of all your read-write volumes/live data
      without shutting down or pausing your containers (for backup purposes)

    - clean up all stopped containers and dangling volumes with one command

    - one self-contained file



    # Usage

    Check out the [full documentation here](docs/index.md).

    ## Basics

    These are the basic required commands for a simple single machine setup
    where all your services are located on the local filesystem inside
    the folder ```/srv/```.
    (see the section ```HOW TO add your service``` for details on how those
    services need to be set up)


    ```
      drudder list                - list all known services
      drudder start <service>     - starts the specified service
      drudder stop <service>      - stops the specified service
      drudder restart <service>   - restarts the given service.
                                    **WARNING**: the containers will *always*
                                    get rebuilt and recreated by this command
                                    (unless this would result in dangling
                                    volumes).
                                    All data in the containers outside of
                                    volumes will be reset!
      drudder rebuild <service>   - force rebuild of the service from the
                                    newest Dockerfile and/or image. Please note
                                    this is only required if you want to force
                                    a rebuild from the ground up, the (re)start
                                    actions will already update the container 
                                    if any of the relevant Dockerfiles were
                                    changed.
      drudder info <service>[/subservice] - show extended info about the
                                            service
      drudder logs <service>      - print logs of all docker containers of the
                                    service
      drudder shell <service>[/<subservice>]  - run a shell in the specified
                                                subservice's container
      drudder snapshot <service>  - makes a snapshot of the live data if
                                    enabled. (optional) This feature requires
                                    btrfs
    ```
    **Hint**: You can always use "all" as service target if you want to apply
    an action to all services on your machine.


    ## Maintenance

    These are rare special commands you might need for the occasional special
    maintenance.

    ```
      drudder install-tools       - install and update all required tools for
                                    running drudder on this computer
      drudder clean               - deletes all containers that aren't running
                                    and all dangling volumes
    ```

    # Installation

    First, make sure a recent Python 3 (3.3 or newer) is installed on your
    system.
    Then, copy the drudder script to /usr/bin/ and set execution bit
    (chmod +x).
    Then run: ```sudo drudder install-tools```


    # HOW TO add your service

    drudder expects services to be grouped with the help of docker-compose /
    docker-compose.yml (or alternatively a [drudder.yml for advanced
    features](docs/drudder.yml.md)). The script will scan the following
    locations for services subfolders with a docker-compose.yml in them:

    - the current working directory when running the script
    - /usr/share/docker-services/
    - /srv/

    You can add more using [the global drudder config](docs/config.md).

    Each service folder inside one of those locations should contain:

    - docker-compose.yml to launch it. (folders without this file are skipped)
    - livedata/ subfolder where all read-write volumes are mounted to
                                (recommended, see snapshots as described below)

    To list all currently recognized services, type: `drudder list`

    Congratulations, you can now manage launch your service(s) with
    drudder!


    # HOW TO backup

    You should backup all your services. drudder provides snapshot
    functionality to help with this. While you could simply copy your service
    folder with all the mounted volumes in it, this can lead to corrupt copies
    when doing this while some services are operating (SQL databases etc.).

    To use drudder snapshots of your writable volumes during service
    operation, do this:

    1. Enable snapshots as described below

    2. Always run "drudder snapshot all" before you make your backup to get
       consistent snapshots of your writable volumes in a subfolder named
       livedata-snapshots/ in each respective service folder.



    # HOW TO enable snapshots (optional)

    This feature allows you to easily copy all the live data of your read-write
    mounted volumes your containers as atomic snapshots even while your
    services are running and continue to write data.

    The snapshots will be atomic, therefore they should be suitable even for
    database realtime operations while the database is running and writing to
    the volume(s).


    ## Enable snapshots for a specific service

    How to enable snapshots for a service:

    1. Make sure your services folder is on a btrfs file system (not ext4).

    2. Each of your snapshot enabled services needs to have a subfolder
       livedata/ where all read-write volumes of it are located.


    ## Test/do it

    **Before you go into production, make sure to test snapshots for your
    service(s) at least once!**

    Calling:
       ``` drudder snapshot <service>|"all" ```
  
    will now use btrfs functionality to add a time-stamped folder with an
    atomic snapshot of livedata/ of the specified service(s) into a new
    livedata-snapshots/ subfolder - while your service can continue using the
    volume thanks to btrfs' copy-on-write snapshot functionality.


    ## Restore a snapshot

    You can easily restore such a snapshot by shutting down your service
    temporarily, copying back a snapshot into livedata/ and turning your
    service back on.

"""

""" Copyright (C) Jonas Thiem et al., 2015-2016
"""

TOOL_VERSION="0.2.0"
BUG_URL="https://github.com/JonasT/drudder"

import argparse
from argparse import RawTextHelpFormatter, HelpFormatter
import base64
from collections import OrderedDict
import copy
import datetime
import json
import multiprocessing
import os
import platform
import queue
import subprocess
import random
import re
import shlex
import shutil
import sys
import textwrap
import threading
import time
import traceback
import uuid

def clear_text(text, allow_line_breaks=True):
    cleared_text = ""
    for c in text:
        if ord(c) < 32 and ((c != "\n"
                and c != "\n") or not allow_line_breaks):
            cleared_text += " "
        else:
            cleared_text += c
    return cleared_text

def repair_terminal():
    """ Since docker-compose and some other tools really don't want to leave
        the terminal alone, this will reset it to a usable state without
        the need to actually do a "reset" command that wipes the scrollback.
    """
    subprocess.call(["stty", "onlcr"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL)
    subprocess.call(["stty", "echo"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL)
    subprocess.call(["stty", "opost"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL)

class SubprocessHelper(object):
    @staticmethod
    def check_output_with_stdout_copy(cmd, shell=False, stdout=None,
            stderr=None, cwd=None, env=None, pty=False, write_function=None):
        """ Like subprocess.check_output, but also copies the stdout/stderr to
            sys.stdout/sys.stderr in realtime while the process runs while
            still returning the full output as a return value after
            termination and while still raising subprocess.CalledProcessError
            when encountering a process exit code other than 0.

            Optional parameters for extended behavior:

            Use pty=True for optional pseudo-TTY wrapping on Unix to make the
            launched command think it runs in an interactive terminal (this
            will also remove any sort of stdout/stdin buffering or delay). All
            other behavior remains the same.

            Use write_function to specify a custom write function in addition
            to sys.stdout/sys.stderr copying which can process the output in
            realtime in some way. If you want to abort the process from inside
            the write function, just raise an exception. If that happens,
            check_output_with_stdout_copy will raise a RuntimeError instead of
            the regular subprocess.CalledProcessError to indicate an abort
            caused by a write_function error.
        """
        return SubprocessHelper._check_output_extended(cmd, shell=shell,
            stdout=stdout, stderr=stderr, cwd=cwd, env=env, pty=pty,
            write_function=write_function, copy_to_regular_stdout_stderr=True)

    @staticmethod
    def _check_output_extended(cmd, shell=False, stdout=None,
            stderr=None, cwd=None, env=None, pty=False, write_function=None,
            copy_to_regular_stdout_stderr=True,
            process_handle_callback=None):
        # Get actual stdout/stderr as to be used internally:
        actual_stdout = stdout
        if stdout == None or stdout == sys.stderr:
            actual_stdout = subprocess.PIPE
        actual_stderr = stderr
        if stderr == None or stderr == subprocess.STDOUT or \
                stderr == sys.stderr:
            actual_stderr = subprocess.PIPE

        # Handle various other parameters:
        if cwd == None:
            cwd = os.getcwd()
        if env == None:
            env = os.environ.copy()

        # Launch command:
        if not pty:
            process = subprocess.Popen(cmd, shell=shell,
                stdout=actual_stdout, stderr=actual_stderr,
                cwd=cwd, env=env)
        else:
            process = subprocess.Popen([sys.executable,
                "-c", "import pty\nimport sys\n" +\
                "exit_code = pty.spawn(sys.argv[1:])\n" +\
                "sys.exit((int(exit_code) & (255 << 8)) >> 8)"] + cmd,
                shell=shell,
                stdout=actual_stdout, stderr=actual_stderr,
                cwd=cwd, env=env)
        if process_handle_callback != None:
            process_handle_callback(process)

        # Output reader thread definition:
        class OutputReader(threading.Thread):
            def __init__(self, process, fileobj, writer, copy_to=None):
                super().__init__()
                self.process = process
                self.fileobj = fileobj
                self.writer_func = writer
                self.copy_to = copy_to

            def run(self):
                if self.fileobj == None:
                    return
                while True:
                    try:
                        try:
                            c = self.fileobj.read(1)
                        except ValueError:
                            raise OSError("file object invalid")

                        # Make sure it is bytes:
                        try:
                            c = c.encode("utf-8", "replace")
                        except AttributeError:
                            pass

                        # Write to copy file handle if present:
                        if self.copy_to != None:
                            try:
                                self.copy_to.write(c)
                            except TypeError:
                                self.copy_to.write(c.decode(
                                    "utf-8", "replace"))

                        # Process writer func:
                        if self.writer_func != None:
                            self.writer_func(c)
                    except OSError:
                        break

        # Launch output readers as required and collect data:
        stdout_reader = None
        stderr_reader = None
        presented_output_store = {
            "data" : b"",
            "user_write_error" : None,
        }
        def writer(data):
            """ This function gets called by the thread to store data from
                stdin to be returned or used later on.
            """
            if len(data) == 0:
                return
            try:
                # Call user write function:
                if write_function != None:
                    write_function(data)
            except Exception as e:
                # Propagate error if the user function caused an error:
                presented_output_store["user_write_error"] = str(e)
                try:
                    process.kill()
                except Exception:
                    pass
            presented_output_store["data"] += data
        if actual_stdout == subprocess.PIPE:  # we need to collect stdout.
            if copy_to_regular_stdout_stderr:
                copy_to = sys.stdout
            else:
                copy_to = None
            stdout_reader = OutputReader(process, process.stdout, writer,
                copy_to=copy_to)
            stdout_reader.start()
        if actual_stderr == subprocess.PIPE:  # we need to collect stderr.
            if copy_to_regular_stdout_stderr:
                copy_to = sys.stderr
            else:
                copy_to = None
            if stderr == subprocess.STDOUT:
                stderr_reader = OutputReader(process, process.stderr, writer,
                    copy_to=sys.stdout)
            else:
                stderr_reader = OutputReader(process, process.stderr, None,
                    copy_to=copy_to)
            stderr_reader.start()

        # Wait for process to end:
        process.wait()
        try:
            process.stdout.close()
        except Exception:
            pass
        try:
            process.stderr.close()
        except Exception:
            pass
        try:
            process.kill()
        except Exception:
            pass

        # Evaluate whether it ran successfully:
        exit_code = process.returncode
        got_data = None
        if presented_output_store["user_write_error"] != None:
            raise RuntimeError("specified write_function " +\
                "encountered an error: " +\
                str(presented_output_store["user_write_error"]))
        if stdout_reader != None or (stderr_reader != None and \
                stderr == subprocess.STDOUT):
            got_data = presented_output_store["data"]
        if exit_code != 0:
            new_error = subprocess.CalledProcessError(
                cmd=cmd, returncode=exit_code)
            new_error.output = got_data
            raise new_error

        return got_data

    @staticmethod
    def check_output_with_isolated_pty(cmd, shell=False, cwd=None,
            stdout=None, stderr=None, timeout=None, env=None):
        """ Like subprocess.check_output, but runs the process in a pseudo-TTY
            using python's pty module.
        """
        return SubprocessHelper._check_output_extended(
            cmd, shell=shell, cwd=cwd, env=env, stdout=stdout, stderr=stderr,
            pty=True, copy_to_regular_stdout_stderr=False)

class GlobalConfig(object):
    """ GlobalConfig is the reader for the global drudder config file provided
        by the user. Once instantiated, it will load up the config and you can
        obtain the config values using the get() member.
    """

    def __init__(self):
        self.config_path = None
        self.data = dict()

    def open(self, read_from_path=None):
        # Locate config, if any:
        if not read_from_path:
            if "HOME" in os.environ:
                if os.path.exists(os.path.join(
                        os.environ["HOME"], ".config", "drudder",
                        "config.yml")):
                    self.config_path = os.path.join(
                        os.environ["HOME"], ".config", "drudder",
                        "config.yml")
            if os.path.exists("/etc/drudder/config.yml") and \
                    self.config_path == None:
                self.config_path = "/etc/drudder/config.yml"
        else:
            self.config_path = read_from_path

        # Read config:
        if self.config_path != None:
            try:
                with open(self.config_path, "r") as f:
                    self.data = yaml.safe_load(f.read())
            except Exception as e:
                print("drudder: error: FATAL: couldn't open or parse " +
                    "global config at: " + str(self.config_path),
                    flush=True, file=sys.stderr)
                print("drudder: error: FATAL: config read error is: " +
                    str(e), flush=True, file=sys.stderr)
                sys.exit(1)

    def get(self, value_path, default=None, _base_obj=None,
            value_type="list"):
        """ Retrieve the value at the given path key1.subkey2.value
            in the config dictionary (as described by the YAML).

            Returns the default instead if the value is not present.
        """

        if default == None and value_type == list:
            default = []
        parts = value_path.split(".")

        # Set base object to config root if none specified:
        if _base_obj == None:
            _base_obj = self.data

        # Get value at first path element:
        result = _base_obj.get(parts[0])
        if result == None:
            return default

        # Recurse deeper if the path is longer than one element:
        if len(parts) > 1:
            if not isinstance(result, dict):
                return default
            return self.get(parts[1:], default=default, _base_obj=result,
                value_type=value_type)

        if not type(result) == value_type:
            return default
        return result
gc = GlobalConfig()

class ExecutionCache(object):
    def __init__(self, duration_ms=600):
        self.duration = duration_ms * 0.001
        self._cache = dict()

    def check_output(self, cmd, **kwargs):
        return self.check_output_with_isolated_pty(cmd, **kwargs)

    def check_output_with_isolated_pty(self, cmd, shell=False, cwd=None,
            stdout=None, stderr=None, timeout=None, env=None,
            ignore_cache=False):
        cache_key = (
            ("cmd", tuple(cmd)),
            ("shell", shell),
            ("cwd", cwd),
            ("_env_keys", frozenset((env or {}).keys()))
        )
        # Wipe cache if we're supposed to ignore it:
        if ignore_cache:
            if cache_key in self._cache:
                del(self._cache[cache_key])

        # Check for old cache result:
        entry = None
        try:
            entry = self._cache[cache_key]
            if entry[0] + self.duration < time.monotonic():
                del(self._cache[cache_key])
                entry = None
        except KeyError:
            pass

        # Return cached result if any:
        if entry != None:
            return entry[1]


        # Run and add to cache:
        _had_error = None
        try:
            result = SubprocessHelper.check_output_with_isolated_pty(
                cmd, shell=shell, cwd=cwd, stdout=stdout, stderr=stderr,
                timeout=timeout, env=env)
        except subprocess.CalledProcessError as e:
            result = e.output
            _had_error = e
        self._cache[cache_key] = (time.monotonic(), result)
        if _had_error != None:
            raise _had_error
        return result
execution_cache = ExecutionCache()

class DoubleLineBreakFormatter(HelpFormatter):
    """ Retains double line breaks/paragraphs """
    def _split_lines(self, text, width):
        return self._fill_text(text, width, "").splitlines(False)

    def _fill_text(self, t, width, indent):
        t = " ".join([s for s in t.replace("\t", " ").strip("\t ").split(" ")\
            if len(s) > 0]).replace("\n ", "\n").replace(" \n", " ")
        ts = re.sub("([^\n])\n([^\n])", "\\1 \\2", t).split("\n\n")
        result = [textwrap.fill(paragraph, width,
            initial_indent=indent, subsequent_indent=indent)\
            for paragraph in ts]
        return "\n\n".join(result)

class CommandLineArgumentParser(object):
    @staticmethod
    def construct_parser():
        parser = argparse.ArgumentParser(description=textwrap.dedent('''\
            drudder: a tool for managing a larger collection of
            docker-compose.yml-specified container groups simply.'''
            ),
            formatter_class=DoubleLineBreakFormatter)
        parser.add_argument("action",
            help=textwrap.dedent('''\
            Possible values:

            "install-tools": install and upgrade all required tools like
                             docker-machine and docker-compose on the local
                             computer. This will automatically fetch and provide
                             anything required to run drudder.
     
            "list": list all known services.

            "start": start the service specified as argument (or "all" for all).

            "stop": stop the service specified as argument (or "all" for all).

            "restart": restart the service specified as argument (or "all" for all).

            "info": show more detailed info for the given service (or "all for
                    all).

            "logs": output the logs of all the docker containers of the service
                    specified as argument (or "all" for all).

            "shell": start an interactive shell in the specified service's specified
                     subservice (parameters: <service> <subservice>) - the subservice
                     is optional if the docker-compose.yml has just one
                     container/subservice.

            "snapshot": store an atomic snapshot of the live data of the service
                        specified as argument (from livedata/) in livedata-snapshots/

            "clean": clean up all stopped containers. THIS IS NOT REVERSIBLE. The
                     docker images of course won't be touched.

            "dump-service-info" : dump all sorts of detailed service info as YAML.
                                  Mainly useful if you want to process the combined
                                  data computed by drudder with another tool
                                  of yours.
            ''')

            )
        parser.add_argument("argument", nargs="*",
            help="argument(s) to given action")
        parser.add_argument("--version", "-v", help="show version and quit",
            default=False, action="store_true",
            dest="show_version")
        parser.add_argument("--force",
            default=False, action="store_true",
            help="Can be used to override various safety warnings. Consult the "+\
                "respective warning prompted by an action to understand the "+\
                "consequences",
            dest="force")
        parser.add_argument("--config-path", "-c",
            default=None, dest="config_path",
            help="Specific path to read the global drudder config from. If not " +\
            "specified, it will be searched and opened from default locations: " +\
            "$HOME/.config/drudder/config.yml, /etc/drudder/config.yml")
        parser.add_argument("--no-rebuild",
            default=False, action="store_true",
            dest="no_rebuild",
            help="Avoid automatic rebuilding if not absolutely required to " +\
            "execute the given action")
        return parser

# Abort early before attempting to install missing packages when --help or
# --version was specified:
if __name__ == "__main__":
    if len(" ".join(sys.argv[1:]).strip()) == 0 or \
            sys.argv[1] == "--version" or sys.argv[1] == "-v" or \
            sys.argv[1] == "-h" or sys.argv[1] == "--help":
        parser = CommandLineArgumentParser.construct_parser()
        if len(sys.argv) > 1 and (sys.argv[1] == "--version" or
                sys.argv[1] == "-v"):
            print("drudder version " + str(TOOL_VERSION))
            sys.exit(0) 
        parser.print_help()
        if len(" ".join(sys.argv[1:]).strip()) == 0:
            sys.exit(1)
        sys.exit(0)

class PyamlInstallInfo(object):
    @staticmethod
    def test_for_pyaml():
        try:
            import pyaml
            return True
        except ImportError:
            path = PyamlInstallInfo.custom_pyaml_install_path()
            if path != None:
                return True
        return False

    @staticmethod
    def custom_pyaml_install_path_parent():
        if platform.system().lower() == "windows":
            return os.path.join(os.environ['WINDIR'], "drudder-extra-pyaml")
        else:
            return os.path.join("/var", "lib", "drudder-extra-pyaml")

    @staticmethod
    def custom_pyaml_install_path():
        parent_folder = PyamlInstallInfo.custom_pyaml_install_path_parent()
        if not os.path.exists(parent_folder):
            return None
        for fname in os.listdir(parent_folder):
            if fname.startswith("pyaml-"):
                full_path = os.path.join(parent_folder, fname)
                if os.path.isdir(full_path):
                    return full_path
        return None
 

class YAMLInstallInfo(object):
    @staticmethod
    def test_for_yaml():
        try:
            import yaml
            return True
        except ImportError:
            path = YAMLInstallInfo.custom_yaml_install_path()
            if path != None:
                return True
        return False

    @staticmethod
    def custom_yaml_install_path_parent():
        if platform.system().lower() == "windows":
            return os.path.join(os.environ['WINDIR'], "drudder-PyYAML")
        else:
            return os.path.join("/var", "lib", "drudder-PyYAML")

    @staticmethod
    def custom_yaml_install_path():
        parent_folder = YAMLInstallInfo.custom_yaml_install_path_parent()
        if not os.path.exists(parent_folder):
            return None
        for fname in os.listdir(parent_folder):
            if fname.startswith("PyYAML-"):
                full_path = os.path.join(parent_folder, fname)
                if os.path.isdir(full_path):
                    return full_path
        return None
       

class Installer(object):
    @staticmethod
    def latest_docker_machine_version():
        import urllib.request
        tag_url = "https://github.com/docker/machine/releases/latest/"
        try:
            target = urllib.request.urlopen(tag_url)
            final_url = target.geturl()
            try:
                final_url = final_url.decode("utf-8")
            except AttributeError:
                pass
            if final_url.endswith("/"):
                final_url = final_url[:-1]
            version_tag = final_url.rpartition("/")[2]
        except Exception as e:
            raise RuntimeError("failed to obtain docker-machine version tag")
        return version_tag

    @staticmethod
    def latest_docker_compose_version():
        import urllib.request
        tag_url = "https://github.com/docker/compose/releases/latest/"
        try:
            target = urllib.request.urlopen(tag_url)
            final_url = target.geturl()
            try:
                final_url = final_url.decode("utf-8")
            except AttributeError:
                pass
            if final_url.endswith("/"):
                final_url = final_url[:-1]
            version_tag = final_url.rpartition("/")[2]
        except Exception as e:
            raise RuntimeError("failed to obtain docker-compose version tag")
        return version_tag

    @staticmethod
    def install_docker_machine():
        return Installer.install_docker_tool(
            tool_name="docker-machine")

    @staticmethod
    def install_docker_compose():
        return Installer.install_docker_tool(
            tool_name="docker-compose")

    @staticmethod
    def install_docker_tool(tool_name=None):
        if tool_name == None:
            raise ValueError(
                "provide the name of the tool to be installed!")

        # Getting various helper functions:
        if tool_name == "docker-machine":
            path_func = SystemInfo.docker_machine_path
            version_func = Installer.latest_docker_machine_version
            short_name = "machine"
        elif tool_name == "docker-compose":
            path_func = SystemInfo.docker_compose_path
            version_func = Installer.latest_docker_compose_version
            short_name = "compose"
        else:
            raise ValueError("unsupported docker tool: " + tool_name)

        # Check if the given tool is already installed:
        if path_func(fail_if_not_found=False) != None:
            # Already installed, nothing to do.
            return

        # Follow the "latest" URL to the actual latest tag:
        try:
            version_tag = version_func()
        except RuntimeError:
            print("drudder: error: failed to obtain " + tool_name +\
                "version tag",
                file=sys.stderr)
            print("Please check your internet connectivity.",
                file=sys.stderr)
            sys.exit(1)

        import urllib.request
        
        # Put together the download URL:
        # Reference URL:
        # https://github.com/docker/machine/releases/download/
        #   v0.7.0/docker-machine-`uname -s`-`uname -m`
        if platform.system().lower() == "windows":
            url = "https://github.com/docker/" + short_name +\
                "/releases/" +\
                "download/" + str(version_tag) + "/docker-" +\
                short_name + "-" +\
                "Windows-x86_64.exe"
        else:
            url = "https://github.com/docker/" +\
                short_name + "/releases/" +\
                "download/" + str(version_tag) + "/docker-" +\
                short_name + "-" +\
                subprocess.check_output(["uname", "-s"]).\
                    decode("utf-8").strip() + "-" +\
                subprocess.check_output(["uname", "-m"]).\
                    decode("utf-8").strip()

        # Download docker-machine binary:
        print("Downloading " + tool_name + "... (this might take " +\
                "a while)")
        try:
            local_filename, headers = urllib.request.urlretrieve(
                url)
        except Exception as e:
            print("drudder: error: failed to obtain " + str(url),
                file=sys.stderr)
            print("Please check your internet connectivity.",
                file=sys.stderr)
            sys.exit(1)

        # Put binary into place for use:
        if platform.system().lower() == "windows":
            docker_tool_target = os.path.join(os.environ['WINDIR'],
                "system32", tool_name + ".exe")
        else:
            docker_tool_target = os.path.join("/usr", "local", "bin",
                tool_name)
        try:
            shutil.copy(local_filename, docker_tool_target)
        except Exception as e:
            print("drudder: error: failed to create path: " +\
                str(docker_tool_target), file=sys.stderr)
            print("Please re-run this with sudo / administrator " +\
                "privileges.", file=sys.stderr)
            sys.exit(1)

        # Set executable flag (+x):
        import stat
        os.chmod(docker_tool_target,
            os.stat(docker_tool_target).st_mode
            | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)

    @staticmethod
    def check_and_install_missing():
        missing = []
        if not YAMLInstallInfo.test_for_yaml():
            missing.append("PyYAML")
        if not PyamlInstallInfo.test_for_pyaml():
            missing.append("pyaml")
        if not SystemInfo.docker_path(fail_if_not_found=False):
            missing.append("docker")
        if not SystemInfo.docker_machine_path(fail_if_not_found=False):
            missing.append("docker-machine")
        if not SystemInfo.docker_compose_path(fail_if_not_found=False):
            missing.append("docker-compose")

        explicit_install = False
        if len(sys.argv) >= 2:
            if sys.argv[1].strip() == "install-tools":
                explicit_install = True

        if len(missing) > 0:
            do_install = True
            if not explicit_install:
                print("drudder: warning: missing components detected: " +\
                    ", ".join(missing) + ". Invoking installer...")
                do_install = SystemInfo.yesno(
                    "Do you want to install (system-wide): " +\
                    ", ".join(missing), default_yes=False)
            else:
                print("Installing the following components: " +\
                    ", ".join(missing))
            if do_install:
                for component in missing:
                    if component == "PyYAML":
                        Installer.install_yaml()
                    elif component == "pyaml":
                        Installer.install_pyaml()
                    elif component == "docker-machine":
                        Installer.install_docker_machine()
                    elif component == "docker-compose":
                        Installer.install_docker_compose()
                    elif component == "docker":
                        Installer.install_docker()
            else:
                print("drudder: error: missing components: " +\
                    ", ".join(missing))
                sys.exit(1)

        if explicit_install:
            print("All tools installed.", file=sys.stderr)
            os._exit(0)

    @staticmethod
    def install_yaml():
        if YAMLInstallInfo.test_for_yaml():
            # YAML is available, nothing to do.
            return
        Installer.install_from_pypi("PyYAML",
            YAMLInstallInfo.custom_yaml_install_path_parent())

    @staticmethod
    def install_pyaml():
        if PyamlInstallInfo.test_for_pyaml():
            # YAML is available, nothing to do.
            return
        Installer.install_from_pypi("pyaml",
            PyamlInstallInfo.custom_pyaml_install_path_parent())

    @staticmethod
    def install_from_pypi(pypi_url_name, parent_location):
        # Obtain PyPI page for download link:
        import urllib.request
        pyyaml_url = 'https://pypi.python.org/pypi/' + pypi_url_name
        try:
            with urllib.request.urlopen(
                    pyyaml_url) as response:
                html = response.read()
                try:
                    html = html.decode("utf-8", "replace")
                except AttributeError:
                    pass
        except Exception as e:
            print("drudder: error: failed to obtain " + str(pyyaml_url),
                file=sys.stderr)
            print("Please check your internet connectivity.", file=sys.stderr)
            sys.exit(1)
        def extract_source_link():
            _html = html
            pos = 0
            while True:
                def extract_block_url_title(block):
                    link_start = block.find("<a href=\"")
                    if link_start < 0:
                        return (None, None)
                    link_start += len("<a href=\"")
                    link_end = block[link_start:].find("\"")
                    if link_end < 0:
                        return (None, None)
                    link_end += link_start
                    text_block_start = block.find(">",
                        block.find("<td ", link_end))
                    if text_block_start < 0:
                        return (None, None)
                    text_block_end = block.find("<", text_block_start)
                    if text_block_end < 0:
                        return (None, None)
                    link_target = block[link_start:link_end]
                    block_title = block[text_block_start:text_block_end]
                    return (link_target, block_title)
                # Find next relevant block start:
                next_block_start_1 = _html.find("<tr class=\"odd", pos)
                next_block_start_2 = _html.find("<tr class=\"even", pos)
                next_block_start = next_block_start_1
                if next_block_start < 0 or (next_block_start_2 >= 0 and \
                        next_block_start_2 < next_block_start):
                    next_block_start = next_block_start_2
                pos = max(pos + 1, next_block_start)
                if next_block_start < 0:
                    return None
                # Get the end to the block:
                next_block_end = _html[next_block_start:].find("</tr>")
                if next_block_end <= 0:
                    return None
                next_block_end += next_block_start
                # Analyze block for link
                (url, title) = extract_block_url_title(_html[
                    next_block_start:next_block_end])
                if url != None and title.strip().lower().find("source") >= 0:
                    return url.partition("#")[0]

        # Get source download link:
        yaml_source_link = extract_source_link()
        if yaml_source_link == None:
            print("drudder: error: failed to obtain " +
                pypi_url_name + " download link. " +
                "Please check your internet connection, and if this " +
                "problem persists, file a bug at " +
                BUG_URL, file=sys.stderr)
            sys.exit(1)
        if not yaml_source_link.startswith("https://"):
            print("drudder: error: " +
                pypi_url_name + " download link is bogus, " +
                "unexpected protocol: " + str(yaml_source_link))
            print("Please file a bug at " +
                BUG_URL, file=sys.stderr)
            sys.exit(1)

        # Make sure the install base path exists:
        parent_path = parent_location
        if not os.path.exists(parent_path):
            try:
                os.mkdir(parent_path) 
            except OSError:
                print("drudder: error: failed to create path: " +
                    str(parent_path), file=sys.stderr)
                print("Please re-run this with sudo / administrator " +
                    "privileges.", file=sys.stderr)
                sys.exit(1)

        # Get source:
        try:
            print("Downloading " + pypi_url_name + "... (this might take " +
                "a while)")
            local_filename, headers = urllib.request.urlretrieve(
                yaml_source_link)
            source_extract_path = os.path.join(
                parent_path, "pypi_package.tar.gz")
            shutil.copy(local_filename, source_extract_path)
            os.remove(local_filename)
        except Exception as e:
            print("drudder: error: failed to obtain " + str(
                yaml_source_link_url),
                file=sys.stderr)
            print("Please check your internet connectivity.", file=sys.stderr)
            sys.exit(1)

        # Extract package:
        import tarfile
        tar_file = tarfile.open(source_extract_path)
        tar_file.extractall(parent_path)

    @staticmethod
    def install_docker():
        # Check if already installed:
        path_to_docker = SystemInfo.docker_path(fail_if_not_found=False)
        if path_to_docker != None:
            # Docker is already installed, nothing to do.
            return

        # Detect distribution:
        distribution = None
        distribution_codename = None
        distribution_version = None
        if os.path.exists("/etc/lsb-release"):
            with open("/etc/lsb-release") as f:
                contents = f.read()
            try:
                contents = contents.decode("utf-8", "replace")
            except AttributeError:
                pass
            contents = contents.replace("\r\n", "\n").replace("\r", "\n")
            for line in contents.split("\n"):
                _line = line.strip()
                if _line.startswith("DISTRIB_ID="):
                    distribution = _line.partition("=")[2].lower()
                elif _line.startswith("DISTRIB_CODENAME"):
                    distribution_codename = _line.partition("=")[2].lower()
                elif _line.startswith("DISTRIB_RELEASE"):
                    distribution_version = _line.partition("=")[2]
        elif os.path.exists("/etc/os-release"):
            with open("/etc/os-release") as f:
                contents = f.read()
            try:
                contents = contents.decode("utf-8", "replace")
            except AttributeError:
                pass
            contents = contents.replace("\r\n", "\n").replace("\r", "\n")
            for line in contents.split("\n"):
                _line = line.strip()
                if _line.startswith("ID="):
                    distribution = _line.partition("=")[2].lower()
                    if distribution.startswith("\"") and \
                            distribution.endswith("\""):
                        distribution = distribution[1:-1]
                elif _line.startswith("VERSION=") and _line.find("(") > 0:
                    distribution_codename = ((_line.partition("=")[2]).\
                        partition("(")[2]).partition(")")[0].lower()
                elif _line.startswith("VERSION_ID="):
                    extract = line.partition("=")[2]
                    if extract.startswith("\"") and \
                            extract.endswith("\""):
                        extract = extract[1:-1]
                    try:
                        distribution_version = int(extract)
                    except (TypeError, ValueError):
                        pass
        if distribution == None and os.path.exists("/etc/redhat-release"):
            with open("/etc/redhat-release") as f:
                contents = f.read()
            try:
                contents = contents.decode("utf-8", "replace")
            except AttributeError:
                pass
            contents = contents.strip()
            if contents.startswith("Fedora release "):
                distribution = "fedora"
                distribution_version = contents[len("Fedora release "):].lstrip()
                distribution_version = distribution_version.partition(" ")[0]
        if distribution == None:
            print("drudder: error: failed to obtain detect distribution. " +\
                "This distribution is most likely not supported for " +\
                "docker install.",
                file=sys.stderr)
            print("If you want to help change this, please file a ticket "+\
                "at " + str(BUG_URL), file=sys.stderr)
            sys.exit(1)

        print("drudder: info: [Installer/docker] " +\
            "detected distribution: " + str(distribution))

        # Debian/Ubuntu docker install:
        if distribution == "ubuntu" or distribution == "debian":
            if distribution_codename == None:
                print("drudder: error: failed to detect detailed " +\
                    "distribution code name: " + distribution,
                    file=sys.stderr)
                print("Please report this problem at " + BUG_URL,
                    file=sys.stderr)
                sys.exit(1)
            try:
                print("drudder: info: [Installer/docker] " +\
                    "Importing docker-engine repository key...")
                subprocess.check_output(["apt-key", "adv", "--keyserver",
                    "hkp://p80.pool.sks-keyservers.net:80", "--recv-keys",
                    "58118E89F3A912897C070ADBF76221572C52609D"])
            except subprocess.CalledProcessError:
                print("drudder: error: distribution key import failed.",
                    file=sys.stderr) 
                print("Please re-run this with sudo / administrator " +\
                    "privileges, and check your internet connectivity.",
                    file=sys.stderr)
                print("If you think this is a bug, please file a ticket "+\
                    "at " + str(BUG_URL), file=sys.stderr)
                sys.exit(1)
            print("drudder: info: [Installer/docker] Adding " +\
                "docker-engine repository to apt sources...")
            sources_lst_file = os.path.join("/etc", "apt", "sources.list.d",
                "docker.list")
            if os.path.exists(sources_lst_file):
                try:
                    os.remove(sources_lst_file)
                except OSError:
                    print("drudder: error: failed to remove file: " +\
                        str(sources_lst_file))
                    print("Please re-run this with sudo / administrator " +\
                        "privileges", file=sys.stderr)
                    sys.exit(1)
            try:
                with open(sources_lst_file, "w") as f:
                    f.write("deb https://apt.dockerproject.org/repo " +\
                        distribution + "-" + distribution_codename + " main")
            except OSError:
                print("drudder: error: failed to create file: " +\
                    str(sources_lst_file))
                print("Please re-run this with sudo / administrator " +\
                    "privileges", file=sys.stderr)
                sys.exit(1)

            # Refresh package index with new repository added:
            while True:
                print("drudder: info: [Installer/docker] " +\
                    "Running: apt-get update")
                try:
                    SubprocessHelper.check_output_with_stdout_copy(
                        ["apt-get", "update"],
                        stderr=subprocess.STDOUT)
                except subprocess.CalledProcessError as e:
                    if e.output.find(b"E: The method driver " +\
                            b"/usr/lib/apt/methods/https could not be found"
                            ) >= 0:
                        print("[Installer/docker] Running: " +\
                            "apt-get install -y docker-engine")
                        SubprocessHelper.check_output_with_stdout_copy(
                            ["apt-get", "install", "-y",
                            "apt-transport-https"],
                            stderr=subprocess.STDOUT)
                        continue
                    raise e
                break

            # Helper function to find out missing media sources:
            media_error = { "temp_output" : b"" }
            def media_change_error_detection(data):
                media_error["temp_output"] += data
                if media_error["temp_output"].find(
                        b"Media change: please insert the disc") >= 0:
                    raise RuntimeError("aborting - missing install media")

            # Install docker-engine package:
            try:
                print("drudder: info: [Installer/docker] running: " +\
                    "apt-get install -y docker-engine")
                media_error["temp_output"] = b""
                SubprocessHelper.check_output_with_stdout_copy(
                    ["apt-get", "install", "-y", "docker-engine"],
                    stderr=subprocess.STDOUT, pty=True,
                    write_function=media_change_error_detection)
            except RuntimeError:
                # Media change hang occured. Abort, and tell user:
                time.sleep(0.5)
                subprocess.call(["reset"]) # fix terminal
                time.sleep(0.5)

                # Tell user:
                print("drudder: error: your apt sources depend on " +\
                    "an install media that is not present. " +\
                    "Please fix your apt sources, or provide the " +\
                    "required install media (CD-ROM, ...).", file=sys.stderr)
                print("This is usually a user error.", file=sys.stderr)
                print("If you are certain this is a bug in " +\
                    "drudder, please report it at " + str(BUG_URL),
                    file=sys.stderr)
                sys.exit(1)
            return
        # Fedora docker install:
        elif distribution == "fedora":
            sources_lst_file = os.path.join("/etc", "yum.repos.d",
                "docker.repo")
            print("drudder: info: [Installer/docker] writing " +\
                "repository info to " +\
                str(sources_lst_file))
            if os.path.exists(sources_lst_file):
                try:
                    os.remove(sources_lst_file)
                except OSError:
                    print("drudder: error: failed to remove file: " +\
                        str(sources_lst_file))
                    print("Please re-run this with sudo / administrator " +\
                        "privileges", file=sys.stderr)
                    sys.exit(1)
            try:
                with open(sources_lst_file, "w") as f:
                    f.write(textwrap.dedent("""\
                    [dockerrepo]
                    name=Docker Repository
                    baseurl=https://yum.dockerproject.org/repo/main/fedora/$releasever/
                    enabled=1
                    gpgcheck=1
                    gpgkey=https://yum.dockerproject.org/gpg
                    """))
            except OSError:
                print("drudder: error: failed to create file: " +\
                    str(sources_lst_file))
                print("Please re-run this with sudo / administrator " +\
                    "privileges", file=sys.stderr)
                sys.exit(1)

            # Install docker-engine package:
            print("drudder: info: [Installer/docker] Running: " +\
                "dnf install -y docker-engine")
            SubprocessHelper.check_output_with_stdout_copy([
                "dnf", "install", \
                "-y", "docker-engine"], stderr=subprocess.STDOUT)
            return
        # Others:
        else:
            print("drudder: error: unsupported distribution: " + str(
                distribution))
            print("If you want to help change this, please file a ticket "+\
                "at " + str(BUG_URL), file=sys.stderr)
            sys.exit(1)


# Only import this on Unix systems:
if platform.system().lower() != "windows":
    import stat

class SystemInfo(object):
    """ This class provides system info of various sorts about the installed
        docker versions, btrfs and more.
    """
    __cached_docker_path = None
    __cached_docker_machine_path = None

    @staticmethod
    def yesno(text, default_yes=False):
        if not default_yes:
            appendix = " [y/N]"
        else:
            appendix = " [Y/n]"
        ask_text = text + appendix
        result = input(ask_text).lower().strip()
        if result == "y" or result == "yes":
            return True
        if result == "n" or result == "no":
            return False
        if default_yes:
            return True
        return False

    @staticmethod
    def locate_binary(name):
        """ Locate a binary of some name in the common system-wide places
            and return the full path, or return None if it can't be found.
        """
        badchars = '\'"$<> %|&():*/\\{}#!?=\n\r\t[]\033'
        for char in badchars:
            if name.find(char) >= 0:
                raise ValueError("dangerous character in binary name")
        output = None
        try:
            output = subprocess.check_output(
                "which " + name, shell=True,
                stderr=subprocess.STDOUT).\
                decode("utf-8", "ignore").strip()
        except subprocess.CalledProcessError:
            pass
        if output == None or len(output) == 0:
            return None
        return output

    @staticmethod
    def docker_machine_path(fail_if_not_found=True):
        if SystemInfo.__cached_docker_machine_path != None:
            return SystemInfo.__cached_docker_machine_path
        path = SystemInfo.locate_binary("docker-machine")
        if path != None:
            return path
        if fail_if_not_found is True:
            print("drudder: error: no docker-machine found. Is it installed?")
            sys.exit(1)
        return None

    @staticmethod
    def docker_path(fail_if_not_found=True):
        """ Locate docker binary and return its path, or print an error
            and exit the program with sys.exit(1) if not available.
        """
        if SystemInfo.__cached_docker_path != None:
            return SystemInfo.__cached_docker_path
        
        def behaves_like_docker(binary_path):
            output = None
            try:
                output = subprocess.check_output([
                    binary_path, "--version"],
                    stderr=subprocess.STDOUT)
            except subprocess.CalledProcessError as e:
                output = e.output
            output = output.decode("utf-8", "ignore")
            return (output.lower().startswith("docker version "))

        test_names = [ "docker.io", "docker" ]

        for test_name in test_names:
            bin_path = SystemInfo.locate_binary(test_name)
            if bin_path == None:
                continue
            if behaves_like_docker(bin_path):
                return bin_path
        if fail_if_not_found is True:
            print("drudder: error: no docker found. Is it installed?")
            sys.exit(1)
        return None
    
    @staticmethod
    def is_btrfs_subvolume(path):
        """ Check if the given path is a btrfs subvolume. Returns True if
            if it is, or False if not. May print out various warnings if there
            were problems detecting this. May raise a ValueError if the path
            couldn't be properly examined for some reason.
        """
        if (not os.path.exists(path) or
                not os.path.isdir(path)) and len(path) > len(os.path.sep):
            if path.endswith(os.path.sep):
                path = path[:-len(os.path.sep)]
            shorten_index = max(0, path.find(os.path.sep))
            return SystemInfo.is_btrfs_subvolume(
                path[:shorten_index])

        path = os.path.realpath(path)
        nontrivial_error = "error: failed to map a btrs subvolume "+\
            "to its POSIX path. This seems to be a non-trivial setup."+\
            " You should do your snapshotting manually!!"

        # First, get the containing mount point:
        mount = SystemInfo.get_fs_mount_root(path)
        if mount == None:
            print("drudder: warning: internal problem: " +\
                "mount point of " +\
                str(base_path) + " was returned as: None")
            details = "cannot check path. Mount point search failed: " +\
                str(path)
            print_msg(nontrivial_error + "\n\nError details: " + details,
                color="red")
            raise ValueError(details)

        # Get btrfs subvolume list:
        output = subprocess.check_output([
            SystemInfo.locate_binary("btrfs"),
            "subvolume", "list", path]).\
            decode("utf-8", "ignore").strip().split("\n")
        for line in output:
            if len(line.strip()) == 0:
                continue
            if not line.startswith("ID ") or line.find(" path ") < 0:
                raise RuntimeError("unexpected btrfs tool output - " + \
                    "maybe incompatible tool version? Please report this." +\
                    " Full output: " + str(output))
            line = line[line.find(" path ")+len(" path "):].strip()
            full_path_guess = mount + os.path.sep + line
            full_path_guess = full_path_guess.replace(
                os.path.sep + os.path.sep, os.path.sep)  # remove double slash
            if not os.path.exists(full_path_guess):
                if line == "DELETED":
                    continue
                print_msg(nontrivial_error + "\n\nError details: " +
                    "btrfs full base path guess doesn't exist: " +
                    str(full_path_guess) + " (checked upon: " +
                    path + ")", color="red")
                return False
            full_path_guess = os.path.normpath(os.path.abspath(
                full_path_guess).replace(os.path.sep + os.path.sep,
                os.path.sep))
            normpath = os.path.normpath(os.path.abspath(path).replace(
                os.path.sep + os.path.sep, os.path.sep))
            if full_path_guess == normpath:
                try:
                    output = subprocess.check_output([
                        SystemInfo.locate_binary("stat"),
                        "-c", "%i", path]).decode('utf-8', 'ignore').strip()
                except subprocess.CalledProcessError as e:
                    # Stat failed, although btrfs subvolume list lists it!
                    details = "btrfs volume listed by btrfs tool " +\
                        "cannot be stat'ed: " + str(path)
                    print_msg(nontrivial_error + "\n\nError details: " +\
                        details, color="red")
                    raise ValueError(details)
                if output != "256":
                    # Not a subvolume, although btrfs subvolume list lists it!
                    details = "btrfs volume listed by btrfs tool " +\
                        "doesn't appear to be " +\
                        "an actual btrfs volume according to stat: " +\
                         str(path)
                    print_msg(nontrivial_error + "\n\nError details: " +\
                        details, color="red")
                    raise ValueError(details)
                return True
        try:
            output = subprocess.check_output([
                SystemInfo.locate_binary("stat"),
                "-c", "%i", path]).decode('utf-8', 'ignore').strip()
        except subprocess.CalledProcessError as e:
            pass
        if output == "256":
            # Stat says it's a subvolume, although we don't think it is!
            details = "stat reports path as btrfs volume, but " +\
                "we didn't detect respective entry from btrfs tool: " +\
                str(path)
            print_msg(nontrivial_error + "\n\nError details: " + \
                details, color="red")
            raise ValueError(details)
        return False


    @staticmethod
    def docker_compose_path(fail_if_not_found=True):
        """ Locate docker-compose binary and return its path, or exit process
            with error if not available.
        """
        bin_path = SystemInfo.locate_binary("docker-compose")
        if bin_path != None:
            return bin_path
        if fail_if_not_found:
            print("drudder: error: no docker-compose found. " + \
                "Is it installed?")
            sys.exit(1)
        return None

    @staticmethod
    def btrfs_path():
        """ Locate btrfs helper tool binary and return its path, or return
            None if not found.
        """
        bin_path = SystemInfo.locate_binary("btrfs")
        if bin_path != None:
            return bin_path
        return None

    @staticmethod
    def get_fs_mount_root(path):
        path = SystemInfo.get_df_mounted_at(path)
        mounts = subprocess.check_output([
            "mount"])
        try:
            mounts = mounts.decode("utf-8", "replace")
        except AttributeError:
            pass
        found_mount = None
        found_mount_len = None
        for mountl in mounts.strip().replace("\r\n", "\n").split("\n"):
            mountl = mountl.strip()
            if mountl.find(" on ") <= 0:
                continue
            mount_place = mountl.partition(" on ")[2].strip().partition(" ")[0]
            if path.startswith(mount_place) and (
                    found_mount == None or
                    len(mount_place) > found_mount_len):
                found_mount_len = len(mount_place)
                found_mount = mount_place
        return found_mount

    @staticmethod
    def get_df_mounted_at(path):
        """ Find out where the mount point of the filesystem is located of the
            given file path. (e.g. for a path "/home/myuser/somefile"
            located inside a home partition mounted at /home/, this would
            return "/home/") The information is probed using the "df" tool.
        """
        # Make the path absolute and make sure it leads to an existing thing:
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise ValueError("given path does not exist: " + str(path))

        # If this is a symlink itself, make sure the parent folder is
        # converted to the actual disk path (-> all parent symlinks are
        # resolved to actual disk paths):
        if os.path.islink(path):
            element_name = os.path.basename(os.path.normpath(path))
            parent = os.path.normpath(os.path.join(
                os.path.normpath(path), ".."))
            path = os.path.join(os.path.normpath(parent), element_name)
        else:
            # Not a symlink itself. Simply make sure it's the actual real
            # path on disk (-> all symlinks in the path are resolved):
            path = os.path.realpath(path)

        # Check again to make sure the real disk path is an existing thing:
        if not os.path.exists(path):
            raise ValueError(
                "converted real path does not exist: " + str(path))

        # Run "df" to find the mount point:
        output = subprocess.check_output([
            SystemInfo.locate_binary("df"), path]).\
            decode("utf-8", "ignore").strip()

        # Skip first line:
        if output.find("\n") <= 0:
            raise RuntimeError("failed to parse df output")
        output = output[output.find("\n")+1:].partition("\n")[0]

        # Skip past first entry:
        skip_pos = output.find(" ")
        if skip_pos <= 0 or skip_pos >= len(output):
            raise RuntimeError("failed to parse df output")
        output = output[skip_pos+1:].strip()

        # Skip past all entries not starting with / (to get to "Mounte don")
        while True:
            fwslash = output.find("/")
            spacepos = output.find(" ")
            if fwslash < 0:
                raise RuntimeError("failed to parse df output")
            if spacepos >= 0 and spacepos < fwslash:
                output = output[spacepos+1:].strip()
                continue
            break

        if not output.startswith("/"):
            raise RuntimeError("failed to parse df output")
        if os.path.exists(output) and os.path.isfile(output):
            raise RuntimeError("failed to parse df output (" +
                "searching mount path for: " + str(path) + "): " +
                "result unexpectedly a regular file: " + str(output))
        return output

    @staticmethod
    def filesystem_type_at_path(path):
        """ Find out the filesystem a given directory or file is on and return
            the name (e.g. "ext4", "btrfs", ...). The information is probed
            using the "df" tool.
        """
        if not os.path.exists(path):
            raise ValueError("given path does not exist: " + str(path))
        output = subprocess.check_output([
            SystemInfo.locate_binary("df"), path]).\
            decode("utf-8", "ignore")

        # Skip first line:
        if output.find("\n") <= 0:
            raise RuntimeError("failed to parse df output")
        output = output[output.find("\n")+1:]

        # Get first word being the FS root of the path:
        end_pos = output.find(" ")
        if end_pos <= 0:
            raise RuntimeError("failed to parse df output")
        device_of_path = output[:end_pos]
        if device_of_path == "-":
            # We can't find out the filesystem of this path.
            # -> try to find out the parent!
            get_parent = os.path.normpath(path + "/../")
            if get_parent == os.path.normpath(path) or path == "/":
                # We are already at the root.
                return None
            return SystemInfo.filesystem_type_at_path(os.path.normpath(
                path + "/../"))

        output = subprocess.check_output([
            SystemInfo.locate_binary("mount")]).\
            decode("utf-8", "ignore")
        def find_mount_for(device):
            for line in output.split("\n"):
                if not line.startswith(device + " "):
                    continue
                i = len(line) - 1
                while not line[i:].startswith(" type ") and i > 0:
                    i -= 1
                if not line[i:].startswith(" type "):
                    raise RuntimeError("failed to parse mount output")
                fs_type = line[i + len(" type "):].strip()
                if fs_type.find(" ") > 0:
                    fs_type = fs_type[:fs_type.find(" ")].strip()
                return fs_type
            return None
        result = find_mount_for(device_of_path)
        if result == None:
            result = find_mount_for(os.path.realpath(device_of_path))
        if result != None:
            return result
        raise RuntimeError('failed to find according mount entry for: ' +\
            path + " (device path: " + str(device_of_path) + ")")

    @staticmethod
    def btrfs_subvolume_stat_check(path):
        """ Checks whether something that is supposedly a btrfs volume is
            also identified as such by the "stat" tool (returns True) or not
            (returns False).
        """
        output = subprocess.check_output([SystemInfo.locate_binary("stat"),
            "-c", "%i", path]).decode('utf-8', 'ignore').strip()
        return (output == "256")

if __name__ == "__main__":
    Installer.check_and_install_missing()

yaml_available = False
try:
    import yaml
    yaml_available = True
except ImportError as e1:
    yaml_available = False
    yml_lib_path = YAMLInstallInfo.custom_yaml_install_path()
    if yml_lib_path != None and os.path.exists(yml_lib_path):
        sys.path.insert(0, yml_lib_path + "/lib3/")
        sys.path.insert(0, yml_lib_path + "/lib3/yaml/")
        try:
            import yaml
            yaml_available = True
        except ImportError as e2:
            pass
if not yaml_available and __name__ == "__main__":
    print("drudder: error: PyYAML import unexpectedly failed. " +
        "Please file a bug at " + str(BUG_URL))
    sys.exit(1)
pyaml_available = False
try:
    import pyaml
    pyaml_available = True
except ImportError as e1:
    pyaml_available = False
    pyaml_lib_path = PyamlInstallInfo.custom_pyaml_install_path()
    if pyaml_lib_path != None and os.path.exists(pyaml_lib_path):
        sys.path.insert(0, pyaml_lib_path + "/")
        try:
            import pyaml
            pyaml_available = True
        except ImportError as e2:
            pass
if not pyaml_available and __name__ == "__main__":
    print("drudder: error: pyaml import unexpectedly failed. " +
        "Please file a bug at " + str(BUG_URL))
    sys.exit(1)

class DockerComposeYml(object):
    def __init__(self, path, service=None):
        # If path just goes to directory, add file name:
        if os.path.isdir(path):
            path = os.path.join(path, "docker-compose.yml")

        # Remember path:
        self.path = path

        # Load up .yml contents:
        contents = None
        with open(path, "rb") as f:
            contents = f.read().decode("utf-8").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")
        self.parsed_obj = yaml.safe_load(contents)

        # Extract desired data:
        self.is_v2 = False
        self._desired_data = self.parsed_obj
        if isinstance(self.parsed_obj, dict) and \
                "version" in self.parsed_obj:
            v = 0
            try:
                v = int(self.parsed_obj["version"])
            except (TypeError, ValueError):
                pass
            if v >= 2:
                self.is_v2 = True
        if service != None: # data requested for specific service:
            self._desired_data = None
            if self.is_v2:
                self._desired_data = None
                if isinstance(self.parsed_obj, dict) and \
                        "services" in self.parsed_obj:
                    if isinstance(self.parsed_obj["services"], dict) and \
                            service in self.parsed_obj["services"]:
                        if isinstance(self.parsed_obj["services"]\
                                [service], dict):
                            self._desired_data = self.parsed_obj\
                                ["services"][container]
            else:
                if isinstance(self.parsed_obj, dict) and \
                        service in self.parsed_obj:
                    if isinstance(self.parsed_obj[service], dict):
                        self._desired_data = self.parsed_obj[service]

    @property
    def data(self):
        return self._desired_data

    @property
    def services(self):
        if not isinstance(self.parsed_obj, dict):
            return dict()
        if self.is_v2:
            if "services" in self.parsed_obj and \
                    isinstance(self.parsed_obj, dict):
                return copy.copy(self.parsed_obj["services"])
            return dict()
        else:
            return copy.copy(self.parsed_obj)
        return dict()

def print_msg(text, group=None,
        service=None, container=None, color="blue"):
    """ Print out a nicely formatted message prefixed with
        [drudder] and/or possibly a service or container name.

        You can specify a signal color, with red and yellow changing the
        displayed message type from INFO to ERROR or WARNING respectively.
    """
    info_msg = "INFO"
    if color == "red":
        info_msg = "\033[1;31mERROR"
    elif color == "yellow":
        info_msg = "\033[1;33mWARNING"
    elif color == "green":
        info_msg = "SUCCESS"

    text = clear_text(text, allow_line_breaks=False)

    ENABLE_COLORS=True

    def color_code():
        part = "\033[1;"
        if color == "blue":
            part += "34"
        elif color == "red":
            part += "31"
        elif color == "yellow":
            part += "33"
        elif color == "green":
            part += "32"
        elif color == "white":
            part += "37"
        if not ENABLE_COLORS:
            return ""
        return part + "m"

    service_part = ""
    if group != None and len(group) > 0:
        if ENABLE_COLORS:
            service_part = "\033[0m" + color_code() + group
        else:
            service_part = group
        if service != None:
            service_part += "/" + service
            if container != None:
                service_part += "/" + container

    docker_services_part = ""
    if service == None or len(service) == 0:
        docker_services_part = color_code() + "drudder"

    initial_length = len("[drudder")
    if group != None:
        initial_length = len("[" + group)
        if service != None:
            initial_length += len("/" + service)
            if container != None:
                initial_length += len("/" + container)
    initial_length += len("] ")
    later_length = int(max(initial_length / 2,
        min(10, initial_length)))

    text_color = ""
    if ENABLE_COLORS:
        text_color = "\033[0m"
        if color == "yellow" or color == "red":
            text_color += color_code()

    # Split lines according to \n\n:
    lines = [line.strip() for line in text.split("\n\n")]

    # First line with complicated lead up:
    print_text = ""
    if ENABLE_COLORS:
        print_text = "\033[0m\033[1m[\033[0m" +\
            docker_services_part + \
            service_part + "\033[0m\033[1m] " + info_msg +\
            " " + text_color
    else:
        print_text = "[" + docker_services_part + service_part + "]"
        print_text += " " + info_msg + " " + text_color
    first_line = True
    for line in lines:
        if first_line:
            print_text += textwrap.fill(line, width=70,
                initial_indent=(" " * initial_length),
                subsequent_indent=(" " * later_length))[
                initial_length:]
            first_line = False
        else:
            print_text += "\n" + textwrap.fill(line, width=70,
                initial_indent=(" " * later_length),
                subsequent_indent=(" " * later_length))
    if ENABLE_COLORS:
        print_text += "\033[0m"
    repair_terminal()
    print(print_text, flush=True)

class DataVolumeMount(object):
    """ A DataVolumeMount represents a docker volume mount on the host fs
        with the known information about this mount.

        This is used by the DataVolume class below to represent the various
        places the volume is mounted on the host's disk.
    """
    def __init__(self, host_path=None,
            mount_owning_service=None, container_mount_path=None):
        self.host_path = host_path
        self.service = mount_owning_service
        self.container_mount_path = container_mount_path

    def __repr__(self):
        if self.host_path != None:
            return self.host_path
        return "<unspecified volume mount of " + str(self.service) + ">"

class DataVolume(object):
    """ This specifies the known information about a data volume. A volume
        can be either with no host directory mount and system-wide/reusable
        with a name, no host directory mount and system-wide/reusable with a
        random id, or with a host directory mount (which means it is usually
        "owned" by just one specific containers).
    """
    def __init__(self, known_host_mount=None, id=None, name=None):
        self.known_host_mount = known_host_mount
        self.id = id
        self.name = None
        self.owning_service = None
        self.container_fs_path = None
        self.specified_in_yml = False

    def __hash__(self):
        return hash(str(self.id) + "/" +
            str(self.owning_service.group_dir) + "/" +
            str(self.known_host_mount))

    def __repr__(self):
        if self.id != None:
            return str(self.id)
        if self.container_fs_path != None:
            if self.known_host_mount != None:
                return str(self.known_host_mount) + ":" +\
                    str(self.container_fs_path)
            return str(self.container_fs_path)
        if self.known_host_mount != None:
            return str(self.known_host_mount)
        return "<DataVolume object with unknown mount and unknown id>"

    def __eq__(self, other):
        if other == None:
            return False
        if not hasattr(other, "id") and not hasattr(other, "name"):
            return False
        if not hasattr(other, "known_host_mount"):
            return False
        if self.id != None and other.id == self.id:
            return True
        if self.known_host_mount != None and \
                self.known_host_mount == other.known_host_mount:
            return True
        if self.owning_container != None and \
                self.owning_container_path != None:
            if self.owning_container == other.owning_container and \
                    os.path.normpath(self.owning_container_path) ==\
                    os.path.normpath(other.owning_container_path):
                return True
        return False

    def is_covered_by_service_backups(self, service):
        if self.known_host_mount == None or \
                self.owning_service == None:
            return None

        # If path exists and is not a dir, we can't backup it:
        if os.path.exists(self.known_host_mount) and \
                not os.path.isdir(self.known_host_mount):
            return False

        # Check whether path is contained in our service group dir:
        base_path = os.path.normpath(self.owning_service.group_dir)
        if os.path.normpath(self.known_host_mount).startswith(base_path):
            return True
        return False

    def _set_owning_service(self, service, container_fs_path=None):
        """ Set the service this volume is associated with. Only for
            internal use. This is called for volumes that are specified as
            host mounts in docker-compose.yml and don't have a proper
            system-wide volume identifier/name for use in other containers.

            DEVELOPER NOTE (Jonas Thiem):
            In theory, a volume that is purely a host directory mount can be
            used by multiple services of course. However, right now
            drudder only builds up volume information per container, and
            therefore this container-centric approach is sufficient for now.
            However, later this should be changed to allow multiple owning
            containers.
        """
        self.owning_service = service
        if container_fs_path != None:
            self.container_fs_path = container_fs_path

    @property
    def mounts(self):
        """ Return the known places this DataVolume is mounted at. Returns
            a list of DataVolumeMount instances (or an empty list).
        """
        if self.known_host_mount != None:
            if self.owning_service != None:
                if self.container_fs_path != None:
                    return [ DataVolumeMount(host_path=self.known_host_mount,
                        mount_owning_service=self.owning_service,
                        container_mount_path=self.container_fs_path) ]
                else:
                    return [ DataVolumeMount(host_path=self.known_host_mount,
                        mount_owning_service=self.owning_service) ]
            else:
                return [ DataVolumeMount(host_path=self.known_host_mount) ]
        elif self.owning_container != None:
            return [ DataVolumeMount(mount_container=self.owning_container) ]
        else:
            return []

class ServiceDependency(object):
    """ This class holds the info describing the dependency to another
        service's container.
    """
    def __init__(self, other_service_name,
            other_group_name,
            other_group_dir):
        self.service_name = other_service_name
        self.group_name = other_group_name
        self.group_dir = other_group_dir

    @property
    def service(self):
        """ The actual container instance to start/stop the container which
            is the target of this dependency.

            Please note this might be unavailable if the container doesn't
            belong to any known service, in which case accessing this property
            will raise ValueError.
        """
        if self.service_name == None:
            raise ValueError("the service that provides this dependency " +\
                "isn't known")
        return Service(
            self.service_name, self.group_name,
            self.group_dir)

    def __repr__(self):
        if self.group_name != None:
            return self.group_name + "/" + self.service_name
        return self.service_name + " (unknown service group!!)"

class ServiceContainer(object):
    """ An instance of this class holds the info for a service's container.
        It can be used to e.g. obtain the system-wide docker container name,
        or the directory for the respective docker-compose.yml where
        docker-compose commands can be run.
    """
    def __init__(self, container_id, service_name, group_name, group_dir):
        self.container_id = container_id
        self.service_name = service_name
        self.group_name = group_name
        self.group_dir = group_dir

    def __repr__(self):
        return self.container_id

    def __hash__(self):
        return hash(self.container_id + "/" +
            self.service_name + "/" + self.group_name + "/" + \
            os.path.normpath(os.path.abspath(self.group_dir))
            )

    def __eq__(self, other):
        if other is None:
            return False
        if not hasattr(other, "service_name") or not hasattr(other,
                "group_dir") or not hasattr(other, "container_id"):
            return False
        if other.container_id != self.container_id:
            return False
        if other.service_name != self.service_name:
            return False
        if other.group_name != self.group_name:
            return False
        if (os.path.normpath(os.path.abspath(self.group_dir)) !=
                os.path.normpath(os.path.abspath(other.group_dir))):
            return False
        return True

    def __neq__(self, other):
        return not self.__eq__(other)

    @property
    def potentially_lost_volumes(self):
        # Find volumes that will be dangling and not re-attached by
        # docker-compose on container recreation:
        potentially_lost_volumes = set()
        for volume in self.volumes:
            if volume.name != None and volume.specified_in_yml:
                # Named volume specified in .yml -> we should fine
                continue
            host_mounted = False
            for mount in volume.mounts:
                if mount.host_path != None:
                    host_mounted = True
                    break
            if host_mounted and volume.specified_in_yml:
                # Host mount specified in .yml -> we should be fine
                continue
            potentially_lost_volumes.append(volume)
        return potentially_lost_volumes

    def start(self, force_container_recreation=False, print_mutex=None):
        if self.running:
            return

        if self.container_id == self.service.default_container_id:
            self.service.start(
                force_container_recreation=force_container_recreation,
                print_mutex=print_mutex)

        raise RuntimeError("launching non-standard containers " +
            "directly is not implemented right now")

    def stop(self):
        subprocess.check_call([SystemInfo.docker_path(),
            "stop", self.container_id],
            cwd=self.service.group.group_dir)

    @property
    def running(self):
        ids = self.service.group._get_running_service_container_ids()
        return (self.container_id in ids)

    @property
    def service(self):
        """ The service this container belongs to. """
        return Service(self.service_name, self.group_name,
            self.group_dir)

    def _get_active_volume_directories(self, rw_only=False):
        """ Get the volumes active in this container as a tuple list.
            If the container hasn't been started before, this might raise
            a ValueError since this information is obtained via docker
            inspection of the container.
        """
        try:
            output = execution_cache.check_output([SystemInfo.docker_path(),
                "inspect", self.container_id],
                stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            raise ValueError("container not created - you might need to "+\
                "launch it first")
        result = json.loads(output.decode("utf-8", "ignore"))
        volumes_list = []
        if not rw_only:
            if "Volumes" in result[0]:
                if result[0]["Volumes"] != None:
                    for volume in result[0]["Volumes"]:
                        volumes_list.append(volume)
            else:
                if result[0]["Config"]["Volumes"] != None:
                    for volume in result[0]["Config"]["Volumes"]:
                        volumes_list.append(volume)
        if rw_only:
            if "Volumes" in result[0]:
                if not ("VolumesRW" in result[0]):
                    return []
                for volume in result[0]["Volumes"]:
                    if not result[0]["VolumesRW"][volume]:
                        volumes_list.remove(volume)
            else:
                if not ("VolumesRW" in result[0]["Config"]):
                    return []
                for volume in result[0]["Config"]["Volumes"]:
                    if not result[0]["Config"]["VolumesRW"][volume]:
                        volumes_list.remove(volume)
        return volumes_list

    def _map_host_dir_to_container_volume_dir(self, volume_dir):
        """ Attempts to find the volume mount information and return the host
            directory currently mapped to the given container volume path.
        """
        try:
            output = SubprocessHelper.check_output_with_isolated_pty(
                [SystemInfo.docker_path(),
                "inspect",
                self.container_id],
                stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            raise ValueError("container not created - you might need to "+\
                "launch it first")
        result = json.loads(output.decode("utf-8", "ignore"))
        for mount in result[0]["Mounts"]:
            if os.path.normpath(mount["Destination"]) == \
                    os.path.normpath(volume_dir):
                return mount["Source"]
        return None

    def _active_volumes(self, rw_only=False):
        """ Get all the active volumes of this container which are actually
            created right now and in use. May return an empty or outdated
            list if the container is stopped.

            Returns a list of tuples (host file system path,
                container filesystem path).
        """

        try:
            return [(self._map_host_dir_to_container_volume_dir(
                volume), volume) \
                for volume in \
                self._get_active_volume_directories(rw_only=rw_only)]
        except ValueError:
            return []

    def _get_volumes(self, rw_only=False):
        volumes = []
        def add_volume(vol_line, from_yml=False):
            host_mount_path = None
            known_name = None
            if vol_line[0] != None:
                if vol_line[0].startswith("/") or \
                        vol_line[0].startswith("./") or \
                        vol_line[0].startswith("../"):
                    host_mount_path = vol_line[0]
                else:
                    known_name = vol_line[0]
            vol = DataVolume(known_host_mount=host_mount_path,
                id=known_name, name=known_name)
            vol._set_owning_service(self.service, vol_line[1])
            vol.specified_in_yml = from_yml
            volumes.append(vol)

        volumes_in_yml_by_normpath = set()
        vol_lines = {}

        # Collect all volumes actually used by the container right now:
        for vol1 in self._active_volumes(rw_only=rw_only):
            if not vol1[0] in vol_lines or vol_lines[vol1[0]] == None:
                vol_lines[os.path.normpath(vol1[0])] = vol1[1]

        # Collect all volumes this container should have according to the
        # docker-compose.yml listing:
        for vol2 in self.service._config_specified_volumes(rw_only=rw_only):
            if not vol2[0] in vol_lines or vol_lines[vol2[0]] == None:
                vol_lines[os.path.normpath(vol2[0])] = vol2[1]
            volumes_in_yml_by_normpath.add(os.path.normpath(vol2[0]))

        # Go through all volumes and construct DataVolume instances:
        for vol_line_part0 in vol_lines:
            if os.path.normpath(vol_line_part0) in\
                    volumes_in_yml_by_normpath:
                add_volume((vol_line_part0, vol_lines[vol_line_part0]),
                    from_yml=True)
            else:
                add_volume((vol_line_part0, vol_lines[vol_line_part0]))
        return volumes

    @property
    def volumes(self):
        """ Returns a list of DataVolume instances representing all volumes
            which are in any sort of known connection to this container.
        """
        return self._get_volumes(rw_only=False)

    @property
    def rw_volumes(self):
        """ Returns a list of DataVolume instances representing all volumes
            which are in any sort of known connection to this container.
            Limited to all the volumes that are actually writable for the
            container, excluding the read-only ones.
        """
        return self._get_volumes(rw_only=True)

class Service(object):
    def __init__(self, service_name, group_name, group_dir):
        self.name = service_name
        self.group_name = group_name
        self.group_dir = group_dir

    @staticmethod
    def all():
        result = set()
        for group in ServiceGroup.all():
            for service in group.services:
                result.add(service)
        return result

    @property
    def potentially_lost_volumes(self):
        result = set()
        for containers in self.containers:
            result.union(containers.potentially_lost_volumes)
        return result

    @property
    def group(self):
        return ServiceGroup(self.group_name, self.group_dir)

    def __repr__(self):
        return self.name + " service (part of " + str(self.group) + ")"

    def stop(self):
        for container in self.running_containers:
            container.stop()

    def start(self, force_container_recreation=False, print_mutex=None):
        if self.running:
            return

        if print_mutex == None:
            print_mutex = threading.Lock()

        # Warn about volumes that might be dangling after recreation
        potentially_lost_volumes = self.potentially_lost_volumes
        if len(potentially_lost_volumes) == 0 or \
                force_container_recreation:
            if len(potentially_lost_volumes) > 0:
                print_mutex.acquire()
                print_msg("some volumes might be potentially dangling, " +\
                    "but container recreation was forced by user",
                    service=self.service.name, container=self.name,
                    color="yellow")
                print_mutex.release()
            try:
                output = SubprocessHelper.check_output_with_isolated_pty([
                    SystemInfo.docker_compose_path(),
                    "rm", "-f", self.name],
                    cwd=self.group.group_dir,
                    stderr=subprocess.DEVNULL)
            except subprocess.CalledProcessError as e:
                output = e.output
            try:
                output = output.decode("utf-8", "replace")
            except AttributeError:
                pass
            #print_mutex.acquire()
            #print(clear_text(output))
            #print_mutex.release()
        else:
            print_mutex.acquire()
            for vol in potentially_lost_volumes:
                print_msg("ONLY LIMITED RESTART, CONTAINER WONT BE "+\
                    "UPDATED TO NEWEST IMAGE. " +\
                    "Cannot recreate container safely: " +\
                    "container has a volume that might be lost " +\
                    "or dangling after recreation: "+\
                    "" + str(vol),
                    service=self.service.name,
                    container=self.name, color="yellow")
            print_mutex.release()
        output = ""
        try:
            output = SubprocessHelper.check_output_with_isolated_pty([
                SystemInfo.docker_compose_path(),
                "up", "-d", self.name],
                cwd=self.group.group_dir, stderr=subprocess.STDOUT)
            pass
        except subprocess.CalledProcessError as e:
            output = e.output
        try:
            output = output.decode("utf-8", "replace")
        except AttributeError:
            pass
        print_mutex.acquire()
        repair_terminal()
        print(clear_text(output), flush=True)
        print_mutex.release()

    def __eq__(self, other):
        if not hasattr(other, "name") or not hasattr(other,
                "group_name") or not hasattr(other, "group_dir"):
            return False
        if hasattr(other, "service_name"):  # ServiceContainer, not Service
            return False
        if self.name == other.name and self.group_name == \
                other.group_name:
            if os.path.normpath(os.path.abspath(
                    os.path.realpath(self.group_dir))) == \
                    os.path.normpath(os.path.abspath(
                    os.path.realpath(other.group_dir))):
                return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name + "/" +
            self.group_name + "/" + os.path.normpath(
            os.path.abspath(os.path.realpath(self.group_dir))))

    @property
    def default_container_id(self):
        fpath = os.path.normpath(os.path.abspath(self.group_dir))
        return (os.path.basename(fpath).replace("-", "").replace(".", "").\
            lower()
            + "_" + str(self.name) + "_1")

    @property
    def running(self):
        return len(self.group._get_running_service_container_names(
            self)) > 0

    @property
    def containers(self):
        container_instance_ids = set()

        # Collect actual existing container instances:
        container_ps_list = self.group._get_ps_service_container_ids(
            only_running=False)
        for cid in container_ps_list:
            if self.group._container_id_belongs_to_service(cid, self): 
                container_instance_ids.add(cid)

        # Add hypothetical default container:
        container_ids = container_instance_ids.union(set([
            self.default_container_id]))

        # Convert to proper ServiceContainer classes:
        result = set()
        for cid in container_ids:
            container = ServiceContainer(cid,
                self.name, self.group_name, self.group_dir)
            result.add(container)
        return result

    @property
    def running_containers(self):
        """ Get only the containers of this service which are currently up and
            running.
        """
        running_names = self.group._get_running_service_container_names(
            self)
        running_containers = []
        for container in self.containers:
            if container.container_id in running_names:
                running_containers.append(container)
        return running_containers

    @property
    def volumes(self):
        volume_set = set()
        for container in self.containers:
            for volume in container.volumes:
                volume_set.add(volume)
        return list(volume_set)

    @property
    def rw_volumes(self):
        volume_set = set()
        for container in self.containers:
            for volume in container.rw_volumes:
                volume_set.add(volume)
        return list(volume_set)

    @property
    def image_name(self):
        # Assemble results:
        yml = DockerComposeYml(self.service_path)
        for service_name in yml.services:
            if not isinstance(yml.services[service_name], dict):
                continue
            for property_name in yml.services[service_name]:
                value = yml.services[service_name][property_name]
                if property_name == "image":
                    # This container is constructed from an image:
                    return value
                elif property_name == "build":
                    # Built from a directory with Dockerfile:
                    return None
        raise RuntimeError("no entry found for service " + str(self.name) +\
            " in group " + str(self.group))

    def _config_specified_volumes(self, rw_only=False):
        """ Parse and return all volumes specified for this service in the
            docker-compose.yml.

            Returns a list of tuples (host file system path,
                container filesystem path).
        """
        volumes = []
        def parse_volume_line(parts):
            """ Helper function to parse a single volume line """

            # Check if read-only or not:
            rwro = "rw"
            if len(parts) >= 3:
                if "ro" in [item.strip() for item in parts[2].split(",")]:
                    rwro = "ro"

            # Make sure path is absolute:
            if len(parts) >= 2:
                if (parts[0].startswith("/") or parts[0].startswith("./") or
                        parts[0].startswith("../")) \
                        and not os.path.isabs(parts[0]):
                    parts[0] = os.path.join(
                        os.path.realpath(self.group_dir), parts[0])
                    parts[0] = os.path.normpath(os.path.abspath(parts[0]))

            # Add to list:
            if len(parts) >= 2 and (rwro == "rw" or not rw_only):
                volumes.append((parts[0], parts[1]))

        # Extract volume lines:
        yml = DockerComposeYml(self.group_dir, self.name)
        if "volumes" in yml.data:
            for line in yml.data["volumes"]:
                parse_volume_line(line.split(":"))
        return volumes

    @property
    def dependencies(self):
        # Collect all external_links and links container references:
        external_links = self.group._external_docker_compose_links(
            self.name)
        internal_links = self.group._internal_docker_compose_links(
            self.name)
        dependencies = []

        # Go through all external links
        for link in external_links:
            link_name = link.partition(":")[0]
            target_found = False
            # See if we can find the target service:
            for service_group in ServiceGroup.all():
                for service in service_group.services:
                    if service.default_container_id == link_name:
                        target_found = True
                        dependencies.append(ServiceDependency(
                            service.name,
                            service.group.name,
                            service.group.group_dir))
            if not target_found:
                dependencies.append(ServiceDependency(
                    link_name, None, None))

        # Go through all internal links:
        for link in internal_links:
            link_name = link.partition(":")[0]
            target_found = False
            # See if we can find the target service:
            for service in self.group.services:
                 if link_name == service.name:
                    target_found = True
                    dependencies.append(ServiceDependency(
                        link_name, self.group_name,
                        self.group_dir))
            if not target_found:
                dependencies.append(ServiceDependency(
                    link_name, None, None))
        return dependencies

class ServiceGroup(object):
    """ This class holds all the info and helper functionality for managing
        a service group, whereas a service group is a folder which
        1.) contains a docker-compose.yml file which describes one or more
            services contained in that group, and
        2.) is located inside a global service group directory (usually /srv/)
    """
    def __init__(self, group_name, group_dir):
        self.name = group_name
        self.group_dir = group_dir

    def __repr__(self):
        return self.name + " service group at " + str(self.group_dir)

    def __eq__(self, other):
        if not hasattr(other, "name") or not hasattr(other,
                "group_dir"):
            return False
        if hasattr(other, "group_name"):  # a Service, not a ServiceGroup
            return False
        if self.name == other.name:
            if os.path.normpath(os.path.abspath(
                    os.path.realpath(self.group_dir))) == \
                    os.path.normpath(os.path.abspath(
                    os.path.realpath(other.group_dir))):
                return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name + "/" + os.path.normpath(
            os.path.abspath(os.path.realpath(self.group_dir))))

    @staticmethod
    def find_by_name(name):
        results = []
        for service_group in ServiceGroup.all():
            if service_group.name == name:
                results.append(service_group)
        return results

    @staticmethod
    def all_services():
        result = set()
        for group in ServiceGroup.all():
            for service in group.services:
                result.add(service)
        return list(result)

    @staticmethod
    def all():
        """ Get a global list of all detected services.
        """
        services = []
        scanned = set()
        def scan_dir(d):
            """ Scan a given directory for containers. """

            if not os.path.exists(d):
                return

            # Only scan directories:
            if not os.path.isdir(d):
                return

            # Duplicate avoidance:
            normd = os.path.normpath(os.path.abspath(
                os.path.realpath(d)))
            if normd in scanned:
                return
            scanned.add(normd)

            # Scan it:
            d = os.path.abspath(d)
            for f in os.listdir(d):
                if not os.path.isdir(os.path.join(d, f)):
                    continue
                if not os.path.exists(os.path.join((os.path.join(d, f)),
                        "docker-compose.yml")):
                    continue
                services.append(ServiceGroup(f, 
                    os.path.normpath(os.path.join(d, f))))

        # Schedule some common places for scanning:
        base_folders = [os.getcwd(), "/usr/share/docker-services", "/srv"]
        base_folders = [os.path.normpath(
            os.path.abspath(f).replace("//", "/")) for f in base_folders]
        user_folders = gc.get("service-locations", value_type=list)
        user_folders = [os.path.normpath(
            os.path.abspath(f).replace("//", "/")) for f in user_folders]
        user_folders = [f for f in user_folders if not f in base_folders]
        for f in base_folders:
            scan_dir(f)
        for f in user_folders:
            scan_dir(f)
        return services

    def _fix_container_id(self, container_id):
        """ !! BIG HACK !!
            Sometimes docker-compose gives us just a shortened name for a
            container. While I am not fully aware of the algorithm, I assume
            it will usually still be unique (it doesn't need to be but
            in almost all cases it still is). In this function, we try to get
            back the full unshortened name.
        """
        assert(container_id != None)
        # Check if a standard container name for a service matches fully:
        for service in self.services:
            if service.default_container_id == container_id:
                return container_id

        # Check if this matches a common name pattern:
        for service in self.services:
            def is_int(v):
                try:
                    dat_int = int(v)
                except (TypeError, ValueError):
                    return False
                return True
            default_id = service.default_container_id
            assert(default_id.endswith("_1"))
            id_base = default_id[:-len("1")]
            if container_id.startswith(id_base + "run_"):
                ending = container_id[len(id_base + "run_"):]
                if is_int(ending):
                    return container_id
            elif container_id.startswith (id_base):
                ending = container_id[len(id_base):]
                if is_int(ending):
                    return container_id

        # Check for whether a default container name starts with this name:
        matched_id = None
        for service in self.services:
            if service.default_container_id.startswith(
                        container_id) or \
                    (service.default_container_id[:-len("1")] +\
                        "_run_").startswith(
                        container_id):
                if matched_id != None:
                    return None  # not unique.
                matched_id = container.container_id
        return matched_id

    def _get_running_service_container_ids(self):
        return self._get_ps_service_container_ids(only_running=True)

    def _get_ps_service_container_ids(self, only_running=False):
        """ Get all running containers of the given service.
            Returns a list of container ids.
        """
        running_containers = []
        try:
            env = os.environ.copy()
            env["COLUMNS"] = "200"
            output = execution_cache.check_output_with_isolated_pty([
                SystemInfo.docker_compose_path(), "ps"],
                cwd=self.group_dir, stderr=subprocess.STDOUT,
                timeout=10, env=env).\
                decode("utf-8", "ignore")
        except subprocess.CalledProcessError as e:
            output = e.output.decode("utf-8", "ignore")
            if output.find("client and server don't have same version") >= 0:
                print_msg("error: it appears docker-compose is " +\
                    "installed with " +\
                    "a version incompatible to docker.", color="red")
                sys.exit(1)
            if output.find("ERROR") > 0 and output.find("ERROR") <= 10 and\
                    output.find("In file") > output.find("ERROR") and \
                    output.find("In file ") <= 20 and \
                    output.find("docker-compose.yml") >= 0:
                print_msg("warning: it appears the docker-compose file " +\
                    "has a syntax error: " + str(output), color="yellow",
                    service=self.name)
                return []
            raise e
        output = output.\
            replace("\r", "\n").replace("\n\n", "\n").split("\n")

        skipped_past_dashes = False
        for output_line in output:
            # Skip irrelevant lines:
            if len(output_line.strip()) == 0:
                continue
            if output_line.startswith("---------") \
                    or output_line.startswith(" ") \
                    or output_line.startswith("\t"):
                skipped_past_dashes = True
                continue

            # Extract containers:
            output_line = output_line.strip()
            if len(output_line) > 0 and (output_line.find(" Up") > 0 or \
                    not only_running):
                space_pos = output_line.find(" ")
                running_containers.append(output_line[:space_pos])
        l = [self._fix_container_id(container) \
            for container in running_containers]
        return [name for name in l if name is not None]

    def _container_id_belongs_to_service(self, cid, service):
        def is_int(i):
            try:
                int_i = int(i)
            except (TypeError, ValueError):
                return False
            return True
        def_base = service.default_container_id[:-len("1")]
        if cid.startswith(def_base):
            remains = cid[len(def_base):]
            if remains.startswith("run_"):
                remains = remains[len("run_"):]
            if is_int(remains):
                return True
        return False

    def _get_running_service_container_names(self, service):
        result = set()

        # Get all running containers in this service group:
        running_ids = set(self._get_running_service_container_ids())

        # Find out which ones belong to the queried service:
        for running_id in running_ids:
            if self._container_id_belongs_to_service(
                    running_id, service):
                result.add(running_id)

        return list(result)

    def _internal_docker_compose_links(self, service_name):
        """ Attempt to parse and return all services referenced as internal
            links used by a service from the respective docker-compose.yml.
        """
        links = []

        # Get docker-compose.yml data:
        yml = DockerComposeYml(self.group_dir, service_name)
        if yml.data == None:
            raise ValueError("no such service found: " +\
                str(container_name))

        # Extract link data:
        if not "links" in yml.data:
            return []
        return [entry.partition(":")[0] for entry in \
            yml.data["links"]]

    def _external_docker_compose_links(self, service_name):
        """ Attempt to parse and return all containers referenced as external
            links used by a service from the respective docker-compose.yml.
        """
        external_links = []
        yml = DockerComposeYml(self.group_dir, service_name)
        if yml.data == None:
            raise ValueError("no such service found: " +\
                str(container_name))
        if not "external_links" in yml.data:
            return []
        return [entry.partition(":")[0] for entry in \
            yml.data["external_links"]]

    @staticmethod
    def global_clean_up(ask=True):
        """ This function will check the status of all docker containers, and
            then irrevocably delete all containers that aren't running.
            It will also delete all dangling volumes.
        """
        if ask:
            answer = input("\033[1m!! DANGER !!\033[0m\n"+\
                "This will irrevocably delete all "+\
                "stopped containers. "+\
                "It will also delete all volumes that are neither "+\
                "a host-mounted directory, nor are currently owned by any "+\
                "currently existing container (in short, all dangling "+\
                "volumes). It will also wipe all container logs." +\
                "\n\n(Avoid this warning next time " +\
                "with --force)\n\n" +\
                "Are you sure you want to continue? [Enter y/N]")
            if not answer == "y" and not answer == "Y":
                print("drudder: error: cleaning was aborted "+\
                    "by user.", file=sys.stderr, flush=True)
                sys.exit(1)
        print_msg("cleaning up stopped containers...")
        output = SubprocessHelper.check_output_with_isolated_pty([
            SystemInfo.docker_path(), "ps", "-a"])
        output = output.decode("utf-8", "ignore").\
            replace("\r", "\n").\
            replace("\n ", "\n").replace(" \n", "\n").replace("\n\n", "\n")
        while output.find("   ") >= 0:
            output = output.replace("   ", "  ")
        output = output.replace("  ", "\t")
        output = output.replace("\t ", "\t").replace(" \t", "\t")
        output = output.split("\n")
        for output_line in output:
            if len(output_line.strip()) == 0:
                continue
            parts = output_line.split("\t")
            if parts[0] == "CONTAINER ID":
                continue
            if len(parts) < 5 or (not parts[3].endswith("ago")):
                print_msg("WARNING: skipping container " + parts[0] +\
                    ", cannot locate STATUS column")
                continue
            if parts[0].find(" ") >= 0:
                print_msg("WARNING: skipping container with invalid " +\
                    "container id: " + parts[0])
                continue
            if len(parts) == 6 and parts[4].find("_") >= 0:
                parts = parts[:4] + [ '' ] + parts[4:]
            if parts[4] == "" or parts[4].startswith("Exited "):
                print_msg("deleting stopped container " + parts[0] + "...")
                try:
                    SubprocessHelper.check_output_with_isolated_pty([
                        SystemInfo.docker_path(),
                        "rm", parts[0]])
                except subprocess.CalledProcessError:
                    print_msg("warning: failed to delete container " +
                        parts[0], color="yellow")
        print_msg("cleaning up unneeded images...")
        images = SubprocessHelper.check_output_with_isolated_pty([
            SystemInfo.docker_path(), "images", "-aq"])
        try:
            images = images.decode("utf-8")
        except AttributeError:
            pass
        images = [image.strip() for image in images.split("\n") \
            if len(image.strip()) > 0]
        for image in images:
            try:
                SubprocessHelper.check_output_with_isolated_pty([
                    SystemInfo.docker_path(), "rmi",
                    image])
                print_msg("removed image " + str(image), color="blue")
            except subprocess.CalledProcessError:
                pass
        print_msg("cleaning up dangling volumes...")
        dangling_vols = SubprocessHelper.check_output_with_isolated_pty([
            SystemInfo.docker_path(),
            "volume", "ls", "-qf", "dangling=true"])
        for vol in dangling_vols.splitlines():
            vol = vol.strip()
            if len(vol) == 0:
                continue
            SubprocessHelper.check_output_with_isolated_pty([
                SystemInfo.docker_path(),
                "volume", "rm", vol])
        print_msg("cleaning up container logs...")
        for f in os.listdir("/var/lib/docker/"):
            for fname in os.path.join("/var/lib/docker/", f):
                if fname.endswith("-json.log"):
                    subprocess.check_output([
                        "truncate", "-s", "0",
                        os.path.join("/var/lib/docker/", f, fname)])

    @property
    def services(self):
        """ Get all services that are part of this services group as
            specified in the group's docker-compose.yml.
        """
        results = set()

        # Assemble results:
        yml = DockerComposeYml(self.group_dir)
        for service_name in yml.services:
            results.add(Service(service_name,
                self.name, self.group_dir))
        return results

class FailedLaunchTracker(object):
    def __init__(self):
        self.access_lock = threading.Lock()
        self.contents = set()

    def __len__(self):
        self.access_lock.acquire()
        result = len(self.contents)
        self.access_lock.release()
        return result

    def __contains__(self, item):
        self.access_lock.acquire()
        result = (item in self.contents)
        self.access_lock.release()
        return result

    def add(self, item):
        self.access_lock.acquire()
        self.contents.add(item)
        self.access_lock.release()

class LaunchThreaded(threading.Thread):
    """ A helper to launch a service and wait for the launch only for a
        limited amount of time, and moving the launch into a background
        thread if it takes too long.
    """
    
    def __init__(self, service, failed_launch_tracker=None,
            force_container_recreation=False, do_on_success=None,
            do_on_failure=None, print_mutex=None):
        super().__init__()
        if print_mutex == None:
            print_mutex = threading.Lock()
        self.print_mutex = print_mutex
        self.service = service
        self.failed_launch_tracker = failed_launch_tracker
        self.path = self.service.group.group_dir
        self.force_container_recreation = force_container_recreation
        def do_nothing(self):
            pass
        self.do_on_success = do_on_success
        if self.do_on_success is None:
            self.do_on_success = do_nothing
        self.do_on_failure = do_on_failure
        if self.do_on_failure is None:
            self.do_on_failure = do_nothing

    def run(self):
        try:
            # Fix permissions if we have instructions for that:
            perms = Permissions(self.service.group)
            perm_info = perms.get_permission_info_from_yml()
            if ("owner" in perm_info["livedata-permissions"]) \
                    and os.path.exists(os.path.join(
                        self.path, "livedata")):
                owner = perm_info["livedata-permissions"]["owner"]
                try:
                    owner = int(owner)
                except TypeError:
                    # Must be a username.
                    try:
                        owner = getpwnam(owner).pw_uid
                    except KeyError:
                        self.print_mutex.acquire()
                        print_msg("invalid user specified for permissions: "+\
                            "can't get uid for user: " + owner, color="red")
                        self.print_mutex.release()
                        raise RuntimeError("invalid user")
                for root, dirs, files in os.walk(os.path.join(self.path, \
                        "livedata")):
                    for f in (dirs + files):
                        fpath = os.path.join(root, f)
                        os.chown(fpath, owner, -1, follow_symlinks=False)

            # Get dependencies and see if they have all been launched:
            waiting_msg = False
            for dependency in self.service.dependencies:
                if not dependency.service.running:
                    if not waiting_msg:
                        waiting_msg = True
                    time.sleep(5)
                    while not dependency.service.running:
                        if self.failed_launch_tracker != None:
                            if dependency.service in \
                                    self.failed_launch_tracker:
                                self.print_mutex.acquire()
                                print_msg("launch aborted due to failed " +\
                                    "dependency launch: " +\
                                    str(dependency),
                                    service=self.service.name,
                                    group=self.service.group.name,
                                    color="red")
                                self.print_mutex.release()
                                self.failed_launch_tracker.add(
                                    self.service)
                                self.do_on_failure()
                                return
                        time.sleep(5)

            # Launch the service:
            self.print_mutex.acquire()
            print_msg("launching...", service=self.service.name,
                group=self.service.group.name, color="blue")
            self.print_mutex.release()
            try:
                self.service.start(
                    force_container_recreation=\
                    self.force_container_recreation,
                    print_mutex=self.print_mutex)
                time.sleep(1)
                if not self.service.running:
                    self.print_mutex.acquire()
                    print_msg("failed to launch. (nothing running after " +\
                        "1 second)",\
                        service=self.service.name,
                        group=self.service.group.name, color="red")
                    self.print_mutex.release()
                    if self.failed_launch_tracker != None:
                        self.failed_launch_tracker.add(self.service)
                    self.do_on_failure()
                    return
                self.print_mutex.acquire()
                print_msg("now running.",
                    service=self.service.name,
                    group=self.service.group.name, color="green")
                self.print_mutex.release()
                self.do_on_success()
            except subprocess.CalledProcessError:
                self.print_mutex.acquire()
                print_msg("failed to launch. (error exit code)",\
                    service=self.service.name,
                    group=self.service.group.name,
                    color="red")
                self.print_mutex.release()
                if self.failed_launch_tracker != None:
                    self.failed_launch_tracker.add(self.service)
                self.do_on_failure()
            except Exception as e:
                self.print_mutex.acquire()
                print_msg("failed to launch. (unknown error)",\
                    service=self.service.name,
                    group=self.service.group.name,
                    color="red")
                self.print_mutex.release()
                if self.failed_launch_tracker != None:
                    self.failed_launch_tracker.add(self.service)
                self.do_on_failure()
                raise e
        except Exception as e:
            print("UNEXPECTED ERROR", file=sys.stderr)
            print("ERROR: " + str(e), flush=True)
            traceback.print_exc()

    @staticmethod
    def attempt_launch(service, to_background_timeout=5,
            failed_launch_tracker=None, force_container_recreation=False,
            do_on_success=None, do_on_failure=None, print_mutex=None):
        """ Launch a given service and wait for it to run for a few seconds.
            If that isn't long enough for it to start running, return
            execution to possibly launch further services while this one is
            still busy launching.
        """

        # Start a new launch thread:
        launch_t = LaunchThreaded(service,
            failed_launch_tracker=failed_launch_tracker,
            do_on_success=do_on_success, do_on_failure=do_on_failure,
            force_container_recreation=force_container_recreation,
            print_mutex=print_mutex)
        launch_t.start()
        
        # Wait for it to complete:
        launch_t.join(to_background_timeout)
        if launch_t.isAlive():
            # This took too long, run in background:
            return launch_t
        return None

    @staticmethod
    def stop(service):
        """ Stop a service. """
        assert(service != None)
        service.stop()

    @staticmethod
    def wait_for_launches(threads):
        for launch_t in threads:
            if launch_t.isAlive():
                launch_t.join()

class Permissions(object):
    """ This represents permissions information parsed from a permssion.yml.
        This allows the administrator to specify permissions that should be
        applied to the live data folder (where usually read-write volumes are
        mounted).
    """
    def __init__(self, service):
        self.service = service

    def get_permission_info_from_yml(self):
        """ Get permission info for the given service
        """
        f = None
        try:
            f = open(os.path.join(self.service.service_path,
                "permissions.yml"), "rb")
        except Exception as e:
            return {"livedata-permissions" : {}}
        perm_dict = dict()
        current_area = None
        try:
            contents = f.read().decode("utf-8").\
                replace("\r\n", "\n").\
                replace("\r", "\n").replace("\t", " ")
        finally:
            f.close()

        # Extract permission info from the YAML:
        parsed_obj = yaml.safe_load(contents)
        for k in parsed_obj:
            if k in set(["livedata-permissions"]):
                if isinstance(parsed_obj[k], dict):
                    perm_dict[k] = parsed_obj[k]
            else:
                print_msg("warning: unrecognized permissions.yml " +\
                    "section: " + str(k),
                    service=self.service.name,
                    color="red")

        # Make sure some stuff is present:
        if not "livedata-permissions" in perm_dict:
            perm_dict["livedata-permissions"] = dict()

        return perm_dict

ensure_docker = SystemInfo.docker_path()
ensure_docker_compose = SystemInfo.docker_compose_path()

class Snapshots(object):
    def __init__(self, service_group):
        self.group = service_group

    def check_running_snapshot_transaction(self):
        if os.path.exists(os.path.join(
                self.group.group_dir, ".drudder-snapshot.lock")):
            output = subprocess.check_output(
                "ps aux | grep drudder | grep -v grep | wc -l",
                shell=True).decode("utf-8", "ignore")
            if output.strip() != "1":
                # Another copy still running??
                return True
            print_msg("warning: stale snapshot lock found but no process " +\
                "appears to be left around, removing.",
                color="yellow", group=self.group.name)
            # No longer running, remove file:
            os.remove(os.path.join(self.group.group_dir,
                ".drudder-snapshot.lock"))
        return False

    @staticmethod
    def btrfs_tool_check():
        # Make sure the btrfs tool is working:
        if SystemInfo.btrfs_path() == None:
            print_msg("error: btrfs tool not found. " +\
                "Are btrfs-progs installed?",
                color="red")
            sys.exit(1)
        output = None
        try:
            output = subprocess.check_output([
                SystemInfo.btrfs_path(),
                "--version"],
                stderr=subprocess.STDOUT).decode("utf-8", "ignore")
        except subprocess.CalledProcessError as e:
            output = e.output.decode("utf-8", "ignore")
        if not output.lower().startswith("btrfs-progrs ") and \
                not output.lower().startswith("btrfs-progs ") and \
                not output.lower().startswith("btrfs "):
            print_msg("error: btrfs tool returned unexpected string. Are " +\
                "btrfs-progrs installed and working?",
                color="red")
            print_msg("Full btrfs tool output was: " + str(output))
            sys.exit(1)

    def subvolume_readiness_check(self, print_errors=True):
        """ Check if the given service is ready for snapshotting or still
            needs btrfs subvolume conversion. Print a warning if not.
        """
        fs = SystemInfo.filesystem_type_at_path(
            self.group.group_dir)
        if fs != "btrfs":
            return False

        self.btrfs_tool_check()

        for service in self.group.services:
            try:
                for volume in service.rw_volumes:
                    if not volume.is_covered_by_service_backups(
                            service):
                        if volume.known_host_mount != None and \
                                os.path.exists(volume.known_host_mount) and \
                                not os.path.isdir(volume.known_host_mount):
                            print_msg(
                                "the volume " + str(volume) +
                                " of this service won't get snapshotted, " +
                                "because it isn't a directory",
                                service=service.name,
                                group=self.group.name,
                                color="yellow")
                        else:
                            print_msg(
                                "the volume " + str(volume) +
                                " of this service won't get snapshotted, " +
                                "because it is not mounted " +
                                "outside of the service " +
                                "group folder (" + self.group.group_dir + ")",
                                service=service.name,
                                group=self.group.name,
                                color="yellow")
                    elif volume.known_host_mount == None or \
                            not SystemInfo.is_btrfs_subvolume(
                            volume.known_host_mount):
                        if service.running:
                            if print_errors:
                                print_msg(
                                    "the volume " + str(volume) + " " +
                                    "still needs to be converted to " +
                                    "a subvolume to enable snapshots.\n" +
                                    "Fix it by doing this:\n" +
                                    "1. Stop the service with: drudder "+
                                        "stop " +
                                        self.group.name + "\n" +
                                    "2. Snapshot the service once with: " +
                                        "drudder "+
                                        "snapshot " + self.group.name + "\n",
                                    service=service.name,
                                    group=self.group.name,
                                    color="yellow")
                        else:
                            if print_errors:
                                print_msg("the volume " + str(volume) + " " +
                                    "of this service still " +
                                    "needs conversion to a btrfs " +
                                    "subvolume.\n" +
                                    "Fix it by snapshotting it once with: " +
                                    "drudder "+
                                    "snapshot " + self.group.name + "\n",
                                    service=service.name,
                                    group=self.group.name,
                                    color="yellow")
                        return False
            except Exception as e:
                print_msg("there was a problem. btrfs snapshotting won't " +\
                    "work as intended. Error details: " + str(e) + "\n" +
                    traceback.format_tb(e.__traceback__),
                    service=service.name,
                    group=self.group.name,
                    color="red")
                return False
        # Everything seems fine so far.
        return True

    def do(self):
        """ Make a backup of the live data of the service.
        """

        self.btrfs_tool_check()

        # Make sure no snapshot is already in progress:
        if self.check_running_snapshot_transaction():
            print_msg("error: snapshot already in progress. " +\
                "try again later", service=self.service.name,
                color="red")
            print_msg("remove .drudder-snapshot.lock if you " +\
                "are sure that is incorrect", service=self.service.name)
            return False

        print_msg("considering for snapshot...",
            service=self.service.name, color="blue")

        # Check which volumes this service has:
        rw_volumes = set()
        for container in self.service.containers:
            for volume in container.rw_volumes:
                rw_volumes.add(volume)
        rw_volumes = list(rw_volumes)
        if len(rw_volumes) == 0:
            print_msg("service has no read-write volumes, nothing to do.",
                service=self.service.name, color="blue")
            return True
        
        # Check if we have livedata/:
        if not os.path.exists(os.path.join(
                self.service.service_path, "livedata")):
            print_msg("error: service has read-write volumes, " + \
                "but no livedata/ " +\
                "folder. Fix this to enable snapshots",
                service=self.service.name,
                color="red")
            return False

        # Check if we have any volumes which are actually in livedata/:
        empty_snapshot = True
        for volume in rw_volumes:
            for mount in volume.mounts:
                if mount.container == None:
                    continue
                if mount.container.service != self.service:
                    continue
                if mount.host_path == None:
                    # This volume mount is not a host mount!!
                    print_msg("warning: volume " + str(volume) + \
                        " used by " + str(mount.container) +\
                        " is NOT a host mount in livedata/" +\
                        " and won't be covered by the snapshot",
                        service=self.service.name, color="yellow")
                relpath = os.path.relpath(
                    os.path.realpath(mount.host_path),
                    os.path.realpath(os.path.join(
                    self.service.service_path, "livedata")),
                )
                if relpath.startswith(os.pardir + os.sep):
                    # This volume mount is not in livedata/!
                    print_msg("warning: volume " + str(volume) + \
                        " used by " + str(mount.container) +\
                        " is a host mount that is NOT mounted in" +\
                        " the livedata/ folder" +\
                        " and won't be covered by the snapshot: " +\
                        str(mount),
                        service=self.service.name, color="yellow")
                else:
                    empty_snapshot = False
        if empty_snapshot:
            print_msg("this snapshot would be empty because no read-write " +\
                "volumes are mounted to livedata/ - skipping.",
                service=self.service.name, color="blue")
            return True

        # Check if filesystem of livedata/ is actually btrfs:
        fs = SystemInfo.filesystem_type_at_path(
            os.path.join(self.service.service_path, "livedata"))
        if fs != "btrfs":
            print_msg("error: livedata/ has other filesystem " + str(fs) + \
                ", should be btrfs!")
            return fs

        actual_path = os.path.realpath(self.service.service_path)
        livedata_renamed_dir = os.path.join(
            actual_path, ".livedata-predeletion-renamed")
        livedata_dir = os.path.join(
            actual_path, "livedata")
        snapshot_dir = os.path.join(
            actual_path, ".btrfs-livedata-snapshot")
        tempvolume_dir = os.path.join(
            actual_path, ".btrfs-livedata-temporary-volume")
        tempdata_dir = os.path.join(
            actual_path, ".livedata-temporary-prevolume-copy")

        # Make sure the livedata/ dir is a btrfs subvolume:    
        if self.service.is_running():
            if not SystemInfo.is_btrfs_subvolume(livedata_dir):
                print_msg("error: can't do btrfs subvolume " +\
                    "conversion because "+\
                    "service is running. The first snapshot is " +\
                    "required to " +\
                    "be done when the service is stopped.",
                    service=self.service.name, color="red")
                return False

        lock_path = os.path.join(self.service.service_path,
            ".drudder-snapshot.lock")

        # Check there is still no transaction running:
        if self.check_running_snapshot_transaction():
            print_msg("error: snapshot already in progress. " +\
                "try again later", service=self.service.name,
                color="red")
            print_msg("remove .drudder-snapshot.lock if you are " +\
                "sure this is incorrect", service=self.service.name)
            return False

        # Add a transaction lock:
        transaction_id = str(uuid.uuid4())
        with open(lock_path, "wb") as f:
            f.write(transaction_id.encode("utf-8"))
        
        # Wait a short amount of time so other race condition writes will
        # be finished with a very high chance:
        time.sleep(0.5)

        # Verify we got the transaction lock:
        contents = None
        with open(lock_path, "rb") as f:
            contents = f.read().decode("utf-8", "ignore")
        if contents.strip() != transaction_id:
            print_msg("error: mid-air snapshot collision detected!! " + \
                "Did you call the script twice?",
                service=self.service.name, color="red")
            return False

        # Make sure the .livedata-predeletion-renamed isn't there:
        if os.path.exists(livedata_renamed_dir):
            if not os.path.exists(livedata_dir):
                print_msg("warning: .livedata-predeletion-renamed/ " + \
                    "is present and no livedata/ folder." +\
                    "Moving it back...",
                    service=self.service.name, color="yellow")
                shutil.move(livedata_renamed_dir, livedata_dir)
                assert(not os.path.exists(livedata_renamed_dir))
            else:
                print_msg("error: .livedata-predeletion-renamed/ " + \
                    "is still there, indicating a previously aborted " +\
                    "run, but livedata/ is also still around. " +\
                    "Please figure out which one you want to keep, and " +\
                    "delete one of the two.", service=self.service.name,
                    color="red")
                sys.exit(1)

        # Make sure the .livedata-temporary-prevolume-copy directory is unused:
        if os.path.exists(tempdata_dir):
            print_msg("warning: .livedata-temporary-prevolume-copy/ " + \
                "already present! " +\
                "This is probably a leftover from a previously " + \
                "aborted attempt. Will now attempt to delete it...",
                service=self.service.name, color="yellow")
            shutil.rmtree(tempdata_dir)
            assert(not os.path.exists(tempdata_dir))

        # Make sure the btrfs snapshot path is unused:
        if os.path.exists(snapshot_dir):
            if SystemInfo.btrfs_subvolume_stat_check(snapshot_dir):
                print_msg("warning: .btrfs-livedata-snapshot/ " \
                    + "already present! " \
                    + "This is probably a leftover from a previously " + \
                    "aborted attempt. Will now attempt to delete it...",
                    service=self.service.name, color="yellow")
                subprocess.check_output([
                    SystemInfo.btrfs_path(),
                    "subvolume",
                    "delete", snapshot_dir])
                assert(not os.path.exists(snapshot_dir))
            else:
                print_msg("error: .btrfs-livedata-snapshot/ already " +\
                    "present, " \
                    + "but it is not a btrfs snapshot!! I don't know how " +\
                    "to deal with this, aborting.",
                    service=self.service.name, color="red")
                return False

        # Make sure the temporary btrfs subvolume path is unused:
        if os.path.exists(tempvolume_dir):
            print_msg("warning: .btrfs-livedata-temporary-volume/ already " +\
                "present! " \
                + "This is probably a leftover from a previously " + \
                "aborted attempt. Will now attempt to delete it...",
                service=self.service.name, color="yellow")
            output = subprocess.check_output([
                SystemInfo.btrfs_path(),
                "subvolume",
                "delete", tempvolume_dir])
            assert(not os.path.exists(tempvolume_dir))

        # If this isn't a btrfs subvolume, we will need to fix that first:
        if not SystemInfo.is_btrfs_subvolume(livedata_dir):
            print_msg("warning: initial subvolume conversion required. "+\
                "DON'T TOUCH livedata/ WHILE THIS HAPPENS!!",
                service=self.service.name, color="yellow")
            try:
                output = subprocess.check_output([
                    SystemInfo.btrfs_path(),
                    "subvolume",
                    "create", tempvolume_dir])
            except Exception as e:
                os.remove(lock_path)
                raise e
            assert(SystemInfo.is_btrfs_subvolume(tempvolume_dir))

            # Copy all contents:
            assert(not os.path.exists(tempdata_dir))
            try:
                shutil.copytree(os.path.realpath(livedata_dir), tempdata_dir, symlinks=True)
            except shutil.Error as e:
                # Check for errors we care about:
                relevant_errors = []
                for err in e.args[0]:
                    src, dst, msg = err
                    if str(msg).startswith("[Errno 6]"):
                        # This occurs with Unix sockets. Check if it is one:
                        mode = os.stat(src).st_mode
                        if stat.S_ISSOCK(mode):
                            continue # It is. Skip it!
                        else:
                            print_msg("warning: errno 6 when accessing " +
                                "file, but os.stat says no unix socket: " +
                                src, color="yellow")
                    # This is an unknown error we most likely care about:
                    relevant_errors.append(err)
                if len(relevant_errors) > 0:
                    e.args[0] = relevant_errors
                    raise e

            assert(os.path.exists(tempdata_dir))
            for f in os.listdir(tempdata_dir):
                orig_path = os.path.join(tempdata_dir, f)
                new_path = os.path.join(tempvolume_dir, f)
                shutil.move(orig_path, new_path)

            # Do a superficial check if we copied all things:
            copy_failed = False
            for f in os.listdir(tempvolume_dir):
                if not os.path.exists(os.path.join(livedata_dir, f)):
                    copy_failed = True
                    break
            for f in os.listdir(livedata_dir):
                if not os.path.exists(os.path.join(tempvolume_dir, f)):
                    copy_failed = True
                    break
            if copy_failed:
                print_msg("error: files of old livedata/ directory and "+\
                    "new subvolume do not match. Did things get changed "+\
                    "during the process??",
                    service=self.service.name, color="red")
                return False

            # Remove old livedata/ dir:
            propagate_interrupt = None
            while True:
                try:
                    shutil.move(livedata_dir, livedata_renamed_dir)
                    shutil.move(tempvolume_dir, livedata_dir)
                    shutil.rmtree(livedata_renamed_dir)
                    break
                except KeyboardInterrupt as e:
                    propagate_interrupt = e
                    continue
            if propagate_interrupt != None:
                raise propagate_interrupt
            print_msg("conversion of livedata/ to btrfs subvolume complete.",
                service=self.service.name)

        snapshots_dir = os.path.join(self.service.service_path,
            "livedata-snapshots")

        # Create livedata-snapshots/ if not present:
        if not os.path.exists(snapshots_dir):
            os.mkdir(snapshots_dir)

        # Go ahead and snapshot:
        print_msg("initiating btrfs snapshot...",
            service=self.service.name)
        output = subprocess.check_output([
            SystemInfo.btrfs_path(),
            "subvolume", "snapshot",
            "-r", "--", livedata_dir, snapshot_dir])
        
        # Copy snapshot to directory:
        now = datetime.datetime.now()
        snapshot_base_name = str(now.year)
        if now.month < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.month)
        if now.day < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.day)
        snapshot_base_name += "-"
        if now.hour < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.hour)
        if now.minute < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.minute)
        if now.second < 10:
            snapshot_base_name += "0"
        snapshot_base_name += str(now.second)
        snapshot_name = snapshot_base_name + "00"
        i = 1
        while os.path.exists(os.path.join(snapshots_dir, snapshot_name)):
            snapshot_name = snapshot_base_name
            if i < 10:
                snapshot_name += "0"
            snapshot_name += str(i)
            i += 1
        snapshot_specific_dir = os.path.join(snapshots_dir,
            snapshot_name)
        print_msg("copying to " + snapshot_specific_dir,
            service=self.service.name)
        try:
            shutil.copytree(snapshot_dir, snapshot_specific_dir, symlinks=True)
        except shutil.Error as e:
            # Check for errors we care about:
            relevant_errors = []
            for err in e.args[0]:
                src, dst, msg = err
                if str(msg).startswith("[Errno 6]"):
                    # This occurs with Unix sockets. Check if it is one:
                    mode = os.stat(src).st_mode
                    if stat.S_ISSOCK(mode):
                        continue # It is. Skip it!
                    else:
                        print_msg("warning: errno 6 when accessing " +
                            "file, but os.stat says no unix socket: " +
                            src, color="yellow")
                # This is an unknown error we most likely care about:
                relevant_errors.append(err)
            if len(relevant_errors) > 0:
                e.args[0] = relevant_errors
                raise e 
        subprocess.check_output([
            SystemInfo.btrfs_path(), "subvolume",
            "delete", snapshot_dir])
        assert(not os.path.exists(snapshot_dir))
        print_msg("snapshot complete.", service=self.service.name,
            color="green")

        # Remove lock file:
        assert(os.path.exists(lock_path) and not os.path.isdir(lock_path))
        os.remove(lock_path)
        assert(not os.path.exists(lock_path))

        return True

class TargetsParser(object):
    """ A helper class to parse user input and turn it into container or
        services lists.
    """
    @staticmethod
    def split_targets(targets):
        # Split up the targets:
        return targets.strip().split(" ")

    @staticmethod
    def get_services(targets, print_error=False):
        while targets.find("  ") >= 0:
            targets.replace("  ", " ")
        result = set()

        targets = TargetsParser.split_targets(targets)

        # Go through all targets in the list:
        for target in targets:
            negated = False
            if target.startswith("+"):
                target = target[1:]
            elif target.startswith("-"):
                target = target[1:]
                negated = True

            # Specific treatment of the "all" keyword:
            if target == "all":
                for group in ServiceGroup.all():
                    for service in group.services:
                        if not negated:
                            result.add(service)
                        else:
                            try:
                                result.remove(service)
                            except KeyError:
                                pass
                continue

            # Examine the current group[/service] entry:
            group_name = target.partition("/")[0]
            groups = ServiceGroup.find_by_name(group_name)
            if len(groups) == 0:
                if print_error:
                    print("drudder: error: " +
                        "no such service group found: " +
                        str(group_name),
                        file=sys.stderr, flush=True)
                return None

            # See if a specific service is specified, or just all of them:
            service_name = target.partition("/")[2]
            services = []
            if len(service_name) == 0: # all services:
                found_services = False
                for g in groups:
                    if len(g.services) > 0:
                        found_services = True
                if not found_services:
                    print_msg("warning: specified group has no " +\
                        "services", color="yellow", group=group.name)
                for g in groups:
                    services += g.services
            else: # a specific service. find it by name:
                services = []
                for group in groups:
                    for service in group.services:
                        if service.name == service_name:
                            services.append(service)
                # Check if the container was found by name or not:
                if len(services) == 0:
                    if print_error:
                        print("drudder: error: " + \
                            "no such service in group \"" +\
                            str(group_name) + "\" found: " +\
                            str(service_name),
                            file=sys.stderr)
                    return None
            # Add/remove all containers collected by this entry:
            for service in services:
                if not negated:
                    result.add(service)
                else:
                    try:
                        result.remove(service)
                    except KeyError:
                        pass
        return list(result)

    @staticmethod
    def get_groups(targets, print_error=False):
        while targets.find("  ") >= 0:
            targets.replace("  ", " ")
        result = set()
        targets = TargetsParser.split_targets(targets)

        # Go through all targets in the list:
        for target in targets:
            negated = False
            if target.startswith("+"):
                target = target[1:]
            elif target.startswith("-"):
                target = target[1:]
                negated = True

            # Specific treatment of the "all" keyword:
            if target == "all":
                for group in ServiceGroup.all():
                    if not negated:
                        result.add(group)
                    else:
                        try:
                            result.remove(group)
                        except KeyError:
                            pass
                return list(result)
            groups = ServiceGroup.find_by_name(target.partition("/")[0])
            if len(groups) == 0:
                if print_error:
                    print("drudder: error: " + \
                        "no such group found: " + str(
                        target.partition("/")[0]),
                        file=sys.stderr)
                return None
            if not negated:
                result += groups
            else:
                try:
                    for group in groups:
                        result.remove(group)
                except KeyError:
                    pass
        return list(result)

class ServiceLinkDependencyNode(object):
    """ A node in the dependency graph of container dependencies.
        Has incoming and outgoing link edges, and has an associated container.
    """
    def __init__(self, service, incoming_links, outgoing_links, graph):
        self.incoming_links = incoming_links
        self.outgoing_links = outgoing_links
        self.service = service
        self.graph = graph

    def __repr__(self):
        return "<ServiceLinkDependencyNode " + str(self.service) +\
            " (" +\
            ", ".join([str(self.service) + " -> " + str(link.service)\
                for link in self.outgoing_links]) + ")>"

    def __hash__(self):
        return hash(self.__repr__())

    def __eq__(self, other):
        if other == None:
            return False
        if not hasattr(other, "service") or \
                not hasattr(other, "incoming_links") or \
                not hasattr(other, "outgoing_links") or \
                not hasattr(other, "graph"):
            return False
        if other.service == self.service:
            if other.graph != self.graph:
                return False
            return True
        return False

    def __neq__(self, other):
        return not self.__eq__(other)

    def fulfills_dependencies(self, services):
        nodes = [self.graph[service] for service in services]
        for outgoing_link in self.outgoing_links:
            if not outgoing_link in nodes:
                return False
        return True

    def is_depending_on(self, services):
        nodes = [self.graph[service] for service in services]
        for outgoing_link in self.outgoing_links:
            if outgoing_link in nodes:
                return True
        return False

    def is_dependency_of(self, services):
        nodes = [self.graph[service] for service in services]
        for incoming_link in self.outgoing_links:
            if incoming_link in nodes:
                return True
        return False

class DoAlongDependencyEdges(object):
    """ This helper class operates on the graph provided by a
        ContainerLinkDependencyResolution object.

        It constructs a subgraph according to the following rules:

        - the graph will be made of the specified initial_services list,
          and then gradually grown along all the outgoing edges or,
          alternatively with along_incoming=True, along all the incoming
          edges to the maximum extent

        You can then yield services in this subgraph in the partial order
        laid out by the connections it was grown along, starting with the root
        nodes, then their children, etc.

        This allows you to apply a function to all yielded services in
        order, e.g. launching them.

        See the iterators DoAlongDependencyEdges.unprocessed_services() or
        DoAlongDependencyEdges.failed_services() for details on how the
        services of this subgraph can be yielded.
    """
    def __init__(self, dependency_resolution_object,
            initial_services,
            along_incoming=False, expand_initial_set=True,
            reverse_yield_order=False):
        self.access_lock = threading.Lock()

        self.expand_initial_set = expand_initial_set
        self.along_incoming = along_incoming
        self.graph = dependency_resolution_object.graph
        self.services = initial_services
        self.start_set = set()
        for service in initial_services:
            self.start_set.add(self.graph[service])
        self.marked_done = set()
        self.marked_failed = set()
        self.yielded_nodes = set()
        self.yielded_failed_nodes = set()
        self.reverse_yield_order = reverse_yield_order

        # Grow to the relevant subgraph which can be possibly returned:
        expand_queue = queue.Queue()
        self.subgraph = {}
        for service in initial_services: # queue up initial services
            self.subgraph[service] = self.graph[service]
            expand_queue.put(self.subgraph[service])
        if expand_initial_set:
            while not expand_queue.empty(): # crawl through graph and grow it
                expand_node = expand_queue.get()
                expanded_nodes = []
                if along_incoming:
                    expanded_nodes = expand_node.incoming_links
                else:
                    expanded_nodes = expand_node.outgoing_links
                for expanded_node in expanded_nodes:
                    if not expanded_node.service in self.subgraph:
                        self.subgraph[expanded_node.service] = expanded_node
                        expand_queue.put(expanded_node)

    def __iter__(self):
        return self

    def get_next_candidates(self):
        """ Internal function. Don't call before obtaining access lock!!

            Returns a set of candidates that can currently be returned as
            unprocessed nodes. Ignores which ones have already been returned
            by the iterator, but it ensures only those reachable through
            success nodes are actually returned.
        """
        start_nodes = set()
        if not self.reverse_yield_order:
            # Get root nodes in relevant sub graph:
            for node in self.subgraph.values():
                if self.along_incoming:
                    if len([node for node in node.outgoing_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
                else:
                    if len([node for node in node.incoming_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
            if len(start_nodes) == 0:
                raise RuntimeError("invalid graph, no root nodes present")
        else:
            # Get leaf nodes in relevant sub graph:
            for node in self.subgraph.values():
                if self.along_incoming:
                    if len([node for node in node.incoming_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
                else:
                    if len([node for node in node.outgoing_links\
                            if node in self.subgraph.values()]) == 0:
                        start_nodes.add(node)
            if len(start_nodes) == 0:
                raise RuntimeError("invalid graph, no leaf nodes present")

        # Prepare some stuff for gradually crawling the graph:
        examine_queue = queue.Queue()
        def get_successors(node):
            if self.along_incoming:
                if not self.reverse_yield_order:
                    return node.incoming_links
                else:
                    return node.outgoing_links
            else:
                if not self.reverse_yield_order:
                    return node.outgoing_links
                else:
                    return node.incoming_links
        def get_predecessors(node):
            if self.along_incoming:
                if not self.reverse_yield_order:
                    return node.outgoing_links
                else:
                    return node.incoming_links
            else:
                if not self.reverse_yield_order:
                    return node.incoming_links
                else:
                    return node.outgoing_links
        for node in start_nodes:
            examine_queue.put(node)

        # Start advancing through graph to next unprocessed nodes:
        seen = set()
        candidates = []
        while not examine_queue.empty():
            next_node = examine_queue.get()
            if not next_node in self.marked_done and \
                    not next_node in self.marked_failed:
                # This node is unprocessed. Check if all predecessors are
                # actually marked as done so we can return it:
                predecessors = get_predecessors(next_node)
                reachable = True
                for predecessor in predecessors:
                    if not predecessor.service in self.subgraph:
                        continue
                    if not predecessor in self.marked_done:
                        # Nope, at least one predecessor isn't done yet.
                        reachable = False
                        break
                    if predecessor in self.marked_failed:
                        # Whoops, this should also be marked failed.
                        self.marked_failed.add(next_node)
                        reachable = False
                        break
                if reachable:
                    candidates.append(next_node)
            else:
                # This node is either failed or done/succeeded. See which:
                if not next_node in self.marked_failed:
                    # It's a succeeded node, therefore look at the successors:
                    for successor in get_successors(next_node):
                        if not successor.service in self.subgraph:
                            continue
                        if not successor in seen:
                            seen.add(successor)
                            examine_queue.put(successor)

        # Return assembled candidates:
        return candidates

    def mark_success(self, service):
        """ Mark a service as succeeded. This opens up all the direct
            dependencies of this service to be returned from
            unprocessed_services() as next unprocessed services to be
            taken care of.
        """
        self.access_lock.acquire()
        self.marked_done.add(self.graph[service])
        self.access_lock.release()

    def mark_failure(self, service):
        """ Mark a service as failed. No service depending on this one
            will be considered for yielding for unprocessed_services()
            anymore.
        """
        self.access_lock.acquire()
        self.marked_failed.add(self.graph[service])
        self.access_lock.release()

    def _next_unprocessed_node(self):
        """ Note: internal function, use unprocessed_nodes() iterator instead.

            Get the next node that is still unprocessed.
            
            Calling this function continuously will gradually yield all
            unprocessed nodes that are obtainable (see
            unprocessed_services() for details), and None if there is
            currently no unprocessed node that can be returned.
        """

        self.access_lock.acquire()
        # Get all the candidates:
        candidates = self.get_next_candidates()

        # Check which one we haven't already returned:
        actual_candidates = []
        for candidate in candidates:
            if not candidate in self.yielded_nodes:
                actual_candidates.append(candidate)

        # If no real candidate remains, return None:
        if len(actual_candidates) == 0:
            self.access_lock.release()
            return None

        # Pick a random choice of the remaining candidates:
        yielded_node = random.choice(actual_candidates)
        self.yielded_nodes.add(yielded_node)
        result = yielded_node
        self.access_lock.release()
        return result

    def _all_nodes_processed(self):
        """ Check if all nodes are either marked as success or failure.
            Returns True if all are marked, False if some are still unmarked.
        """
        result = True
        self.access_lock.acquire()

        def get_predecessors(node):
            if self.along_incoming:
                if not self.reverse_yield_order:
                    return node.outgoing_links
                else:
                    return node.incoming_links
            else:
                if not self.reverse_yield_order:
                    return node.incoming_links
                else:
                    return node.outgoing_links

        # Check for completely unmarked nodes:
        for node in self.subgraph.values():
            if not node in self.marked_failed and \
                    not node in self.marked_done:
                # Make sure to propagate failures, eventually:
                predecessors = get_predecessors(node) 
                for predecessor in predecessors:
                    if predecessor in self.marked_failed:
                        self.marked_failed.add(node)
                        break

                # Stop since we found an unmarked node:
                result = False
                break

        self.access_lock.release()
        return result

    def unprocessed_services(self):
        """ This function returns an iterator which yields the items according
            to the order specified in the DoAlongDependencyEdges class
            description.

            However, it introduces two additional criteria (while maintaing
            the order implicated by the subgraph):

            - a service can only be yielded as soon as all the direct
              predecessors are marked as "succeeded" with mark_success().

            - all services where any direct or indirect predecessors were
              marked as "failed" with mark_failure() will be ignored and not
              yielded at all.

            If services cannot be yielded because the first criterion
            isn't fulfilled and they weren't discarded through the second
            criterion yet and if all services that could have been yielded
            so far have been yielded, the iterator will hang and wait for
            this situation to be resolved through further mark_success()/
            mark_failure() calls.

            As a result, this iterator will eventually hang unless you make
            sure to eventually mark all returned items with "mark_success" or
            "mark_failure". If you fail to do that, the iterator will remain
            stuck.
        """

        class UnprocessedContainersIterator(object):
            def __init__(self, doalongobj):
                self.doalongobj = doalongobj

            def __iter__(self):
                return self

            def __next__(self):
                skipped = True
                while skipped:
                    skipped = False

                    # Get next node or wait until we can get it:
                    node = self.doalongobj._next_unprocessed_node()
                    while node == None and \
                            not self.doalongobj._all_nodes_processed():
                        time.sleep(0.2)
                        node = self.doalongobj._next_unprocessed_node()
                    if node == None:
                        raise StopIteration

                    # Make sure we only expand beyond initial set if we were
                    # supposed to do that:
                    if not self.doalongobj.expand_initial_set and \
                            not node.service in self.doalongobj.services:
                        # Skip this, it would expand beyond initial set.
                        skipped = True
                        continue
                    break
                return node.service
        return UnprocessedContainersIterator(self)

    def failed_services(self):
        """ This function returns an iterator which yields services
            in no particular order which fulfill the following criterion:

            - services which have been marked as "failed" or where any
              direct or indirect predecessors were marked as "failed" with
              mark_failure() will be yielded

            If services have neither met by this criterion, nor been marked
            as succeeded and if all services that could have been yielded so
            far have been yielded already, this iterator will hang and wait
            for this situation to be resolved through further mark_success()/
            mark_failure() calls.

            As a result, this iterator will eventually hang unless you make
            sure to eventually mark all returned items with "mark_success" or
            "mark_failure". If you fail to do that, the iterator will remain
            stuck.
        """
        raise NotImplementedError("not implemented at this point")

class ContainerLinkDependencyResolution(object):
    """ This class builds a graph based on the service.dependencies (which
        are themselves constructed from the docker links of each service).

        This graph can then be used with DoAlongDependencyEdges to do things
        according to the dependency order.
    """
    def __init__(self, services):
        self.services = services
        self.graph = dict()

        # Add all services as nodes without arcs:
        for service in self.services:
            if not service in self.graph:
                self.graph[service] = ServiceLinkDependencyNode(
                    service, [], [], self.graph)

        # Go through services again and add arcs:
        node_added = True
        while node_added:
            node_added = False
            for service in self.services:
                source_node = self.graph[service]
                for dependency in service.dependencies:
                    dep_service = None
                    try:
                        dep_service = dependency.service
                    except ValueError:
                        print("drudder: error: dependency for " +\
                            str(service) + " is not in known services, " +\
                            "can't resolve dependency chain: " +\
                            str(dependency), file=sys.stderr)
                        sys.exit(1)

                    # Make sure the target for our new arc is in the graph:
                    if not dep_service in self.graph:
                        self.graph[dep_service] = \
                            ServiceLinkDependencyNode(
                            dep_service, [], [], self.graph)
                        node_added = True

                    # Obtain target node ref:
                    target_node = self.graph[dep_service]

                    # Add arcs in both directions, if any is missing:
                    if not target_node in source_node.incoming_links:
                        source_node.outgoing_links.append(target_node)
                    if not source_node in target_node.outgoing_links:
                        target_node.incoming_links.append(source_node)
                if node_added:
                    break
        # Done! Graph complete

    def compute_cliques(self, service_subset=None):
        """ Returns a list of sets of cliques in the service graph.
        """
        if len(self.graph.keys()) == 0:
            return []
        cliques = [set([list(self.graph.keys())[0]])]
        def grow_clique(node_clique):
            result = copy.copy(node_clique)
            something_changed = True
            while something_changed:
                something_changed = False
                add_services = set()
                for service in result:
                    node = self.graph[service]
                    for neighbor in node.incoming_links:
                        add_services.add(neighbor.service)
                    for neighbor in node.outgoing_links:
                        add_services.add(neighbor.service)
                result = result.union(add_services)
            return result

        # Processing loop of our clique algorithm:
        while True:
            new_cliques = []
            for clique in cliques:
                new_cliques.append(grow_clique(clique))
            cliques = new_cliques
            found_missing = False
            for service in self.graph:
                service_found = False
                for clique in cliques:
                    if service in clique:
                        service_found = True
                        break
                if not service_found:
                    found_missing = True
                    cliques.append(set([service]))
                    break
            if not found_missing:
                break

        # Remove duplicate cliques, and trim to given subset:
        if service_subset == None:
            service_subset = self.graph.keys()
        else:
            if not type(service_subset) == set:
                service_subset = set(service_subset)
        result = []
        for clique in cliques:
            clique_clipped = clique.intersection(service_subset)
            if clique_clipped in result:
                continue
            result.append(clique_clipped)
        return result 

    def print_graph_debug(self):
        print("Graph:")
        seen = set()
        for service in self.graph:
            node = self.graph[service]
            for outgoing in node.outgoing_links:
                assert(node in outgoing.incoming_links)
                print(str(node.service) + " -> " + str(outgoing.service))
                seen.add(node.service)
                seen.add(outgoing.service)
        for service in self.graph:
            if not service in seen:    
                print(str(service))
        print("End of Graph.")

class ServiceDetailInfo(object):
    """ A class to compute a dictionary of all sorts of extra information for
        a container. Used by the "info" command.
    """
    def __init__(self, service):
        self.service = service

    def get(self, ordered=False):
        dict_class = dict
        if ordered:
            dict_class = OrderedDict
        info = dict_class()
        info["Default docker container name"] = \
            self.service.default_container_id
        info["Container ids"] = [container.container_id for container in \
            self.service.containers]
        info["Owning service group"] = OrderedDict()
        info["Owning service group"]["Name"] = self.service.group.name
        info["Owning service group"]["Location"] = \
            self.service.group_dir
        info["Running"] = True if len([container for container in \
            self.service.containers if container.running]) > 0 else False
        info["Dependencies"] = OrderedDict()
        resolution = ContainerLinkDependencyResolution(
            Service.all())
        info["Dependencies"]["Pre-Start"] = []
        pre_start = \
            DoAlongDependencyEdges(resolution, [self.service],
            along_incoming=False, reverse_yield_order=True)
        for service in pre_start.unprocessed_services():
            if service == self.service:
                pre_start.mark_success(service)
                continue
            info["Dependencies"]["Pre-Start"].append(
                str(service))
            pre_start.mark_success(service)
        info["Dependencies"]["Pre-Stop-Post-Restart"] = []
        pre_stop = \
            DoAlongDependencyEdges(resolution, [self.service],
            along_incoming=True, reverse_yield_order=True)
        for service in pre_stop.unprocessed_services():
            if service == self.service:
                pre_stop.mark_success(service)
                continue
            info["Dependencies"]["Pre-Stop-Post-Restart"].append(
                str(service))
            pre_stop.mark_success(service)

        # Collect volumes:
        potentially_lost = self.service.potentially_lost_volumes
        info["Volumes"] = list()
        for volume in self.service.volumes:
            vol_info = dict_class()
            if volume.id != None:
                vol_info["Id"] = str(volume.id)
            if volume.name != None:
                vol_info["Name"] = volume.name
                vol_info["Unnamed"] = False
            else:
                vol_info["Unnamed"] = True
            vol_info["Specified in YAML"] = volume.specified_in_yml
            if volume in potentially_lost:
                vol_info["Potentially lost on container recreation"] = True
            mounts = volume.mounts
            if len(mounts) > 0:
                vol_info["Mounts"] = list()
                for mount in mounts:
                    mount_info = dict_class() 
                    if mount.host_path != None:
                        mount_info["Host mount path"] = mount.host_path
                    if mount.container_mount_path != None:
                        mount_info["Container mount path"] =\
                            mount.container_mount_path
                    vol_info["Mounts"].append(mount_info)
            info["Volumes"].append(vol_info)
        return info

def bulk_build_services(services, reason, force_complete_rebuild=False,
        print_mutex=None):
    # Rebuild all containers of our core restart group
    print_lock = print_mutex
    if print_lock == None:
        print_lock = threading.Lock()
    counter = { "count" : 0, "total" : 0,
        "some_failed" : False }
    class RebuildThread(threading.Thread):
        def __init__(self, service, print_lock, counter):
            super().__init__()
            self.service = service
            self.done = False
            self.counter = counter

        def run(self):
            output = None
            try:
                if not force_complete_rebuild:
                    output = SubprocessHelper.check_output_with_isolated_pty([
                        SystemInfo.docker_compose_path(), "build",
                        self.service.name],
                        cwd=self.service.group_dir,
                        stderr=subprocess.STDOUT)
                else:
                    output = SubprocessHelper.check_output_with_isolated_pty([
                        SystemInfo.docker_compose_path(),
                        "build",
                        "--no-cache", "--pull", self.service.name],
                        cwd=self.service.group_dir,
                        stderr=subprocess.STDOUT)
            except subprocess.CalledProcessError as e:
                output = e.output
                counter["some_failed"] = True
            try:
                output = output.decode("utf-8", "replace")
            except AttributeError:
                pass
            self.counter["count"] += 1
            print_lock.acquire()
            repair_terminal()
            print(clear_text(output.rstrip()), flush=True)
            print_msg("BUILD PROCESS: " + str(self.counter["count"]) + "/"
                + str(self.counter["total"]) + " services", color="blue")
            print_lock.release()
            self.done = True
    tlist = []
    for service in services:
        counter["total"] += 1
        print_lock.acquire()
        print_msg("rebuilding... " +\
            "(" + reason + ")",
            service=service.name,
            group=service.group.name,
            color="blue")
        print_lock.release()
        tlist.append(RebuildThread(service, print_lock, counter)) 
    for t in tlist:
        t.start()
    all_done = False
    while not all_done:
        all_done = True
        for t in tlist:
            if not t.done:
                all_done = False
        time.sleep(2)
    return not counter["some_failed"]


def main():
    if not yaml_available:
        yaml_error()
        sys.exit(1)

    parser = CommandLineArgumentParser.construct_parser()
    if len(" ".join(sys.argv[1:]).strip()) == 0:
        parser.print_help()
        sys.exit(1)
    for arg in sys.argv[1:]:
        if arg == "--version" or arg == "-v" or arg == "-V":
            print("drudder version " + str(TOOL_VERSION))
            sys.exit(0)
    args = parser.parse_args()
    gc.open(read_from_path=args.config_path)

    ensure_docker = SystemInfo.docker_path()
    ensure_docker_compose = SystemInfo.docker_compose_path()

    def unknown_action(hint=None):
        """ Print an error that the given action to drudder is invalid,
            with a possible hint to suggest another action.
        """
        print("drudder: error: unknown action: " + \
            args.action, file=sys.stderr)
        if hint != None:
            print("Did you mean: " + str(hint) + "?")
        sys.exit(1)

    # Ensure the docker main service is running:
    error_output = None
    try:
        subprocess.check_output([SystemInfo.docker_path(), "ps"],
            stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        error_output = e.output.decode("utf-8", "ignore")
    if error_output != None:
        # Old-style error message:
        if error_output.find("dial unix") >= 0 and \
                error_output.find("no such file or directory") >= 0:
            print("drudder: error: " + \
                "docker daemon appears to be not running." +\
                " Please start it and ensure it is reachable.")
            sys.exit(1)
        # Newer error message:
        elif error_output.find("Cannot connect to the Docker daemon") >= 0:
            print("drudder: error: " + \
                "docker daemon appears to be not running." +\
                " Please start it and ensure it is reachable.")
            sys.exit(1)
        else:
            print("drudder: error: " + \
                "there appears to be some unknown problem with " + \
                "docker! (test run of \"docker ps\" returned error code)")
            sys.exit(1)

    # Check if services are btrfs ready, and give warning if not:
    if args.action != "snapshot" and args.action != "info":
        for service_group in ServiceGroup.all():
            snapshots = Snapshots(service_group)
            snapshots.subvolume_readiness_check()

    # --- Main handling of actions here:

    if args.action == "list":
        all_groups = ServiceGroup.all()
        print("\033[1mService Group list (" + str(len(all_groups)) +\
            " group(s)):\033[0m")
        for group in all_groups:
            state = ""
            running_services = []
            stopped_services = []
            for service in group.services:
                containers = [container for container in service.containers]
                running_containers = [container for container in containers if \
                    container.running]
                if len(running_containers) > 0:
                    running_services.append(service)
                else:
                    stopped_services.append(service)
            if len(running_services) + len(stopped_services) == 0:
                state = "\033[1;33mgroup has no services" +\
                    "\033[0m"
            elif len(stopped_services) == 0:
                state = "\033[1;32mrunning (" +\
                    " ".join([service.name for service in \
                        running_services]) + ")\033[0m"
            elif len(running_services) > 0:
                state = "\033[1;33mpartial (" +\
                    ", ".join([service.name for service in \
                        running_services]) +\
                    "), others stopped (" +\
                    ", ".join([service.name for service in \
                        stopped_services]) + ")\033[0m"
            else:
                state = "\033[1;31mstopped (" +\
                    ", ".join([service.name for service in \
                        stopped_services]) +\
                        ")\033[0m"
            print("\033[0m  - \033[1mgroup " + group.name + "\033[0m (" + \
                group.group_dir + "): " + state)
    elif args.action == "help":
        parser.print_help()
        sys.exit(1)
    elif args.action == "logs":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service for which docker logs shold be printed, "+\
                "or \"all\"", file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None:
            sys.exit(1)
        for service in services:
            for container in service.containers:
                print_msg("printing log of container " + str(container),
                    service=service.name,
                    group=service.group.name, color='blue')
                try:
                    retcode = subprocess.call([SystemInfo.docker_path(), "logs",
                            container.container_id],
                            stderr=subprocess.STDOUT)
                    if retcode != 0:
                        raise subprocess.CalledProcessError(
                            retcode, " ".join([SystemInfo.docker_path(),
                            "logs", container.container_id]))
                except subprocess.CalledProcessError:
                    print_msg("failed printing logs. " +\
                        "Maybe container has no logs yet?",
                        service=service.name,
                        group=service.group.name,
                        color='yellow')
                    pass
    elif args.action == "rebuild" or args.action == "build":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service which should be " +\
                ("" if args.action == "build" else "re") + "built",
                file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None:
            sys.exit(1)
        some_failed = not bulk_build_services(services,
            "this service is now being rebuilt with no cache due to "
            "a \"rebuild\" command having been issued",
            force_complete_rebuild=True)

        if some_failed:
            print_msg("Rebuilding completed with errors.", color="red")
            sys.exit(1)
        print_msg("Rebuilding complete without errors.", color="green")
        sys.exit(0)
    elif args.action == "shell":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the group or service for which an interactive " +\
                "shell should be started",
                file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None:
            sys.exit(1)
        if len(services) != 1:
            print("drudder: error: this command can only be used " + \
                "on a single services, but the given argument \"" +\
                " ".join(args.argument) +\
                "\" matches multiple " +\
                "services: " +\
                ", ".join([str(service) for service in services]),
                file=sys.stderr)
            sys.exit(1)
        running_containers = services[0].running_containers
        if len(running_containers) > 0:
            containers = services[0].running_containers
            cname = containers[0].container_id
            print_msg("attaching to running container " + str(cname),
                group=services[0].group.name,
                service=services[0].name,
                color="blue")
            subprocess.call([SystemInfo.docker_path(), "exec", "-t", "-i",
                    str(cname), "/bin/bash"],
                stderr=subprocess.STDOUT)
        else:
            print_msg("launching container for service " +
                str(services[0].name) + " with shell",
                group=services[0].group.name,
                service=services[0].name,
                color="blue")
            subprocess.call([SystemInfo.docker_compose_path(), "build",
                services[0].name],
                cwd=services[0].group_dir)
            subprocess.call([SystemInfo.docker_compose_path(), "run",
                services[0].name, "/bin/bash"],
                cwd=services[0].group_dir)
    elif args.action == "start" or args.action == "restart":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service to be started, or \"all\"", file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None:
            sys.exit(1)
        if len(services) == 0:
            sys.exit(0)

        # Create a dependency resolution graph which we use for partial
        # order dependency resolution for parallel launches:
        resolution = ContainerLinkDependencyResolution(
            Service.all())

        # Create a launch tracker used later for parallel launches:
        tracker = FailedLaunchTracker()
        scheduled_action = "restart"
        if args.action == "start":
            scheduled_action = "start"

        # Get all cliques of our dependency graph:
        cliques = resolution.compute_cliques()

        # Clique threads that deal with start/restart:
        print_mutex = threading.Lock()
        threads = []
        class CliqueThread(threading.Thread):
            def __init__(self, clique, print_mutex):
                super().__init__()
                self.services = clique.intersection(set(services))
                self.done = False
                self.mutex = print_mutex

            def run(self):
                if len(self.services) == 0:
                    self.done = True
                    return

                # Rebuild all containers of our core restart group
                if not args.no_rebuild and args.action == "restart":
                    bulk_build_services(self.services,
                        "this service is rebuilt for a fresh " +
                        "up-to-date restart, since it was scheduled for " +
                        scheduled_action + "ing", print_mutex=self.mutex)

                restart_dependencies = set()
                if scheduled_action == "restart":
                    # Rebuild services to be restarted, and stop all services
                    # to be restarted including their indirect dependencies:

                    # Create a graph traversal helper:
                    unreversed_stop_before_restart = \
                        DoAlongDependencyEdges(resolution, self.services,
                        along_incoming=True)
                    stop_before_restart = []

                    # Collect all indirect dependencies that need to be
                    # stopped:
                    # (starts out with restart container set, and grows
                    # gradually outwards to outer indirect dependencies)
                    for service in \
                            unreversed_stop_before_restart.\
                            unprocessed_services():
                        stop_before_restart.append(service)
                        unreversed_stop_before_restart.mark_success(service)

                    # Reverse list so we have the "most outer" services first:
                    stop_before_restart = list(reversed(stop_before_restart))

                    # Do the stopping:
                    for service in stop_before_restart:
                        if not service in services:
                            self.mutex.acquire()
                            print_msg("stopping... " +\
                                "(this service is depending on 1+ " +\
                                "of the service(s) scheduled for restart)",
                                service=service.name,
                                group=service.group.name,
                                color="blue")
                            self.mutex.release()
                            restart_dependencies.add(service)
                        else:
                            self.mutex.acquire()
                            print_msg("stopping... " +\
                                "(this service was scheduled for " +\
                                scheduled_action + ")",
                                service=service.name,
                                group=service.group.name,
                                color="blue")
                            self.mutex.release()
                        LaunchThreaded.stop(service)

                # Helper function to launch a service:
                def start_service(service, is_last=False,
                        do_on_failure=None, do_on_success=None):
                    if service.running:
                        if service in self.services:
                            self.mutex.acquire()
                            print_msg("already running. (this service was " +\
                                    "scheduled for " +\
                                    scheduled_action + ")",
                                group=service.group.name,
                                service=service.name,
                                color="green")
                            self.mutex.release()
                        elif service in restart_dependencies:
                            self.mutex.acquire()
                            print_msg("already running. " +\
                                    "(this service was " +\
                                    "an external dependency on the " +\
                                    scheduled_action +\
                                    "ed containers) - " +\
                                    "weird, didn't we stop it before?? (" +
                                    "if you can reproduce this, report it)",
                                service=service.name,
                                group=service.group.name,
                                color="yellow")
                            self.mutex.release()
                        else:
                            self.mutex.acquire()
                            print_msg("already running. (this service is " +\
                                    "a dependency of a service scheduled " +\
                                    "for " +\
                                    scheduled_action + ")",
                                service=service.name,
                                group=service.group.name,
                                color="green")
                            self.mutex.release()
                        if do_on_success != None:
                            do_on_success()
                        return
                    if not is_last:
                        t = LaunchThreaded.attempt_launch(service,
                            failed_launch_tracker=tracker,
                            force_container_recreation=(args.force is True),
                            do_on_failure=do_on_failure,
                            do_on_success=do_on_success,
                            print_mutex=self.mutex)
                    else:
                        t = LaunchThreaded.attempt_launch(service,
                            failed_launch_tracker=tracker,
                            to_background_timeout=None,
                            force_container_recreation=(args.force is True),
                            do_on_failure=do_on_failure,
                            do_on_success=do_on_success,
                            print_mutex=self.mutex)
                    if t != None:
                        threads.append(t)

                def mark_success_func(net, container):
                    return lambda: net.mark_success(container)

                def mark_failure_func(net, container):
                    return lambda: net.mark_failure(container)

                # Start the dependencies of our container launch set:
                start_dependency_services = \
                    DoAlongDependencyEdges(resolution, self.services,
                    along_incoming=False, expand_initial_set=True,
                    reverse_yield_order=True)
                for service in start_dependency_services.\
                        unprocessed_services():
                    if service in self.services: # not a dependency
                        start_dependency_services.mark_success(service)
                        continue
                    # Launch it:
                    start_service(service, is_last=False,
                        do_on_failure=mark_failure_func(
                            start_dependency_services, service),
                        do_on_success=mark_success_func(
                            start_dependency_services, service))

                # Start everything again:
                start_services = \
                    DoAlongDependencyEdges(resolution,
                        set(self.services).union(\
                            restart_dependencies),
                        along_incoming=False, expand_initial_set=False,
                        reverse_yield_order=True)
                for service in start_services.unprocessed_services():
                    start_service(service, is_last=False,
                        do_on_failure=mark_failure_func(
                            start_services, service),
                        do_on_success=mark_success_func(
                            start_services, service))

                self.done = True

        # Wait until clique threads are finished:
        clique_threads = []
        for clique in cliques:
            t = CliqueThread(clique, print_mutex)
            t.start()
            clique_threads.append(t)
        while True:
            for t in clique_threads:
                if not t.done:
                    time.sleep(1)
                    continue
            break

        # Wait until launches are finished:
        LaunchThreaded.wait_for_launches(threads)
        if len(tracker) > 0:
            print("drudder: error: some launches failed.",
                file=sys.stderr)
            sys.exit(1)
        else:
            sys.exit(0)
    elif args.action == "stop":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service to be stopped, or \"all\"", file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None:
            sys.exit(1)
        for service in services:
            assert(service != None)
            if service.running:
                print_msg("stopping...",
                    service=service.name,
                    group=service.group.name,
                    color="blue")
                LaunchThreaded.stop(service)
                print_msg("stopped.",
                    service=service.name,
                    group=service.group.name, color="green")
            else:
                print_msg("not currently running.",
                    service=service.name,
                    group=service.group.name,
                    color="blue")
        sys.exit(0)
    elif args.action == "snapshot":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service to be snapshotted, or \"all\"",
                file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_groups(" ".join(args.argument),
            print_error=True)
        return_error = False
        for service in services:
            fs = SystemInfo.filesystem_type_at_path(service.service_path)
            if fs != "btrfs":
                print_msg("cannot snapshot service. filesystem " +\
                    "is " + fs + ", would need to be btrfs",
                    service=service.name, color="red")
                return_error = True
                continue
            snapshots = Snapshots(service)
            if not snapshots.do():
                return_error = True
        if return_error:
            print("drudder: error: some snapshots failed.",
                file=sys.stderr)
            sys.exit(1)
        else:
            sys.exit(0)
    elif args.action == "info":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service or container to print info for, " +\
                "or \"all\"", file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None or len(services) == 0:
            sys.exit(1)
        def print_info(key, value=None, color=None, bold=False,
                no_line_break=False):
            if hasattr(value, "append"):
                if len(value) == 0:
                    value = "<empty list>"
                else:
                    value = ", ".join([str(item) for item in value])
            if bold:
                t = "\033[1m" + key
            else:
                t = key
            if value == None:
                if bold:
                    t += "\033[0m"
                print(t)
                return
            if (len(value) > 25 or value.find("\n") >= 0) and \
                    not no_line_break:
                value = "\n  " + "\n  ".join(
                    [line for line in value.split("\n")])
            if bold:
                t += "\033[0m: "
            else:
                t += ": "
            if color == "red":
                t += "\033[31m\033[1m" + value
            elif color == "yellow":
                t += "\033[33m\033[1m" + value
            elif color == "green":
                t += "\033[32m\033[1m" + value
            elif color == "blue":
                t += "\033[34m\033[1m" + value
            else:
                t += value
            if color != None:
                t += "\033[0m"
            print(t)
        first = True
        for service in services:
            if first:
                first = False
            else:
                print("")
            print_info("Info for", str(service), bold=True,
                no_line_break=True)
            print("-" * (len("Info for: ") + len(str(
                service))))
            info = ServiceDetailInfo(service).get()
            if info["Running"]:
                print_info("Running", "yes", color="green")
            else:
                print_info("Running", "no", color="red")
            print_info("Default docker container name",
                info["Default docker container name"])
            print_info("Configuration file",
                os.path.join(service.group_dir,
                "docker-compose.yml"))
            snapshots = Snapshots(service)
            if not snapshots.subvolume_readiness_check():
                print_info("Live data snapshot support",
                    "no",
                    color="red")
                print("  Hint: try \"drudder snapshot " +
                    str(service.group.name) + "/" +
                    str(service.name) + "\" for details")
            else:
                print_info("Live data snapshot support", "ok", color="green")
            print("This service depends on "+\
                "(ensured to run if this one starts):")
            l = info["Dependencies"]["Pre-Start"]
            if len(l) == 0:
                print("  <no dependencies>")
            else:
                for item in l:
                    print(" - " + str(item))
            print("Other services depending on this " +\
                "(restarted along with this one):")
            l = info["Dependencies"]["Pre-Stop-Post-Restart"]
            if len(l) == 0:
                print("  <no dependencies>")
            else:
                for item in l:
                    print(" - " + str(item))
            have_potentially_lost_volumes = False
            for volume in info["Volumes"]:
                if "Potentially lost on container recreation" in volume:
                    if volume["Potentially lost on container recreation"] \
                            is True:
                        have_potentially_lost_volumes = True
                        break
            if have_potentially_lost_volumes:
                potentially_lost = service.potentially_lost_volumes
                if len(potentially_lost) > 0:
                    print("\033[33m\033[1mWarning: some volumes might be " +\
                        "lost on container recreation:\n         "+\
                        ", ".join([str(vol) for vol in potentially_lost]) +\
                        "\n" +\
                        "         Make sure to specify them in your " +\
                            "docker-compose.yml!\033[0m")
        sys.exit(0)
    elif args.action == "dump-container-info" or\
            args.action == "dump-service-info":
        if len(args.argument) == 0:
            print("drudder: error: please specify the name " + \
                "of the service or container to print info for, " +\
                "or \"all\"", file=sys.stderr)
            sys.exit(1)
        services = TargetsParser.get_services(" ".join(args.argument),
            print_error=True)
        if services == None:
            sys.exit(1)
        for service in services:
            info = ServiceDetailInfo(service).get(ordered=True)
            print("# --- Service: " + str(service))
            print("\"" + str(service) + "\":")
            print("  " + "\n  ".join(pyaml.dump(info).split("\n")))
        sys.exit(0)
    elif args.action == "clean":
        ServiceGroup.global_clean_up(args.force != True)
    else:
        unknown_action()

if __name__ == "__main__":
    main()

